{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "7yrwLkQG4Zf2",
        "outputId": "bd343efe-cbfd-4373-da09-a6673128c3ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0753f5d2-baa7-41ee-84b5-ca7d662f9537\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0753f5d2-baa7-41ee-84b5-ca7d662f9537\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mt0010\",\"key\":\"bc8eb0133e92bbc5e2f73f99ce5118e0\"}'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json when prompted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kVwe0Cl4U1S",
        "outputId": "08e5a4ad-89cf-4ea4-c795-6fb3107d5e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/wilyzh/imagenet100\n",
            "License(s): unknown\n",
            "Downloading imagenet100.zip to /content\n",
            " 99% 13.9G/14.1G [00:46<00:00, 144MB/s] \n",
            "100% 14.1G/14.1G [00:46<00:00, 326MB/s]\n",
            "ls: cannot access 'imagenet100': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d wilyzh/imagenet100 --unzip\n",
        "!ls imagenet100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NCv9Emm84rcL"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "train_full = datasets.ImageFolder(root='ImageNet100/train')\n",
        "val_full = datasets.ImageFolder(root='ImageNet100/val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdsj2CMx3v0q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from typing import Any, Dict, List, Tuple\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset,TensorDataset\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.amp import autocast, GradScaler\n",
        "from typing import Any, Dict\n",
        "\n",
        "# Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_SIZE = 224  #  image size\n",
        "BATCH_SIZE = 16  # 4 rotations × 16 = 66 processed images\n",
        "NUM_EPOCHS_PER_TASK = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "LAMBDA_CASSLE = 3 # Weight for CaSSle loss\n",
        "NUM_CLASSES_PER_TASK = 10\n",
        "NUM_TOTAL_CLASSES = 100  # Total  classes\n",
        "NUM_ROT_CLASSES = 4  # 0°, 90°, 180°, 270°\n",
        "LINEAR_EVAL_EPOCHS = 5\n",
        "LINEAR_EVAL_BATCH_SIZE = 128\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class RotNetImageNet100Dataset(Dataset):\n",
        "    def __init__(self, imagenet_dataset, class_list, base_transform):\n",
        "        self.imagenet_dataset = imagenet_dataset\n",
        "        self.class_set = set(class_list)\n",
        "        self.valid_indices = []\n",
        "\n",
        "        for i, (_, label) in enumerate(imagenet_dataset.imgs):  # Use imgs attribute directly\n",
        "            if label in self.class_set:\n",
        "                self.valid_indices.append(i)\n",
        "\n",
        "        self.base_transform = base_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.valid_indices[idx]\n",
        "        img_path, original_label = self.imagenet_dataset.imgs[real_idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        rotated_imgs = []\n",
        "        rotation_labels = []\n",
        "\n",
        "        for angle, rot_label in zip([0, 90, 180, 270], range(4)):\n",
        "            rotated_img = transforms.functional.rotate(img, angle)\n",
        "            rotated_img = self.base_transform(rotated_img)\n",
        "            rotated_imgs.append(rotated_img)\n",
        "            rotation_labels.append(torch.tensor(rot_label, dtype=torch.long))\n",
        "\n",
        "        return torch.stack(rotated_imgs), torch.stack(rotation_labels), original_label\n",
        "\n",
        "\n",
        "# --- RotNet Model using Custom Backbone (no changes needed here) ---\n",
        "class RotNetModel(nn.Module):\n",
        "    def __init__(self, num_rot_classes: int = 4, backbone: str = 'pretrained_resnet18'):\n",
        "        super().__init__()\n",
        "\n",
        "        if backbone == 'pretrained_resnet18':\n",
        "            # ImageNet-pretrained ResNet18\n",
        "            self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "            self.backbone.fc = nn.Identity()\n",
        "            self.features_dim = 512\n",
        "\n",
        "        elif backbone == 'places_resnet18':\n",
        "            # Load ResNet18 pretrained on Places365\n",
        "            self.backbone = models.resnet18(num_classes=365)\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                'http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar',\n",
        "                map_location='cpu'\n",
        "            )\n",
        "            state_dict = {k.replace('module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
        "            self.backbone.load_state_dict(state_dict)\n",
        "            self.backbone.fc = nn.Identity()\n",
        "            self.features_dim = 512\n",
        "\n",
        "        else:\n",
        "            # Use custom backbone if specified\n",
        "            self.backbone = CustomBackbone()\n",
        "            self.features_dim = self.backbone.features_dim\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.features_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_rot_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, Any]:\n",
        "        features = self.backbone(x)\n",
        "        features_flat = features.view(features.size(0), -1)\n",
        "        logits = self.classifier(features_flat)\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'features': features_flat\n",
        "        }\n",
        "\n",
        "    def calculate_ssl_loss(self, logits: torch.Tensor, rot_labels: torch.Tensor) -> torch.Tensor:\n",
        "        return F.cross_entropy(logits, rot_labels.long())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ozMT0kEj3x9i"
      },
      "outputs": [],
      "source": [
        "class CaSSLePredictor(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),  # Add BatchNorm\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Add Dropout for regularization\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),  # Additional layer\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim // 2, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaflUz6Y3zOq"
      },
      "outputs": [],
      "source": [
        "class CaSSleTrainer:\n",
        "    def __init__(self, base_ssl_model: RotNetModel,\n",
        "                 ca_predictor_hidden_dim: int,\n",
        "                 learning_rate: float = 1e-4, lambda_cassle: float = 0.1, device: str = 'cuda'):\n",
        "\n",
        "        self.base_ssl_model = base_ssl_model.to(device) # This is f_t + rotation_head\n",
        "        self.lambda_cassle = lambda_cassle\n",
        "        self.device = device\n",
        "\n",
        "        # Input and output dimensions for CaSSLe Predictor are the backbone's feature dimension\n",
        "        predictor_input_output_dim = self.base_ssl_model.features_dim\n",
        "\n",
        "        # Initialize the current CaSSLe predictor\n",
        "        self.g_current = CaSSLePredictor(\n",
        "            predictor_input_output_dim,\n",
        "            ca_predictor_hidden_dim,\n",
        "            predictor_input_output_dim\n",
        "        ).to(device)\n",
        "\n",
        "        # Optimizer for ALL trainable parameters: current RotNet model (f_t + head) AND predictor g\n",
        "        self.optimizer = torch.optim.AdamW([\n",
        "            {'params': self.base_ssl_model.backbone.parameters(), 'lr': learning_rate * 0.1},  # Lower LR for backbone\n",
        "            {'params': self.base_ssl_model.classifier.parameters(), 'lr': learning_rate},\n",
        "            {'params': self.g_current.parameters(), 'lr': learning_rate}\n",
        "        ], weight_decay=0.01)\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            self.optimizer, T_max=50, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "\n",
        "        # This will hold the frozen previous encoder (f_{t-1}^{frozen})\n",
        "        self.f_frozen_teacher = None\n",
        "\n",
        "    def set_previous_frozen_encoder(self, encoder_state_dict: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Loads the state of the previous encoder (f_{t-1}) and freezes it.\n",
        "        \"\"\"\n",
        "        # Create new backbone with same configuration as original\n",
        "        self.f_frozen_teacher = copy.deepcopy(self.base_ssl_model.backbone)\n",
        "        self.f_frozen_teacher.to(self.device)\n",
        "\n",
        "        # Load state dict\n",
        "        self.f_frozen_teacher.load_state_dict(encoder_state_dict)\n",
        "\n",
        "        # Freeze the parameters\n",
        "        for param in self.f_frozen_teacher.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(f\"Frozen encoder (f_t-1) loaded and parameters frozen: {all(not p.requires_grad for p in self.f_frozen_teacher.parameters())}\")\n",
        "\n",
        "\n",
        "    def train_task(self, data_loader: torch.utils.data.DataLoader, epochs: int):\n",
        "        self.base_ssl_model.train() # f_t + rotation_head is trainable\n",
        "        self.g_current.train() # g is trainable\n",
        "\n",
        "        # Set f_frozen_teacher to eval mode to disable dropout/batchnorm updates for teacher\n",
        "        if self.f_frozen_teacher:\n",
        "            self.f_frozen_teacher.eval()\n",
        "\n",
        "        print(f\"Distilling from frozen teacher encoder (f_t-1): {self.f_frozen_teacher is not None}\")\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience = 3\n",
        "        patience_counter = 0\n",
        "        min_delta = 0.001\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_ssl_loss = 0\n",
        "            total_cassle_loss = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(data_loader):\n",
        "                rotated_imgs, rotation_labels, original_labels = batch[:3] # _ for original_label\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # rotated_imgs: (batch_size, 4, C, H, W)\n",
        "                # rotation_labels: (batch_size, 4)\n",
        "\n",
        "                # Flatten the batch and rotation dimensions for model input\n",
        "                imgs_flat = rotated_imgs.view(-1, *rotated_imgs.shape[2:]).to(self.device)\n",
        "                labels_flat = rotation_labels.view(-1).to(self.device)\n",
        "                self.scaler = GradScaler()\n",
        "                with autocast(device_type='cuda'):\n",
        "                    #  Forward Pass through the current trainable RotNet model (f_t + head)\n",
        "                    ssl_output = self.base_ssl_model(imgs_flat)\n",
        "\n",
        "                    # Calculate Base Self-Supervised Loss  (Cross-Entropy)\n",
        "                    loss_ssl = self.base_ssl_model.calculate_ssl_loss(ssl_output['logits'], labels_flat)\n",
        "\n",
        "                    # Calculate CaSSle Distillation Loss (L_D)\n",
        "                    loss_cassle = torch.tensor(0.0).to(self.device) # Initialize to 0 for the first task\n",
        "\n",
        "                    if self.f_frozen_teacher:\n",
        "                        # Get features from the *frozen previous encoder* (f_{t-1}^{frozen})\n",
        "                        with torch.no_grad():\n",
        "                            features_from_frozen = self.f_frozen_teacher(imgs_flat)\n",
        "\n",
        "                        # Student predictions from current trainable 'g'\n",
        "                        # g takes features from current f_t\n",
        "                        student_pred = self.g_current(ssl_output['features'])\n",
        "                        teacher_target = features_from_frozen\n",
        "\n",
        "                        # Normalize both before cosine similarity\n",
        "                        student_pred_norm = F.normalize(student_pred, dim=-1)\n",
        "                        teacher_target_norm = F.normalize(teacher_target, dim=-1)\n",
        "\n",
        "                        # Compute distillation loss \n",
        "                        loss_cassle = (1 - F.cosine_similarity(student_pred_norm, teacher_target_norm, dim=-1).mean()) * self.lambda_cassle\n",
        "\n",
        "                    # --- Total Loss and Optimization ---\n",
        "                    loss = loss_ssl +  loss_cassle\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                total_ssl_loss += loss_ssl.item()\n",
        "                total_cassle_loss += loss_cassle.item()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            #Early stopping\n",
        "            avg_loss = total_loss / len(data_loader)\n",
        "            #scheduler step\n",
        "            self.scheduler.step()\n",
        "\n",
        "            if avg_loss < best_loss - min_delta:\n",
        "                best_loss = avg_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - SSL Loss: {total_ssl_loss / len(data_loader):.4f}, \"\n",
        "                  f\"CaSSle Loss: {total_cassle_loss / len(data_loader):.4f}, \"\n",
        "                  f\"Total Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "        # State_dict of the current RotNet model's backbone (f_t).\n",
        "        return self.base_ssl_model.backbone.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqyBWcxU31A4",
        "outputId": "7aa483ac-21ee-4628-f2fe-997bc89f73ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set to 42\n",
            "Task 1 includes classes: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Task 2 includes classes: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Task 3 includes classes: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Task 4 includes classes: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Task 5 includes classes: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Task 6 includes classes: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Task 7 includes classes: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Task 8 includes classes: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Task 9 includes classes: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Task 10 includes classes: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\" to /root/.cache/torch/hub/checkpoints/resnet18_places365.pth.tar\n",
            "100%|██████████| 43.4M/43.4M [00:04<00:00, 9.96MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating random baselines for all tasks...\n",
            "Calculating random baseline for Task 1\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 20.40%\n",
            "Calculating random baseline for Task 2\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 27.80%\n",
            "Calculating random baseline for Task 3\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 22.60%\n",
            "Calculating random baseline for Task 4\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 35.00%\n",
            "Calculating random baseline for Task 5\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 23.40%\n",
            "Calculating random baseline for Task 6\n",
            "Filtered dataset: 12560 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 19.40%\n",
            "Calculating random baseline for Task 7\n",
            "Filtered dataset: 12849 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 19.60%\n",
            "Calculating random baseline for Task 8\n",
            "Filtered dataset: 12854 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 27.20%\n",
            "Calculating random baseline for Task 9\n",
            "Filtered dataset: 12678 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 24.60%\n",
            "Calculating random baseline for Task 10\n",
            "Filtered dataset: 11856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 21.00%\n",
            "Random baselines calculated!\n",
            "\n",
            "===== Training Task 1/10 =====\n",
            "Distilling from frozen teacher encoder (f_t-1): False\n",
            "Epoch 1/20 - SSL Loss: 0.7650, CaSSle Loss: 0.0000, Total Loss: 0.7650\n",
            "Epoch 2/20 - SSL Loss: 0.3830, CaSSle Loss: 0.0000, Total Loss: 0.3830\n",
            "Epoch 3/20 - SSL Loss: 0.2688, CaSSle Loss: 0.0000, Total Loss: 0.2688\n",
            "Epoch 4/20 - SSL Loss: 0.1987, CaSSle Loss: 0.0000, Total Loss: 0.1987\n",
            "Epoch 5/20 - SSL Loss: 0.1446, CaSSle Loss: 0.0000, Total Loss: 0.1446\n",
            "Epoch 6/20 - SSL Loss: 0.0996, CaSSle Loss: 0.0000, Total Loss: 0.0996\n",
            "Epoch 7/20 - SSL Loss: 0.0701, CaSSle Loss: 0.0000, Total Loss: 0.0701\n",
            "Epoch 8/20 - SSL Loss: 0.0470, CaSSle Loss: 0.0000, Total Loss: 0.0470\n",
            "Epoch 9/20 - SSL Loss: 0.0318, CaSSle Loss: 0.0000, Total Loss: 0.0318\n",
            "Epoch 10/20 - SSL Loss: 0.0245, CaSSle Loss: 0.0000, Total Loss: 0.0245\n",
            "Epoch 11/20 - SSL Loss: 0.0190, CaSSle Loss: 0.0000, Total Loss: 0.0190\n",
            "Epoch 12/20 - SSL Loss: 0.0160, CaSSle Loss: 0.0000, Total Loss: 0.0160\n",
            "Epoch 13/20 - SSL Loss: 0.0129, CaSSle Loss: 0.0000, Total Loss: 0.0129\n",
            "Epoch 14/20 - SSL Loss: 0.0128, CaSSle Loss: 0.0000, Total Loss: 0.0128\n",
            "Epoch 15/20 - SSL Loss: 0.0103, CaSSle Loss: 0.0000, Total Loss: 0.0103\n",
            "Epoch 16/20 - SSL Loss: 0.0070, CaSSle Loss: 0.0000, Total Loss: 0.0070\n",
            "Epoch 17/20 - SSL Loss: 0.0089, CaSSle Loss: 0.0000, Total Loss: 0.0089\n",
            "Epoch 18/20 - SSL Loss: 0.0078, CaSSle Loss: 0.0000, Total Loss: 0.0078\n",
            "Early stopping triggered.\n",
            "\n",
            "--- Evaluating after Task 1 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 83.60%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 83.60%\n",
            "    Improvement over random: 63.20%\n",
            "\n",
            "===== Training Task 2/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.6451, CaSSle Loss: 0.5599, Total Loss: 1.2051\n",
            "Epoch 2/20 - SSL Loss: 0.3200, CaSSle Loss: 0.3511, Total Loss: 0.6711\n",
            "Epoch 3/20 - SSL Loss: 0.2503, CaSSle Loss: 0.3070, Total Loss: 0.5574\n",
            "Epoch 4/20 - SSL Loss: 0.1933, CaSSle Loss: 0.2858, Total Loss: 0.4791\n",
            "Epoch 5/20 - SSL Loss: 0.1404, CaSSle Loss: 0.2741, Total Loss: 0.4145\n",
            "Epoch 6/20 - SSL Loss: 0.0939, CaSSle Loss: 0.2663, Total Loss: 0.3602\n",
            "Epoch 7/20 - SSL Loss: 0.0619, CaSSle Loss: 0.2614, Total Loss: 0.3233\n",
            "Epoch 8/20 - SSL Loss: 0.0409, CaSSle Loss: 0.2571, Total Loss: 0.2981\n",
            "Epoch 9/20 - SSL Loss: 0.0286, CaSSle Loss: 0.2530, Total Loss: 0.2816\n",
            "Epoch 10/20 - SSL Loss: 0.0213, CaSSle Loss: 0.2500, Total Loss: 0.2712\n",
            "Epoch 11/20 - SSL Loss: 0.0170, CaSSle Loss: 0.2476, Total Loss: 0.2646\n",
            "Epoch 12/20 - SSL Loss: 0.0146, CaSSle Loss: 0.2445, Total Loss: 0.2592\n",
            "Epoch 13/20 - SSL Loss: 0.0149, CaSSle Loss: 0.2428, Total Loss: 0.2577\n",
            "Epoch 14/20 - SSL Loss: 0.0117, CaSSle Loss: 0.2413, Total Loss: 0.2531\n",
            "Epoch 15/20 - SSL Loss: 0.0098, CaSSle Loss: 0.2395, Total Loss: 0.2492\n",
            "Epoch 16/20 - SSL Loss: 0.0098, CaSSle Loss: 0.2383, Total Loss: 0.2481\n",
            "Epoch 17/20 - SSL Loss: 0.0086, CaSSle Loss: 0.2362, Total Loss: 0.2449\n",
            "Epoch 18/20 - SSL Loss: 0.0074, CaSSle Loss: 0.2351, Total Loss: 0.2425\n",
            "Epoch 19/20 - SSL Loss: 0.0068, CaSSle Loss: 0.2338, Total Loss: 0.2406\n",
            "Epoch 20/20 - SSL Loss: 0.0062, CaSSle Loss: 0.2328, Total Loss: 0.2391\n",
            "\n",
            "--- Evaluating after Task 2 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.40%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 84.40%\n",
            "    Improvement over random: 64.00%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 89.00%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 89.00%\n",
            "    Improvement over random: 61.20%\n",
            "\n",
            "===== Training Task 3/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.5640, CaSSle Loss: 0.5557, Total Loss: 1.1196\n",
            "Epoch 2/20 - SSL Loss: 0.2742, CaSSle Loss: 0.3512, Total Loss: 0.6255\n",
            "Epoch 3/20 - SSL Loss: 0.2182, CaSSle Loss: 0.3057, Total Loss: 0.5238\n",
            "Epoch 4/20 - SSL Loss: 0.1751, CaSSle Loss: 0.2837, Total Loss: 0.4588\n",
            "Epoch 5/20 - SSL Loss: 0.1339, CaSSle Loss: 0.2723, Total Loss: 0.4062\n",
            "Epoch 6/20 - SSL Loss: 0.0965, CaSSle Loss: 0.2646, Total Loss: 0.3611\n",
            "Epoch 7/20 - SSL Loss: 0.0670, CaSSle Loss: 0.2591, Total Loss: 0.3261\n",
            "Epoch 8/20 - SSL Loss: 0.0451, CaSSle Loss: 0.2538, Total Loss: 0.2989\n",
            "Epoch 9/20 - SSL Loss: 0.0340, CaSSle Loss: 0.2505, Total Loss: 0.2844\n",
            "Epoch 10/20 - SSL Loss: 0.0260, CaSSle Loss: 0.2479, Total Loss: 0.2740\n",
            "Epoch 11/20 - SSL Loss: 0.0205, CaSSle Loss: 0.2451, Total Loss: 0.2657\n",
            "Epoch 12/20 - SSL Loss: 0.0158, CaSSle Loss: 0.2427, Total Loss: 0.2585\n",
            "Epoch 13/20 - SSL Loss: 0.0133, CaSSle Loss: 0.2401, Total Loss: 0.2534\n",
            "Epoch 14/20 - SSL Loss: 0.0118, CaSSle Loss: 0.2376, Total Loss: 0.2495\n",
            "Epoch 15/20 - SSL Loss: 0.0109, CaSSle Loss: 0.2375, Total Loss: 0.2485\n",
            "Epoch 16/20 - SSL Loss: 0.0106, CaSSle Loss: 0.2356, Total Loss: 0.2462\n",
            "Epoch 17/20 - SSL Loss: 0.0100, CaSSle Loss: 0.2342, Total Loss: 0.2441\n",
            "Epoch 18/20 - SSL Loss: 0.0083, CaSSle Loss: 0.2334, Total Loss: 0.2417\n",
            "Epoch 19/20 - SSL Loss: 0.0078, CaSSle Loss: 0.2313, Total Loss: 0.2391\n",
            "Epoch 20/20 - SSL Loss: 0.0071, CaSSle Loss: 0.2302, Total Loss: 0.2373\n",
            "\n",
            "--- Evaluating after Task 3 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.20%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 84.20%\n",
            "    Improvement over random: 63.80%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 88.60%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 88.60%\n",
            "    Improvement over random: 60.80%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 78.80%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 78.80%\n",
            "    Improvement over random: 56.20%\n",
            "\n",
            "===== Training Task 4/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.4912, CaSSle Loss: 0.5224, Total Loss: 1.0136\n",
            "Epoch 2/20 - SSL Loss: 0.2222, CaSSle Loss: 0.3254, Total Loss: 0.5477\n",
            "Epoch 3/20 - SSL Loss: 0.1655, CaSSle Loss: 0.2816, Total Loss: 0.4471\n",
            "Epoch 4/20 - SSL Loss: 0.1215, CaSSle Loss: 0.2602, Total Loss: 0.3817\n",
            "Epoch 5/20 - SSL Loss: 0.0819, CaSSle Loss: 0.2481, Total Loss: 0.3300\n",
            "Epoch 6/20 - SSL Loss: 0.0515, CaSSle Loss: 0.2394, Total Loss: 0.2909\n",
            "Epoch 7/20 - SSL Loss: 0.0332, CaSSle Loss: 0.2337, Total Loss: 0.2669\n",
            "Epoch 8/20 - SSL Loss: 0.0213, CaSSle Loss: 0.2288, Total Loss: 0.2501\n",
            "Epoch 9/20 - SSL Loss: 0.0160, CaSSle Loss: 0.2254, Total Loss: 0.2414\n",
            "Epoch 10/20 - SSL Loss: 0.0131, CaSSle Loss: 0.2224, Total Loss: 0.2355\n",
            "Epoch 11/20 - SSL Loss: 0.0101, CaSSle Loss: 0.2193, Total Loss: 0.2294\n",
            "Epoch 12/20 - SSL Loss: 0.0095, CaSSle Loss: 0.2170, Total Loss: 0.2265\n",
            "Epoch 13/20 - SSL Loss: 0.0081, CaSSle Loss: 0.2152, Total Loss: 0.2233\n",
            "Epoch 14/20 - SSL Loss: 0.0078, CaSSle Loss: 0.2135, Total Loss: 0.2213\n",
            "Epoch 15/20 - SSL Loss: 0.0064, CaSSle Loss: 0.2122, Total Loss: 0.2186\n",
            "Epoch 16/20 - SSL Loss: 0.0064, CaSSle Loss: 0.2103, Total Loss: 0.2167\n",
            "Epoch 17/20 - SSL Loss: 0.0060, CaSSle Loss: 0.2089, Total Loss: 0.2150\n",
            "Epoch 18/20 - SSL Loss: 0.0054, CaSSle Loss: 0.2090, Total Loss: 0.2144\n",
            "Epoch 19/20 - SSL Loss: 0.0044, CaSSle Loss: 0.2068, Total Loss: 0.2112\n",
            "Epoch 20/20 - SSL Loss: 0.0044, CaSSle Loss: 0.2060, Total Loss: 0.2105\n",
            "\n",
            "--- Evaluating after Task 4 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 85.60%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 85.60%\n",
            "    Improvement over random: 65.20%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 87.60%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 87.60%\n",
            "    Improvement over random: 59.80%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 80.60%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 80.60%\n",
            "    Improvement over random: 58.00%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 93.80%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 93.80%\n",
            "    Improvement over random: 58.80%\n",
            "\n",
            "===== Training Task 5/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.5586, CaSSle Loss: 0.5254, Total Loss: 1.0841\n",
            "Epoch 2/20 - SSL Loss: 0.2540, CaSSle Loss: 0.3286, Total Loss: 0.5825\n",
            "Epoch 3/20 - SSL Loss: 0.1934, CaSSle Loss: 0.2835, Total Loss: 0.4769\n",
            "Epoch 4/20 - SSL Loss: 0.1498, CaSSle Loss: 0.2609, Total Loss: 0.4108\n",
            "Epoch 5/20 - SSL Loss: 0.1125, CaSSle Loss: 0.2480, Total Loss: 0.3605\n",
            "Epoch 6/20 - SSL Loss: 0.0780, CaSSle Loss: 0.2395, Total Loss: 0.3175\n",
            "Epoch 7/20 - SSL Loss: 0.0532, CaSSle Loss: 0.2339, Total Loss: 0.2871\n",
            "Epoch 8/20 - SSL Loss: 0.0365, CaSSle Loss: 0.2292, Total Loss: 0.2657\n",
            "Epoch 9/20 - SSL Loss: 0.0257, CaSSle Loss: 0.2261, Total Loss: 0.2517\n",
            "Epoch 10/20 - SSL Loss: 0.0184, CaSSle Loss: 0.2218, Total Loss: 0.2402\n",
            "Epoch 11/20 - SSL Loss: 0.0153, CaSSle Loss: 0.2196, Total Loss: 0.2349\n",
            "Epoch 12/20 - SSL Loss: 0.0140, CaSSle Loss: 0.2168, Total Loss: 0.2308\n",
            "Epoch 13/20 - SSL Loss: 0.0101, CaSSle Loss: 0.2149, Total Loss: 0.2251\n",
            "Epoch 14/20 - SSL Loss: 0.0100, CaSSle Loss: 0.2133, Total Loss: 0.2233\n",
            "Epoch 15/20 - SSL Loss: 0.0110, CaSSle Loss: 0.2119, Total Loss: 0.2228\n",
            "Epoch 16/20 - SSL Loss: 0.0077, CaSSle Loss: 0.2097, Total Loss: 0.2174\n",
            "Epoch 17/20 - SSL Loss: 0.0079, CaSSle Loss: 0.2091, Total Loss: 0.2170\n",
            "Epoch 18/20 - SSL Loss: 0.0054, CaSSle Loss: 0.2066, Total Loss: 0.2120\n",
            "Epoch 19/20 - SSL Loss: 0.0062, CaSSle Loss: 0.2060, Total Loss: 0.2121\n",
            "Epoch 20/20 - SSL Loss: 0.0048, CaSSle Loss: 0.2045, Total Loss: 0.2092\n",
            "\n",
            "--- Evaluating after Task 5 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 85.00%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 85.00%\n",
            "    Improvement over random: 64.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 87.00%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 87.00%\n",
            "    Improvement over random: 59.20%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 78.80%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 78.80%\n",
            "    Improvement over random: 56.20%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 93.20%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 93.20%\n",
            "    Improvement over random: 58.20%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 81.60%\n",
            "    Random baseline for Task 5: 23.40%\n",
            "    Current accuracy for Task 5: 81.60%\n",
            "    Improvement over random: 58.20%\n",
            "\n",
            "===== Training Task 6/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.6591, CaSSle Loss: 0.5103, Total Loss: 1.1695\n",
            "Epoch 2/20 - SSL Loss: 0.2947, CaSSle Loss: 0.3118, Total Loss: 0.6064\n",
            "Epoch 3/20 - SSL Loss: 0.2279, CaSSle Loss: 0.2668, Total Loss: 0.4946\n",
            "Epoch 4/20 - SSL Loss: 0.1698, CaSSle Loss: 0.2456, Total Loss: 0.4154\n",
            "Epoch 5/20 - SSL Loss: 0.1172, CaSSle Loss: 0.2337, Total Loss: 0.3509\n",
            "Epoch 6/20 - SSL Loss: 0.0710, CaSSle Loss: 0.2262, Total Loss: 0.2972\n",
            "Epoch 7/20 - SSL Loss: 0.0452, CaSSle Loss: 0.2217, Total Loss: 0.2669\n",
            "Epoch 8/20 - SSL Loss: 0.0278, CaSSle Loss: 0.2179, Total Loss: 0.2457\n",
            "Epoch 9/20 - SSL Loss: 0.0198, CaSSle Loss: 0.2141, Total Loss: 0.2340\n",
            "Epoch 10/20 - SSL Loss: 0.0163, CaSSle Loss: 0.2107, Total Loss: 0.2270\n",
            "Epoch 11/20 - SSL Loss: 0.0116, CaSSle Loss: 0.2096, Total Loss: 0.2212\n",
            "Epoch 12/20 - SSL Loss: 0.0118, CaSSle Loss: 0.2075, Total Loss: 0.2193\n",
            "Epoch 13/20 - SSL Loss: 0.0105, CaSSle Loss: 0.2057, Total Loss: 0.2162\n",
            "Epoch 14/20 - SSL Loss: 0.0102, CaSSle Loss: 0.2042, Total Loss: 0.2144\n",
            "Epoch 15/20 - SSL Loss: 0.0087, CaSSle Loss: 0.2032, Total Loss: 0.2119\n",
            "Epoch 16/20 - SSL Loss: 0.0069, CaSSle Loss: 0.2014, Total Loss: 0.2083\n",
            "Epoch 17/20 - SSL Loss: 0.0072, CaSSle Loss: 0.1999, Total Loss: 0.2071\n",
            "Epoch 18/20 - SSL Loss: 0.0074, CaSSle Loss: 0.1988, Total Loss: 0.2062\n",
            "Epoch 19/20 - SSL Loss: 0.0056, CaSSle Loss: 0.1983, Total Loss: 0.2039\n",
            "Epoch 20/20 - SSL Loss: 0.0059, CaSSle Loss: 0.1972, Total Loss: 0.2030\n",
            "\n",
            "--- Evaluating after Task 6 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.00%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 84.00%\n",
            "    Improvement over random: 63.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 86.20%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 86.20%\n",
            "    Improvement over random: 58.40%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 80.20%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 80.20%\n",
            "    Improvement over random: 57.60%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 92.20%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 92.20%\n",
            "    Improvement over random: 57.20%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 81.60%\n",
            "    Random baseline for Task 5: 23.40%\n",
            "    Current accuracy for Task 5: 81.60%\n",
            "    Improvement over random: 58.20%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Filtered dataset: 12560 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.80%\n",
            "    Random baseline for Task 6: 19.40%\n",
            "    Current accuracy for Task 6: 84.80%\n",
            "    Improvement over random: 65.40%\n",
            "\n",
            "===== Training Task 7/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.4300, CaSSle Loss: 0.4844, Total Loss: 0.9143\n",
            "Epoch 2/20 - SSL Loss: 0.1977, CaSSle Loss: 0.2917, Total Loss: 0.4894\n",
            "Epoch 3/20 - SSL Loss: 0.1367, CaSSle Loss: 0.2485, Total Loss: 0.3851\n",
            "Epoch 4/20 - SSL Loss: 0.0879, CaSSle Loss: 0.2280, Total Loss: 0.3159\n",
            "Epoch 5/20 - SSL Loss: 0.0508, CaSSle Loss: 0.2165, Total Loss: 0.2673\n",
            "Epoch 6/20 - SSL Loss: 0.0295, CaSSle Loss: 0.2089, Total Loss: 0.2384\n",
            "Epoch 7/20 - SSL Loss: 0.0177, CaSSle Loss: 0.2044, Total Loss: 0.2221\n",
            "Epoch 8/20 - SSL Loss: 0.0134, CaSSle Loss: 0.2003, Total Loss: 0.2137\n",
            "Epoch 9/20 - SSL Loss: 0.0101, CaSSle Loss: 0.1977, Total Loss: 0.2078\n",
            "Epoch 10/20 - SSL Loss: 0.0087, CaSSle Loss: 0.1954, Total Loss: 0.2041\n",
            "Epoch 11/20 - SSL Loss: 0.0111, CaSSle Loss: 0.1926, Total Loss: 0.2036\n",
            "Epoch 12/20 - SSL Loss: 0.0064, CaSSle Loss: 0.1900, Total Loss: 0.1965\n",
            "Epoch 13/20 - SSL Loss: 0.0060, CaSSle Loss: 0.1888, Total Loss: 0.1947\n",
            "Epoch 14/20 - SSL Loss: 0.0063, CaSSle Loss: 0.1877, Total Loss: 0.1940\n",
            "Epoch 15/20 - SSL Loss: 0.0045, CaSSle Loss: 0.1864, Total Loss: 0.1909\n",
            "Epoch 16/20 - SSL Loss: 0.0046, CaSSle Loss: 0.1853, Total Loss: 0.1899\n",
            "Epoch 17/20 - SSL Loss: 0.0047, CaSSle Loss: 0.1848, Total Loss: 0.1895\n",
            "Epoch 18/20 - SSL Loss: 0.0039, CaSSle Loss: 0.1834, Total Loss: 0.1873\n",
            "Epoch 19/20 - SSL Loss: 0.0033, CaSSle Loss: 0.1825, Total Loss: 0.1858\n",
            "Epoch 20/20 - SSL Loss: 0.0025, CaSSle Loss: 0.1817, Total Loss: 0.1842\n",
            "\n",
            "--- Evaluating after Task 7 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 83.40%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 83.40%\n",
            "    Improvement over random: 63.00%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 86.40%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 86.40%\n",
            "    Improvement over random: 58.60%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 78.00%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 78.00%\n",
            "    Improvement over random: 55.40%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 92.40%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 92.40%\n",
            "    Improvement over random: 57.40%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.00%\n",
            "    Random baseline for Task 5: 23.40%\n",
            "    Current accuracy for Task 5: 82.00%\n",
            "    Improvement over random: 58.60%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Filtered dataset: 12560 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 86.00%\n",
            "    Random baseline for Task 6: 19.40%\n",
            "    Current accuracy for Task 6: 86.00%\n",
            "    Improvement over random: 66.60%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Filtered dataset: 12849 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.20%\n",
            "    Random baseline for Task 7: 19.60%\n",
            "    Current accuracy for Task 7: 82.20%\n",
            "    Improvement over random: 62.60%\n",
            "\n",
            "===== Training Task 8/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.5632, CaSSle Loss: 0.4764, Total Loss: 1.0396\n",
            "Epoch 2/20 - SSL Loss: 0.2410, CaSSle Loss: 0.2875, Total Loss: 0.5286\n",
            "Epoch 3/20 - SSL Loss: 0.1689, CaSSle Loss: 0.2443, Total Loss: 0.4132\n",
            "Epoch 4/20 - SSL Loss: 0.1172, CaSSle Loss: 0.2236, Total Loss: 0.3408\n",
            "Epoch 5/20 - SSL Loss: 0.0752, CaSSle Loss: 0.2130, Total Loss: 0.2882\n",
            "Epoch 6/20 - SSL Loss: 0.0439, CaSSle Loss: 0.2060, Total Loss: 0.2499\n",
            "Epoch 7/20 - SSL Loss: 0.0270, CaSSle Loss: 0.2013, Total Loss: 0.2283\n",
            "Epoch 8/20 - SSL Loss: 0.0172, CaSSle Loss: 0.1981, Total Loss: 0.2153\n",
            "Epoch 9/20 - SSL Loss: 0.0125, CaSSle Loss: 0.1943, Total Loss: 0.2068\n",
            "Epoch 10/20 - SSL Loss: 0.0117, CaSSle Loss: 0.1914, Total Loss: 0.2031\n",
            "Epoch 11/20 - SSL Loss: 0.0114, CaSSle Loss: 0.1901, Total Loss: 0.2015\n",
            "Epoch 12/20 - SSL Loss: 0.0083, CaSSle Loss: 0.1879, Total Loss: 0.1962\n",
            "Epoch 13/20 - SSL Loss: 0.0063, CaSSle Loss: 0.1866, Total Loss: 0.1930\n",
            "Epoch 14/20 - SSL Loss: 0.0062, CaSSle Loss: 0.1844, Total Loss: 0.1906\n",
            "Epoch 15/20 - SSL Loss: 0.0051, CaSSle Loss: 0.1837, Total Loss: 0.1887\n",
            "Epoch 16/20 - SSL Loss: 0.0055, CaSSle Loss: 0.1826, Total Loss: 0.1881\n",
            "Epoch 17/20 - SSL Loss: 0.0063, CaSSle Loss: 0.1813, Total Loss: 0.1876\n",
            "Epoch 18/20 - SSL Loss: 0.0046, CaSSle Loss: 0.1805, Total Loss: 0.1851\n",
            "Epoch 19/20 - SSL Loss: 0.0043, CaSSle Loss: 0.1802, Total Loss: 0.1845\n",
            "Epoch 20/20 - SSL Loss: 0.0031, CaSSle Loss: 0.1781, Total Loss: 0.1812\n",
            "\n",
            "--- Evaluating after Task 8 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.20%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 84.20%\n",
            "    Improvement over random: 63.80%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 86.60%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 86.60%\n",
            "    Improvement over random: 58.80%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 79.40%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 79.40%\n",
            "    Improvement over random: 56.80%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 92.20%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 92.20%\n",
            "    Improvement over random: 57.20%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 80.80%\n",
            "    Random baseline for Task 5: 23.40%\n",
            "    Current accuracy for Task 5: 80.80%\n",
            "    Improvement over random: 57.40%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Filtered dataset: 12560 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 86.40%\n",
            "    Random baseline for Task 6: 19.40%\n",
            "    Current accuracy for Task 6: 86.40%\n",
            "    Improvement over random: 67.00%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Filtered dataset: 12849 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.40%\n",
            "    Random baseline for Task 7: 19.60%\n",
            "    Current accuracy for Task 7: 82.40%\n",
            "    Improvement over random: 62.80%\n",
            "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Filtered dataset: 12854 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 85.60%\n",
            "    Random baseline for Task 8: 27.20%\n",
            "    Current accuracy for Task 8: 85.60%\n",
            "    Improvement over random: 58.40%\n",
            "\n",
            "===== Training Task 9/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.3865, CaSSle Loss: 0.4759, Total Loss: 0.8624\n",
            "Epoch 2/20 - SSL Loss: 0.1547, CaSSle Loss: 0.2826, Total Loss: 0.4373\n",
            "Epoch 3/20 - SSL Loss: 0.1004, CaSSle Loss: 0.2378, Total Loss: 0.3382\n",
            "Epoch 4/20 - SSL Loss: 0.0604, CaSSle Loss: 0.2165, Total Loss: 0.2769\n",
            "Epoch 5/20 - SSL Loss: 0.0334, CaSSle Loss: 0.2055, Total Loss: 0.2389\n",
            "Epoch 6/20 - SSL Loss: 0.0188, CaSSle Loss: 0.1979, Total Loss: 0.2167\n",
            "Epoch 7/20 - SSL Loss: 0.0125, CaSSle Loss: 0.1927, Total Loss: 0.2052\n",
            "Epoch 8/20 - SSL Loss: 0.0096, CaSSle Loss: 0.1891, Total Loss: 0.1987\n",
            "Epoch 9/20 - SSL Loss: 0.0086, CaSSle Loss: 0.1861, Total Loss: 0.1946\n",
            "Epoch 10/20 - SSL Loss: 0.0078, CaSSle Loss: 0.1837, Total Loss: 0.1915\n",
            "Epoch 11/20 - SSL Loss: 0.0062, CaSSle Loss: 0.1822, Total Loss: 0.1884\n",
            "Epoch 12/20 - SSL Loss: 0.0049, CaSSle Loss: 0.1799, Total Loss: 0.1848\n",
            "Epoch 13/20 - SSL Loss: 0.0048, CaSSle Loss: 0.1781, Total Loss: 0.1829\n",
            "Epoch 14/20 - SSL Loss: 0.0049, CaSSle Loss: 0.1773, Total Loss: 0.1822\n",
            "Epoch 15/20 - SSL Loss: 0.0044, CaSSle Loss: 0.1762, Total Loss: 0.1806\n",
            "Epoch 16/20 - SSL Loss: 0.0035, CaSSle Loss: 0.1751, Total Loss: 0.1786\n",
            "Epoch 17/20 - SSL Loss: 0.0029, CaSSle Loss: 0.1736, Total Loss: 0.1765\n",
            "Epoch 18/20 - SSL Loss: 0.0031, CaSSle Loss: 0.1727, Total Loss: 0.1759\n",
            "Epoch 19/20 - SSL Loss: 0.0033, CaSSle Loss: 0.1724, Total Loss: 0.1756\n",
            "Epoch 20/20 - SSL Loss: 0.0029, CaSSle Loss: 0.1716, Total Loss: 0.1745\n",
            "\n",
            "--- Evaluating after Task 9 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.00%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 84.00%\n",
            "    Improvement over random: 63.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 87.00%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 87.00%\n",
            "    Improvement over random: 59.20%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 78.40%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 78.40%\n",
            "    Improvement over random: 55.80%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 91.20%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 91.20%\n",
            "    Improvement over random: 56.20%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.60%\n",
            "    Random baseline for Task 5: 23.40%\n",
            "    Current accuracy for Task 5: 82.60%\n",
            "    Improvement over random: 59.20%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Filtered dataset: 12560 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.60%\n",
            "    Random baseline for Task 6: 19.40%\n",
            "    Current accuracy for Task 6: 84.60%\n",
            "    Improvement over random: 65.20%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Filtered dataset: 12849 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.40%\n",
            "    Random baseline for Task 7: 19.60%\n",
            "    Current accuracy for Task 7: 82.40%\n",
            "    Improvement over random: 62.80%\n",
            "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Filtered dataset: 12854 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.20%\n",
            "    Random baseline for Task 8: 27.20%\n",
            "    Current accuracy for Task 8: 84.20%\n",
            "    Improvement over random: 57.00%\n",
            "  Evaluating on classes from Task 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Filtered dataset: 12678 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.20%\n",
            "    Random baseline for Task 9: 24.60%\n",
            "    Current accuracy for Task 9: 82.20%\n",
            "    Improvement over random: 57.60%\n",
            "\n",
            "===== Training Task 10/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/20 - SSL Loss: 0.4283, CaSSle Loss: 0.4551, Total Loss: 0.8834\n",
            "Epoch 2/20 - SSL Loss: 0.1741, CaSSle Loss: 0.2727, Total Loss: 0.4468\n",
            "Epoch 3/20 - SSL Loss: 0.1114, CaSSle Loss: 0.2290, Total Loss: 0.3404\n",
            "Epoch 4/20 - SSL Loss: 0.0677, CaSSle Loss: 0.2080, Total Loss: 0.2757\n",
            "Epoch 5/20 - SSL Loss: 0.0364, CaSSle Loss: 0.1953, Total Loss: 0.2318\n",
            "Epoch 6/20 - SSL Loss: 0.0210, CaSSle Loss: 0.1877, Total Loss: 0.2087\n",
            "Epoch 7/20 - SSL Loss: 0.0132, CaSSle Loss: 0.1816, Total Loss: 0.1948\n",
            "Epoch 8/20 - SSL Loss: 0.0087, CaSSle Loss: 0.1773, Total Loss: 0.1860\n",
            "Epoch 9/20 - SSL Loss: 0.0072, CaSSle Loss: 0.1740, Total Loss: 0.1812\n",
            "Epoch 10/20 - SSL Loss: 0.0059, CaSSle Loss: 0.1719, Total Loss: 0.1778\n",
            "Epoch 11/20 - SSL Loss: 0.0068, CaSSle Loss: 0.1689, Total Loss: 0.1757\n",
            "Epoch 12/20 - SSL Loss: 0.0057, CaSSle Loss: 0.1681, Total Loss: 0.1738\n",
            "Epoch 13/20 - SSL Loss: 0.0052, CaSSle Loss: 0.1660, Total Loss: 0.1712\n",
            "Epoch 14/20 - SSL Loss: 0.0047, CaSSle Loss: 0.1647, Total Loss: 0.1694\n",
            "Epoch 15/20 - SSL Loss: 0.0046, CaSSle Loss: 0.1634, Total Loss: 0.1681\n",
            "Epoch 16/20 - SSL Loss: 0.0036, CaSSle Loss: 0.1625, Total Loss: 0.1660\n",
            "Epoch 17/20 - SSL Loss: 0.0028, CaSSle Loss: 0.1610, Total Loss: 0.1638\n",
            "Epoch 18/20 - SSL Loss: 0.0036, CaSSle Loss: 0.1600, Total Loss: 0.1636\n",
            "Epoch 19/20 - SSL Loss: 0.0024, CaSSle Loss: 0.1591, Total Loss: 0.1615\n",
            "Epoch 20/20 - SSL Loss: 0.0023, CaSSle Loss: 0.1578, Total Loss: 0.1601\n",
            "\n",
            "--- Evaluating after Task 10 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Filtered dataset: 12797 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 83.20%\n",
            "    Random baseline for Task 1: 20.40%\n",
            "    Current accuracy for Task 1: 83.20%\n",
            "    Improvement over random: 62.80%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Filtered dataset: 12856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 84.80%\n",
            "    Random baseline for Task 2: 27.80%\n",
            "    Current accuracy for Task 2: 84.80%\n",
            "    Improvement over random: 57.00%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Filtered dataset: 12821 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 78.60%\n",
            "    Random baseline for Task 3: 22.60%\n",
            "    Current accuracy for Task 3: 78.60%\n",
            "    Improvement over random: 56.00%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Filtered dataset: 12964 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 90.80%\n",
            "    Random baseline for Task 4: 35.00%\n",
            "    Current accuracy for Task 4: 90.80%\n",
            "    Improvement over random: 55.80%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Filtered dataset: 12454 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 81.40%\n",
            "    Random baseline for Task 5: 23.40%\n",
            "    Current accuracy for Task 5: 81.40%\n",
            "    Improvement over random: 58.00%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Filtered dataset: 12560 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 86.00%\n",
            "    Random baseline for Task 6: 19.40%\n",
            "    Current accuracy for Task 6: 86.00%\n",
            "    Improvement over random: 66.60%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Filtered dataset: 12849 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 81.60%\n",
            "    Random baseline for Task 7: 19.60%\n",
            "    Current accuracy for Task 7: 81.60%\n",
            "    Improvement over random: 62.00%\n",
            "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Filtered dataset: 12854 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.60%\n",
            "    Random baseline for Task 8: 27.20%\n",
            "    Current accuracy for Task 8: 82.60%\n",
            "    Improvement over random: 55.40%\n",
            "  Evaluating on classes from Task 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Filtered dataset: 12678 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 82.00%\n",
            "    Random baseline for Task 9: 24.60%\n",
            "    Current accuracy for Task 9: 82.00%\n",
            "    Improvement over random: 57.40%\n",
            "  Evaluating on classes from Task 10: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
            "Filtered dataset: 11856 samples from 10 classes\n",
            "Filtered dataset: 500 samples from 10 classes\n",
            "Extracting training features...\n",
            "Extracting validation features...\n",
            "Linear evaluation accuracy on 10 classes: 71.20%\n",
            "    Random baseline for Task 10: 21.00%\n",
            "    Current accuracy for Task 10: 71.20%\n",
            "    Improvement over random: 50.20%\n",
            "\n",
            "Final Average Accuracy (A): 82.22%\n",
            "Average Random Baseline: 24.10%\n",
            "Improvement over Random: 58.12%\n",
            "Final Forgetting (F): 1.91%\n",
            "Final Backward Transfer (BT): -0.24%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "#Evaluation Function\n",
        "class LinearEvalDataset(Dataset):\n",
        "    \"\"\"Pre-filter indices for better performance\"\"\"\n",
        "    def __init__(self, original_dataset, class_list, transform):\n",
        "        if hasattr(original_dataset, 'targets'):\n",
        "            targets = original_dataset.targets\n",
        "        else:\n",
        "            targets = [label for _, label in original_dataset]\n",
        "\n",
        "        # Pre-filter all indices at once\n",
        "        self.indices = [i for i, label in enumerate(targets) if label in class_list]\n",
        "        self.original_dataset = original_dataset\n",
        "        self.transform = transform\n",
        "        print(f\"Filtered dataset: {len(self.indices)} samples from {len(class_list)} classes\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.original_dataset[self.indices[idx]]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "def extract_features_once(feature_extractor, dataset, batch_size=128, device=torch.device(\"cuda\")):\n",
        "    \"\"\"Pre-extract all features once and cache them\"\"\"\n",
        "    feature_extractor.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                           num_workers=4, pin_memory=True)\n",
        "\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_batch, label_batch in dataloader:\n",
        "            img_batch = img_batch.to(device)\n",
        "            features = feature_extractor(img_batch).view(img_batch.size(0), -1)\n",
        "\n",
        "            all_features.append(features.cpu())  # Move to CPU to save GPU memory\n",
        "            all_labels.append(label_batch)\n",
        "\n",
        "    return torch.cat(all_features, dim=0), torch.cat(all_labels, dim=0)\n",
        "\n",
        "def evaluate_model(feature_extractor: torch.nn.Module,\n",
        "                       all_seen_classes: List[int],\n",
        "                       train_full,\n",
        "                       val_full,\n",
        "                       base_transform,\n",
        "                       batch_size: int = 128,\n",
        "                       linear_eval_epochs: int = 100,\n",
        "                       device: torch.device = torch.device(\"cuda\")):\n",
        "\n",
        "    # Freeze feature extractor\n",
        "    feature_extractor.eval()\n",
        "    for param in feature_extractor.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Create datasets\n",
        "    train_linear_dataset = LinearEvalDataset(train_full, all_seen_classes, base_transform)\n",
        "    val_linear_dataset = LinearEvalDataset(val_full, all_seen_classes, base_transform)\n",
        "\n",
        "    # PRE-EXTRACT FEATURES ONCE\n",
        "    print(\"Extracting training features...\")\n",
        "    train_features, train_labels = extract_features_once(feature_extractor, train_linear_dataset, batch_size, device)\n",
        "    print(\"Extracting validation features...\")\n",
        "    val_features, val_labels = extract_features_once(feature_extractor, val_linear_dataset, batch_size, device)\n",
        "\n",
        "    # Map labels to contiguous range\n",
        "    label_to_contiguous_map = {label: i for i, label in enumerate(sorted(all_seen_classes))}\n",
        "    train_labels_mapped = torch.tensor([label_to_contiguous_map[l.item()] for l in train_labels])\n",
        "    val_labels_mapped = torch.tensor([label_to_contiguous_map[l.item()] for l in val_labels])\n",
        "\n",
        "    # Create feature datasets\n",
        "    train_feature_dataset = TensorDataset(train_features, train_labels_mapped)\n",
        "    val_feature_dataset = TensorDataset(val_features, val_labels_mapped)\n",
        "\n",
        "    train_loader = DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_feature_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize classifier\n",
        "    features_dim = train_features.shape[1]\n",
        "    num_output_classes = len(all_seen_classes)\n",
        "\n",
        "    linear_classifier = nn.Sequential(\n",
        "        nn.Linear(features_dim, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, num_output_classes)\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(linear_classifier.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=linear_eval_epochs)\n",
        "\n",
        "    # Train classifier (much faster now - no feature extraction)\n",
        "    linear_classifier.train()\n",
        "    for epoch in range(linear_eval_epochs):\n",
        "        for features_batch, labels_batch in train_loader:\n",
        "            features_batch = features_batch.to(device)\n",
        "            labels_batch = labels_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = linear_classifier(features_batch)\n",
        "            loss = criterion(outputs, labels_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluate\n",
        "    linear_classifier.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features_batch, labels_batch in val_loader:\n",
        "            features_batch = features_batch.to(device)\n",
        "            labels_batch = labels_batch.to(device)\n",
        "\n",
        "            outputs = linear_classifier(features_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_samples += labels_batch.size(0)\n",
        "            total_correct += (predicted == labels_batch).sum().item()\n",
        "\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    print(f\"Linear evaluation accuracy on {len(all_seen_classes)} classes: {accuracy:.2f}%\")\n",
        "\n",
        "    # Restore feature extractor\n",
        "    for param in feature_extractor.parameters():\n",
        "        param.requires_grad = True\n",
        "    feature_extractor.train()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Calculate all random baselines once at the beginning\n",
        "def calculate_all_random_baselines(task_class_splits, train_full, val_full, base_transform, device):\n",
        "    \"\"\"Calculate random baselines for all tasks once\"\"\"\n",
        "    random_accuracies = {}\n",
        "\n",
        "    # Create a single random model for all evaluations\n",
        "    random_model = models.resnet18(weights=None)\n",
        "    random_model.fc = nn.Identity()\n",
        "    random_model.to(device)\n",
        "    random_model.eval()\n",
        "\n",
        "    try:\n",
        "        for task_idx, class_list in enumerate(task_class_splits):\n",
        "            print(f\"Calculating random baseline for Task {task_idx+1}\")\n",
        "            random_acc = evaluate_model(\n",
        "                random_model, class_list, train_full, val_full, base_transform,\n",
        "                batch_size=128, linear_eval_epochs=5, device=device  # Fewer epochs for random\n",
        "            )\n",
        "            random_accuracies[task_idx] = random_acc\n",
        "    finally:\n",
        "        del random_model\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return random_accuracies\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    print(f\"Random seed set to {seed}\")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "# Transforms\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize(224),          \n",
        "    transforms.CenterCrop(224),       \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Task Split\n",
        "#all_classes_shuffled = list(range(NUM_TOTAL_CLASSES))\n",
        "#random.shuffle(all_classes_shuffled)\n",
        "\n",
        "#task_class_splits = [all_classes_shuffled[i:i + NUM_CLASSES_PER_TASK] for i in range(0, NUM_TOTAL_CLASSES, NUM_CLASSES_PER_TASK)]\n",
        "task_class_splits = [\n",
        "    [42, 41, 91, 9, 65, 50, 1, 70, 15, 78],\n",
        "    [73, 10, 55, 56, 72, 45, 48, 92, 76, 37],\n",
        "    [30, 21, 32, 96, 80, 49, 83, 26, 87, 33],\n",
        "    [8, 47, 59, 63, 74, 44, 98, 52, 85, 12],\n",
        "    [36, 23, 39, 40, 18, 66, 61, 60, 7, 34],\n",
        "    [99, 46, 2, 51, 16, 38, 58, 68, 22, 62],\n",
        "    [24, 5, 6, 67, 82, 19, 79, 43, 90, 20],\n",
        "    [0, 95, 57, 93, 53, 89, 25, 71, 84, 77],\n",
        "    [64, 29, 27, 88, 97, 4, 54, 75, 11, 69],\n",
        "    [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
        "]\n",
        "task_datasets = []\n",
        "for i, class_list in enumerate(task_class_splits):\n",
        "    print(f\"Task {i+1} includes classes: {class_list}\")\n",
        "    task_dataset = RotNetImageNet100Dataset(train_full, class_list, base_transform)\n",
        "    task_datasets.append(task_dataset)\n",
        "\n",
        "# Init Model\n",
        "resnet18_backbone = models.resnet18(weights=None)\n",
        "resnet18_backbone.fc = nn.Identity()\n",
        "\n",
        "# Initialize RotNet with pretrained ResNet18\n",
        "base_ssl_model_instance = RotNetModel(\n",
        "    num_rot_classes=NUM_ROT_CLASSES,\n",
        "    backbone='places_resnet18'  # Add this parameter to RotNetModel\n",
        ").to(DEVICE)\n",
        "prev_encoder_state_dict = None\n",
        "\n",
        "# Training Loop\n",
        "all_task_accuracies = []\n",
        "random_accuracies_Ri = {}\n",
        "\n",
        "print(\"Calculating random baselines for all tasks...\")\n",
        "random_accuracies_Ri = calculate_all_random_baselines(\n",
        "    task_class_splits, train_full, val_full, base_transform, DEVICE\n",
        ")\n",
        "print(\"Random baselines calculated!\")\n",
        "\n",
        "# STEP 2: Modified training loop\n",
        "all_task_accuracies = []\n",
        "\n",
        "for task_id, current_task_dataset in enumerate(task_datasets):\n",
        "    print(f\"\\n===== Training Task {task_id + 1}/{len(task_datasets)} =====\")\n",
        "\n",
        "    current_task_loader = DataLoader(\n",
        "        current_task_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=2, pin_memory=True,\n",
        "        prefetch_factor=2\n",
        "    )\n",
        "\n",
        "    trainer = CaSSleTrainer(\n",
        "        base_ssl_model=base_ssl_model_instance,\n",
        "        ca_predictor_hidden_dim=1024,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        lambda_cassle=LAMBDA_CASSLE,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    # Save previous encoder\n",
        "    if prev_encoder_state_dict:\n",
        "        trainer.set_previous_frozen_encoder(prev_encoder_state_dict)\n",
        "\n",
        "    # Train encoder on current task and save its state_dict\n",
        "    prev_encoder_state_dict = trainer.train_task(current_task_loader, NUM_EPOCHS_PER_TASK)\n",
        "\n",
        "    print(f\"\\n--- Evaluating after Task {task_id + 1} ---\")\n",
        "    current_seen_classes = sorted(set().union(*task_class_splits[:task_id + 1]))\n",
        "    accuracies_after_this_task = []\n",
        "\n",
        "    # Evaluate model for each task seen so far\n",
        "    for eval_task_idx in range(task_id + 1):\n",
        "        eval_task_classes = task_class_splits[eval_task_idx]\n",
        "        print(f\"  Evaluating on classes from Task {eval_task_idx+1}: {eval_task_classes}\")\n",
        "\n",
        "        # USE THE FAST EVALUATION FUNCTION\n",
        "        acc_jk = evaluate_model(  # Changed from evaluate_model to evaluate_model_fast\n",
        "            base_ssl_model_instance.backbone,\n",
        "            eval_task_classes,\n",
        "            train_full,\n",
        "            val_full,\n",
        "            base_transform,\n",
        "            LINEAR_EVAL_BATCH_SIZE,\n",
        "            LINEAR_EVAL_EPOCHS,  # Consider reducing this to 10-20\n",
        "            DEVICE\n",
        "        )\n",
        "        accuracies_after_this_task.append(acc_jk)\n",
        "\n",
        "        # Random baseline is already calculated - just print it\n",
        "        random_acc = random_accuracies_Ri[eval_task_idx]\n",
        "        print(f\"    Random baseline for Task {eval_task_idx+1}: {random_acc:.2f}%\")\n",
        "        print(f\"    Current accuracy for Task {eval_task_idx+1}: {acc_jk:.2f}%\")\n",
        "        print(f\"    Improvement over random: {acc_jk - random_acc:.2f}%\")\n",
        "\n",
        "    all_task_accuracies.append(accuracies_after_this_task)\n",
        "\n",
        "    # Optional: Clear GPU cache after each task\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Rest of your metrics calculation remains the same\n",
        "T = len(task_datasets)\n",
        "\n",
        "# Average Accuracy\n",
        "final_accuracies_row = all_task_accuracies[T-1]\n",
        "avg_accuracy = sum(final_accuracies_row) / T\n",
        "print(f\"\\nFinal Average Accuracy (A): {avg_accuracy:.2f}%\")\n",
        "\n",
        "# Calculate average random baseline for comparison\n",
        "avg_random_accuracy = sum(random_accuracies_Ri.values()) / len(random_accuracies_Ri)\n",
        "print(f\"Average Random Baseline: {avg_random_accuracy:.2f}%\")\n",
        "print(f\"Improvement over Random: {avg_accuracy - avg_random_accuracy:.2f}%\")\n",
        "\n",
        "# Forgetting calculation (unchanged)\n",
        "forgetting = 0\n",
        "if T > 1:\n",
        "    for i in range(T - 1):\n",
        "        max_acc = max(all_task_accuracies[t][i] for t in range(T) if i < len(all_task_accuracies[t]))\n",
        "        final_acc = all_task_accuracies[T-1][i]\n",
        "        forgetting += (max_acc - final_acc)\n",
        "    forgetting /= (T - 1)\n",
        "print(f\"Final Forgetting (F): {forgetting:.2f}%\")\n",
        "\n",
        "# Backward Transfer calculation (unchanged)\n",
        "backward_transfer = 0\n",
        "count = 0\n",
        "\n",
        "if T > 1:\n",
        "    for new_task in range(1, T):\n",
        "        for old_task in range(new_task):\n",
        "            if old_task < len(all_task_accuracies[new_task - 1]) and old_task < len(all_task_accuracies[new_task]):\n",
        "                acc_before = all_task_accuracies[new_task - 1][old_task]\n",
        "                acc_after = all_task_accuracies[new_task][old_task]\n",
        "                backward_transfer += (acc_after - acc_before)\n",
        "                count += 1\n",
        "            else:\n",
        "                print(f\"Skipping BT for old_task {old_task+1}, new_task {new_task+1}: missing data\")\n",
        "\n",
        "    backward_transfer /= count if count > 0 else 1\n",
        "else:\n",
        "    backward_transfer = 0\n",
        "\n",
        "print(f\"Final Backward Transfer (BT): {backward_transfer:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
