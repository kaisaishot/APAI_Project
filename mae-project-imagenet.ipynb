{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bcee25",
   "metadata": {
    "id": "_ZQ0XzSFS2BC",
    "papermill": {
     "duration": 0.007637,
     "end_time": "2025-07-02T10:40:15.947869",
     "exception": false,
     "start_time": "2025-07-02T10:40:15.940232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "CaSSLe : prepare the base ssl model then divide the data set into tasks and then train the model on each task while saving encoder (frozen) after each task\n",
    "during training Train the current model (student) using two losses\n",
    "\n",
    "training loop for task t(After learning task t-1):\n",
    "  \n",
    "\n",
    "*   save a copy of the models backbone and prediction/projection head (ft-1 ,gt-1)\n",
    "*   alongside the cassle predictor ht-1 from previous task\n",
    "\n",
    "\n",
    "*   your main backbone is the same from previous task\n",
    "\n",
    "*  the cassle predictor ht is reinitialized\n",
    "\n",
    "*     Apply two different augmentations(e.g.,v1,v2) to each image, just like standard SSL.\n",
    "\n",
    "*     Take the representation from one of the augmented views of the current imagePass it through the current CaSSLe predictor network ht (p1)\n",
    "\n",
    "\n",
    "*     Pass the same representation through the frozen previous task's network and its predictor (q1)\n",
    "\n",
    "\n",
    "*     compute CaSSLe Distillation Loss typically a cross-entropy or MSE loss\n",
    "      between q1 adn p1\n",
    "\n",
    "\n",
    "*     compute the ssl loss for current task using z1 and z2\n",
    "      combined loss Ltotal\n",
    "\n",
    "*    The entire current mode (ft,ht,gt) is updated using backpropagation on\n",
    "     Ltotal\n",
    "\n",
    "\n",
    "*The main representation learning components (f and g) are carried over.\n",
    "The CaSSLe predictor from the previous task is saved and frozen as a teacher.\n",
    "A new CaSSLe predictor is re-initialized for the current task and trained.*\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff855ab",
   "metadata": {
    "id": "q7K-KM05Doay",
    "papermill": {
     "duration": 0.005837,
     "end_time": "2025-07-02T10:40:15.960111",
     "exception": false,
     "start_time": "2025-07-02T10:40:15.954274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "#**MAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cc2884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:47:42.574822Z",
     "iopub.status.busy": "2025-07-26T13:47:42.574493Z",
     "iopub.status.idle": "2025-07-26T13:48:12.280312Z",
     "shell.execute_reply": "2025-07-26T13:48:12.279577Z",
     "shell.execute_reply.started": "2025-07-26T13:47:42.574797Z"
    },
    "id": "50Eg3HXjLmJM",
    "papermill": {
     "duration": 35.568353,
     "end_time": "2025-07-02T10:40:51.534677",
     "exception": false,
     "start_time": "2025-07-02T10:40:15.966324",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 13:47:58.685072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753537678.948623      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753537679.037238      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTMAEForPreTraining\n",
    "from transformers import ViTMAEConfig\n",
    "from transformers import AutoImageProcessor\n",
    "from typing import List, Dict, Any\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import ViTConfig\n",
    "from transformers import ViTMAEModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e8cff",
   "metadata": {
    "id": "20hBi_npi1sA",
    "papermill": {
     "duration": 0.005925,
     "end_time": "2025-07-02T10:40:51.547601",
     "exception": false,
     "start_time": "2025-07-02T10:40:51.541676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "mae finetune on class 1\n",
    "freeze mae1 and train mae1 on class 2 using ssl loss from mae1 with two augmentations from class 2 and ssl loss from frozen mae1 and g(z) of one aumentatio\n",
    "then save the trained mae1 as mae2\n",
    "etc..\n",
    "so we use the output of the decoder for the ssl loss and use the output of the encoder for the distillation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defe8834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:48:14.618354Z",
     "iopub.status.busy": "2025-07-26T13:48:14.618062Z",
     "iopub.status.idle": "2025-07-26T13:48:14.641920Z",
     "shell.execute_reply": "2025-07-26T13:48:14.640959Z",
     "shell.execute_reply.started": "2025-07-26T13:48:14.618331Z"
    },
    "id": "3tKUeDfQpMgX",
    "papermill": {
     "duration": 0.03879,
     "end_time": "2025-07-02T10:40:51.592661",
     "exception": false,
     "start_time": "2025-07-02T10:40:51.553871",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Custom Dataset for Two Augmentations and Task Filtering\n",
    "class CustomImageNet100TaskDataset(Dataset):\n",
    "    def __init__(self, imagenet_dataset, class_list: List[int], transform_v1, transform_v2, processor):\n",
    "        self.imagenet_dataset = imagenet_dataset\n",
    "        self.class_set = set(class_list)\n",
    "        self.valid_indices = []\n",
    "        self.processor = processor\n",
    "        \n",
    "        for i, (_, label) in enumerate(imagenet_dataset.imgs):\n",
    "            if label in self.class_set:\n",
    "                self.valid_indices.append(i)\n",
    "        \n",
    "        # Make sure transforms don't include ToTensor() or normalization\n",
    "        # as the processor will handle those\n",
    "        self.transform_v1 = transform_v1\n",
    "        self.transform_v2 = transform_v2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        img_path, original_label = self.imagenet_dataset.imgs[real_idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms that keep the image as PIL Image\n",
    "        \n",
    "        img_v1 = self.transform_v1(img)\n",
    "        img_v2 = self.transform_v2(img)\n",
    "        \n",
    "        # Then apply processor to convert to tensor and normalize\n",
    "        inputs_v1 = self.processor(images=img_v1, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        inputs_v2 = self.processor(images=img_v2, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        \n",
    "        return inputs_v1, inputs_v2, original_label\n",
    "#CaSSLe predictor g(z)\n",
    "class CaSSLePredictor(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Add BatchNorm\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Add Dropout for regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),  # Additional layer\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class MAECaSSLeModel(nn.Module):\n",
    "    def __init__(self, mae_backbone: ViTMAEForPreTraining):\n",
    "        super().__init__()\n",
    "        self.mae_backbone = mae_backbone\n",
    "        # The output dimension of the MAE encoder (hidden_size in config)\n",
    "        self.features_for_h_dim = mae_backbone.config.hidden_size\n",
    "        #Freezing the decoder\n",
    "        for param in self.mae_backbone.decoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, img_v1: torch.Tensor, img_v2: torch.Tensor) -> Dict[str, Any]:\n",
    "\n",
    "        # Ensure images have a batch dimension for the MAE model\n",
    "        # Using output_hidden_states=True to get encoder outputs\n",
    "        outputs_v1 = self.mae_backbone(pixel_values=img_v1, return_dict=True, output_hidden_states=True)\n",
    "        outputs_v2 = self.mae_backbone(pixel_values=img_v2, return_dict=True, output_hidden_states=True)\n",
    "\n",
    "        # For MAE, the last_hidden_state is (batch_size, sequence_length, hidden_size).\n",
    "        # We average over the sequence length (patches + CLS token) for a single feature vector per image.\n",
    "        features_for_h_v1 = outputs_v1.hidden_states[-1].mean(dim=1)\n",
    "        features_for_h_v2 = outputs_v2.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "        # MAE models often compute the reconstruction loss internally.\n",
    "        ssl_loss_components_v1 = {'internal_mae_loss': outputs_v1.loss}\n",
    "        ssl_loss_components_v2 = {'internal_mae_loss': outputs_v2.loss}\n",
    "\n",
    "        return {\n",
    "            'features_for_h_v1': features_for_h_v1,\n",
    "            'features_for_h_v2': features_for_h_v2,\n",
    "            'ssl_loss_components_v1': ssl_loss_components_v1,\n",
    "            'ssl_loss_components_v2': ssl_loss_components_v2\n",
    "        }\n",
    "\n",
    "    def calculate_ssl_loss(self, ssl_loss_components: Dict[str, Any]) -> torch.Tensor:\n",
    "\n",
    "        #Extracts and returns the MAE's internal reconstruction loss.\n",
    "        return ssl_loss_components['internal_mae_loss']\n",
    "\n",
    "    def get_learnable_params(self) -> List[dict]:\n",
    "        #Returns the parameters of the entire MAE backbone\n",
    "        return [{\"params\": self.mae_backbone.parameters()}]\n",
    "\n",
    "\n",
    "#training process for each task, handling the current MAE model,\n",
    "#the frozen previous MAE model, the g predictor, and the two loss terms (L_SSL and L_D).\n",
    "class CaSSLeTrainer:\n",
    "    def __init__(self, base_ssl_model: MAECaSSLeModel,\n",
    "                 ca_predictor_hidden_dim: int,\n",
    "                 learning_rate: float = 1e-4, lambda_cassle: float =1.0, device: str = 'cuda'):\n",
    "\n",
    "        self.base_ssl_model = base_ssl_model.to(device) # This is f_t\n",
    "        self.lambda_cassle = lambda_cassle\n",
    "        self.device = device\n",
    "\n",
    "        # Input and output dimensions for CaSSLe Predictor (g) are the MAE encoder's feature dimension\n",
    "        predictor_g_input_output_dim = self.base_ssl_model.features_for_h_dim\n",
    "\n",
    "        \n",
    "        # Initialize the current CaSSLe predictor (g from the paper)\n",
    "        # It's newly initialized for each task\n",
    "        self.g_current = CaSSLePredictor(\n",
    "            predictor_g_input_output_dim,\n",
    "            ca_predictor_hidden_dim,\n",
    "            predictor_g_input_output_dim\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizer for ALL trainable parameters: current MAE model (f_t) AND predictor g\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "          self.base_ssl_model.get_learnable_params() + [{\"params\": self.g_current.parameters()}],\n",
    "          lr=learning_rate\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=50, eta_min=1e-6\n",
    "        )\n",
    "\n",
    "        # This will hold the frozen previous encoder (f_{t-1}^{frozen})\n",
    "        self.f_frozen_teacher = None\n",
    "\n",
    "    def set_previous_frozen_encoder(self, encoder_state_dict: Dict[str, Any]):\n",
    "\n",
    "        # Re-instantiate the MAE model to load the full state dictionary\n",
    "        self.f_frozen_teacher = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base').to(self.device)\n",
    "        self.f_frozen_teacher.load_state_dict(encoder_state_dict)\n",
    "\n",
    "        # set requires_grad to False for all parameters of the teacher model\n",
    "        for param in self.f_frozen_teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        print(f\"Frozen encoder (f_t-1) loaded and parameters frozen: {all(not p.requires_grad for p in self.f_frozen_teacher.parameters())}\")\n",
    "\n",
    "\n",
    "    def train_task(self, data_loader: torch.utils.data.DataLoader, epochs: int):\n",
    "\n",
    "        self.base_ssl_model.train()\n",
    "        self.g_current.train()\n",
    "        scaler = torch.amp.GradScaler('cuda')  # Initialize GradScaler for AMP\n",
    "        # Set f_frozen_teacher to eval mode to disable updates for teacher\n",
    "        if self.f_frozen_teacher:\n",
    "            self.f_frozen_teacher.eval()\n",
    "\n",
    "        print(f\"Distilling from frozen teacher encoder (f_t-1): {self.f_frozen_teacher is not None}\")\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience = 4\n",
    "        patience_counter = 0\n",
    "        min_delta = 0.001\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_ssl_loss = 0\n",
    "            total_cassle_loss = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_idx, (img_v1, img_v2, _) in enumerate(data_loader): # _ for labels\n",
    "                self.optimizer.zero_grad()\n",
    "                img_v1 = img_v1.to(self.device)\n",
    "                img_v2 = img_v2.to(self.device)\n",
    "\n",
    "                \n",
    "                self.scaler = GradScaler()\n",
    "                with autocast():\n",
    "                  # Forward Pass through the current trainable MAE model (f_t)\n",
    "                  # This returns features (z_A, z_B) and components for L_SSL\n",
    "                  ssl_output = self.base_ssl_model(img_v1, img_v2)\n",
    "\n",
    "                  # Extract features for CaSSLe predictor and APPLY L2 NORMALIZATION HERE\n",
    "                  features_for_h_v1_norm = F.normalize(ssl_output['features_for_h_v1'], dim=-1)\n",
    "                  features_for_h_v2_norm = F.normalize(ssl_output['features_for_h_v2'], dim=-1)\n",
    "\n",
    "                  # Calculate Base Self-Supervised Loss (L_SSL - MAE Reconstruction Loss)\n",
    "                  # Sum MAE reconstruction loss for both views\n",
    "                  loss_ssl = self.base_ssl_model.calculate_ssl_loss(ssl_output['ssl_loss_components_v1']) + \\\n",
    "                            self.base_ssl_model.calculate_ssl_loss(ssl_output['ssl_loss_components_v2'])\n",
    "\n",
    "                  # Calculate CaSSLe Distillation Loss (L_D)\n",
    "                  loss_cassle = torch.tensor(0.0).to(self.device) # Initialize to 0 for the first task\n",
    "\n",
    "                  if self.f_frozen_teacher:\n",
    "                      # Get features from the frozen previous encoder\n",
    "\n",
    "                      with torch.no_grad():\n",
    "                        # Get encoder outputs only (bypass the decoder)\n",
    "                        outputs_v1 = self.f_frozen_teacher.vit(img_v1, output_hidden_states=True)\n",
    "                        outputs_v1_features= outputs_v1.hidden_states[-1].mean(dim=1)\n",
    "                        features_from_frozen_v1 = F.normalize(outputs_v1_features, dim=-1)\n",
    "\n",
    "                        outputs_v2 = self.f_frozen_teacher.vit(img_v2, output_hidden_states=True)\n",
    "                        outputs_v2_features= outputs_v2.hidden_states[-1].mean(dim=1)\n",
    "                        features_from_frozen_v2 = F.normalize(outputs_v2_features, dim=-1)\n",
    "\n",
    "                      # Student predictions from current trainable g\n",
    "                      # g takes features from current f_t\n",
    "                      student_pred_v1 = self.g_current(features_for_h_v1_norm)\n",
    "                      student_pred_v2 = self.g_current(features_for_h_v2_norm)\n",
    "\n",
    "                      # Teacher targets (from frozen f_t-1)\n",
    "                      teacher_target_v1 = features_from_frozen_v1\n",
    "                      teacher_target_v2 = features_from_frozen_v2\n",
    "\n",
    "                      # Compute distillation loss using MSE\n",
    "                      loss_cassle_v1 = 1 - F.cosine_similarity(student_pred_v1, teacher_target_v1, dim=-1).mean()\n",
    "                      loss_cassle_v2 = 1 - F.cosine_similarity(student_pred_v2, teacher_target_v2 ,dim=-1).mean()\n",
    "                      loss_cassle = (loss_cassle_v1 + loss_cassle_v2 )* self.lambda_cassle\n",
    "\n",
    "                  # Total Loss and backpropagation\n",
    "                  loss = loss_ssl +  loss_cassle\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "\n",
    "                total_ssl_loss += loss_ssl.item()\n",
    "                total_cassle_loss += loss_cassle.item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            #Early stopping\n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "            self.scheduler.step()\n",
    "            if avg_loss < best_loss - min_delta:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - SSL Loss: {total_ssl_loss / len(data_loader):.4f}, \"\n",
    "                  f\"CaSSLe Loss: {total_cassle_loss / len(data_loader):.4f}, \"\n",
    "                  f\"Total Loss: {total_loss / len(data_loader):.4f}\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # After training the task, return the state_dict of the current MAE model (f_t)\n",
    "        return self.base_ssl_model.mae_backbone.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841c1e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T13:48:19.375984Z",
     "iopub.status.busy": "2025-07-26T13:48:19.375694Z",
     "iopub.status.idle": "2025-07-26T13:48:20.413424Z",
     "shell.execute_reply": "2025-07-26T13:48:20.412626Z",
     "shell.execute_reply.started": "2025-07-26T13:48:19.375960Z"
    },
    "id": "iB7DIj_OsSBt",
    "papermill": {
     "duration": 0.027817,
     "end_time": "2025-07-02T10:40:51.626943",
     "exception": false,
     "start_time": "2025-07-02T10:40:51.599126",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "class LinearEvalDataset(Dataset):\n",
    "    \"\"\"Pre-filter indices for better performance\"\"\"\n",
    "    def __init__(self, original_dataset, class_list, transform):\n",
    "        if hasattr(original_dataset, 'targets'):\n",
    "            targets = original_dataset.targets\n",
    "        else:\n",
    "            targets = [label for _, label in original_dataset]\n",
    "        \n",
    "        # Pre-filter all indices at once\n",
    "        self.indices = [i for i, label in enumerate(targets) if label in class_list]\n",
    "        self.original_dataset = original_dataset\n",
    "        self.transform = transform\n",
    "        print(f\"Filtered dataset: {len(self.indices)} samples from {len(class_list)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.original_dataset[self.indices[idx]]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "def extract_features_once(feature_extractor, dataset, batch_size, device):\n",
    "    \"\"\"\n",
    "    Fixed version that handles BatchFeature objects properly.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4,drop_last=True)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch_data in dataloader:\n",
    "            if len(batch_data) == 3:\n",
    "                img_batch, _, label_batch = batch_data\n",
    "            elif len(batch_data) == 2:\n",
    "                img_batch, label_batch = batch_data\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected batch format: {len(batch_data)} elements\")\n",
    "    \n",
    "            # Handle HuggingFace BatchFeature or regular tensors\n",
    "            if hasattr(img_batch, 'pixel_values'):\n",
    "                pixel_values = img_batch.pixel_values\n",
    "                if isinstance(pixel_values, list):\n",
    "                    if isinstance(pixel_values[0], torch.Tensor):\n",
    "                        img_tensor = torch.stack(pixel_values).to(device)  # ✅ stack list of tensors\n",
    "                    else:\n",
    "                        raise TypeError(\"pixel_values list items are not tensors.\")\n",
    "                else:\n",
    "                    img_tensor = pixel_values.to(device)\n",
    "            elif isinstance(img_batch, torch.Tensor):\n",
    "                img_tensor = img_batch.to(device)\n",
    "            elif isinstance(img_batch, dict) and 'pixel_values' in img_batch:\n",
    "                pixel_values = img_batch['pixel_values']\n",
    "                if isinstance(pixel_values, list):\n",
    "                    img_tensor = torch.stack(pixel_values).to(device)\n",
    "                else:\n",
    "                    img_tensor = pixel_values.to(device)\n",
    "            else:\n",
    "                raise TypeError(f\"Cannot handle batch type: {type(img_batch)}\")\n",
    "    \n",
    "            # Ensure tensor has shape [batch_size, 3, H, W]\n",
    "            if img_tensor.dim() == 5:\n",
    "                img_tensor = img_tensor.squeeze(0)  # remove extra batch dimension\n",
    "    \n",
    "            # Extract features\n",
    "            output = feature_extractor(img_tensor, output_hidden_states=True)\n",
    "            features = output.hidden_states[-1][:, 0]  # CLS token\n",
    "            \n",
    "            all_features.append(features.cpu())  # Move to CPU to save GPU memory\n",
    "            all_labels.append(label_batch)\n",
    "    \n",
    "    # Concatenate all features and labels\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_features, all_labels\n",
    "\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(feature_extractor: torch.nn.Module,\n",
    "                   all_seen_classes: List[int],\n",
    "                   train_full, # Full CIFAR100 train dataset\n",
    "                   val_full,  # Full CIFAR100 test dataset\n",
    "                   processor, # HuggingFace processor\n",
    "                   batch_size: int = 128,\n",
    "                   linear_eval_epochs: int = 15,\n",
    "                   device: torch.device = torch.device(\"cuda\")):\n",
    "\n",
    "    # Put feature extractor is in eval mode and frozen\n",
    "    feature_extractor.eval()\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Prepare Data for Linear Classifier Training\n",
    "    # Filter training data to include only classes seen so far\n",
    "    train_linear_dataset = LinearEvalDataset(train_full, all_seen_classes, processor)\n",
    "    val_linear_dataset = LinearEvalDataset(val_full, all_seen_classes, processor)\n",
    "\n",
    "    print(\"Extracting training features...\")\n",
    "    train_features, train_labels = extract_features_once(feature_extractor, train_linear_dataset, batch_size, device)\n",
    "    print(\"Extracting validation features...\")\n",
    "    val_features, val_labels = extract_features_once(feature_extractor, val_linear_dataset, batch_size, device)\n",
    "\n",
    "\n",
    "    # Map labels to contiguous range\n",
    "    label_to_contiguous_map = {label: i for i, label in enumerate(sorted(all_seen_classes))}\n",
    "    train_labels_mapped = torch.tensor([label_to_contiguous_map[l.item()] for l in train_labels])\n",
    "    val_labels_mapped = torch.tensor([label_to_contiguous_map[l.item()] for l in val_labels])\n",
    "    \n",
    "    # Create feature datasets\n",
    "    train_feature_dataset = TensorDataset(train_features, train_labels_mapped)\n",
    "    val_feature_dataset = TensorDataset(val_features, val_labels_mapped)\n",
    "    \n",
    "    train_loader = DataLoader(train_feature_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "    val_loader = DataLoader(val_feature_dataset, batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "    \n",
    "    # Initialize classifier\n",
    "    features_dim = train_features.shape[1]\n",
    "    num_output_classes = len(all_seen_classes)\n",
    "    \n",
    "    linear_classifier = nn.Sequential(\n",
    "        nn.Linear(features_dim, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_output_classes)\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(linear_classifier.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=linear_eval_epochs)\n",
    "    \n",
    "    # Train classifier (much faster now - no feature extraction)\n",
    "    linear_classifier.train()\n",
    "    for epoch in range(linear_eval_epochs):\n",
    "        for features_batch, labels_batch in train_loader:\n",
    "            features_batch = features_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = linear_classifier(features_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    linear_classifier.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features_batch, labels_batch in val_loader:\n",
    "            features_batch = features_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            outputs = linear_classifier(features_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels_batch.size(0)\n",
    "            total_correct += (predicted == labels_batch).sum().item()\n",
    "    \n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    print(f\"Linear evaluation accuracy on {len(all_seen_classes)} classes: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Restore feature extractor\n",
    "    for param in feature_extractor.parameters():\n",
    "        param.requires_grad = True\n",
    "    feature_extractor.train()\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "# Calculate all random baselines once at the beginning\n",
    "def calculate_all_random_baselines(task_class_splits, train_full, val_full, processor, device):\n",
    "    \"\"\"Calculate random baselines using a randomly initialized ViT-MAE\"\"\"\n",
    "    random_accuracies = {}\n",
    "\n",
    "    config = ViTMAEConfig()\n",
    "    random_model = ViTMAEModel(config)\n",
    "    random_model.to(device)\n",
    "    random_model.eval()\n",
    "\n",
    "    try:\n",
    "        for task_idx, class_list in enumerate(task_class_splits):\n",
    "            print(f\"Calculating random baseline for Task {task_idx+1}\")\n",
    "            random_acc = evaluate_model(\n",
    "                random_model, class_list, train_full, val_full, processor,\n",
    "                batch_size=128, linear_eval_epochs=5, device=device\n",
    "            )\n",
    "            random_accuracies[task_idx] = random_acc\n",
    "    finally:\n",
    "        del random_model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return random_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57051e8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "374ee98c27ce4cf5afd0b8ac1a103771",
      "65397a410c4b4e1e96239c8e754723a3",
      "f5bc2af6bfb142648d52ce3a44e989ab",
      "8122bdd0004545f6b0eaec4be58e92e8",
      "939ef0d76d314175a0fad8fc9372e43d",
      "91e282d5c03e469fb95bc657d9e8ec62",
      "afbfab04e1a7493b82b8279796218d78",
      "613eeed1f5b44b03b71c799446c3d4af",
      "caa63590a1d441198ddbd635c747c57e",
      "0cbf9ead6cb1447c8e536602c758419a",
      "ba62365bed54479fbfba1baf23381733",
      "0f19e579ef494840a3eab1351223618d",
      "2c05084d73814a3aada143c0ce17269d",
      "79143ef0bfab40d397f399efac7f7a3c",
      "45cd04d066604d478a7ed5214ed884c5",
      "cc22e146f14f49c683c61fd840f0c5d2",
      "da74f7b558704e1c95ffd28a029f5457",
      "e8a4d972837c420aa793bee20a6185c7",
      "c1057174e53c4f69ac6562b71d9c06e5",
      "a0f0ac4d67c94d67afe2b0839e407f50",
      "e875f645c9d648feab26cd3d7e408ff3",
      "174833fb5c61481baafdcee3143d42f0",
      "846dae2791684dd6ac5bef458fd9ed99",
      "ae5208bc04ac4a7ca9e11ab11db3308c",
      "d229166e14584fcab457470d5d25199e",
      "3516bbc7424c4cb6b69f5b73111a319b",
      "66a94539bc1c4015a8e44fac7f226a71",
      "45aa5b61f1ed42fd8aa5e1aa4dc2deab",
      "251c1268861e4c41bdbca3323c00603b",
      "eda38306c12f4a56a1558c17ed2b3aa5",
      "fc5727b6df6a4bc7be1a73ffb61db0c4",
      "84e755572250483ca693fc4af18c14bf",
      "ec5e3756b0fb4fdf803c6e4305e5d4e8"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-26T13:48:23.417383Z",
     "iopub.status.busy": "2025-07-26T13:48:23.416292Z"
    },
    "id": "YfKclZBO7qpu",
    "outputId": "4f5f483f-40fd-4473-ce8c-6ed81904c7c0",
    "papermill": {
     "duration": 4075.12951,
     "end_time": "2025-07-02T11:48:46.762859",
     "exception": false,
     "start_time": "2025-07-02T10:40:51.633349",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d586af74bb43279753497cf4c294b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778766b1de4d4cf38ab1091583ad5cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/448M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff66297d6c92491082157d239d3e61ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 includes classes: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Task 2 includes classes: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Task 3 includes classes: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
      "Task 4 includes classes: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
      "Task 5 includes classes: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
      "Task 6 includes classes: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
      "Task 7 includes classes: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
      "Task 8 includes classes: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
      "Task 9 includes classes: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
      "Task 10 includes classes: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
      "\n",
      "--- Starting CaSSLe Continual Training ---\n",
      "Running on device: cuda\n",
      "Calculating random baselines for all tasks...\n",
      "Calculating random baseline for Task 1\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 30.47%\n",
      "Calculating random baseline for Task 2\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 32.81%\n",
      "Calculating random baseline for Task 3\n",
      "Filtered dataset: 12821 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 21.35%\n",
      "Calculating random baseline for Task 4\n",
      "Filtered dataset: 12964 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 35.42%\n",
      "Calculating random baseline for Task 5\n",
      "Filtered dataset: 12454 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 28.65%\n",
      "Calculating random baseline for Task 6\n",
      "Filtered dataset: 12560 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 25.78%\n",
      "Calculating random baseline for Task 7\n",
      "Filtered dataset: 12849 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 30.99%\n",
      "Calculating random baseline for Task 8\n",
      "Filtered dataset: 12854 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 32.29%\n",
      "Calculating random baseline for Task 9\n",
      "Filtered dataset: 12678 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 35.68%\n",
      "Calculating random baseline for Task 10\n",
      "Filtered dataset: 11856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 25.00%\n",
      "Random baselines calculated!\n",
      "\n",
      "===== Training Task 1/10 =====\n",
      "Distilling from frozen teacher encoder (f_t-1): False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/108681772.py:174: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "/tmp/ipykernel_36/108681772.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - SSL Loss: 0.4027, CaSSLe Loss: 0.0000, Total Loss: 0.4027\n",
      "Epoch 2/10 - SSL Loss: 0.4032, CaSSLe Loss: 0.0000, Total Loss: 0.4032\n",
      "Epoch 3/10 - SSL Loss: 0.4008, CaSSLe Loss: 0.0000, Total Loss: 0.4008\n",
      "Epoch 4/10 - SSL Loss: 0.4062, CaSSLe Loss: 0.0000, Total Loss: 0.4062\n",
      "Epoch 5/10 - SSL Loss: 0.4053, CaSSLe Loss: 0.0000, Total Loss: 0.4053\n",
      "Epoch 6/10 - SSL Loss: 0.4022, CaSSLe Loss: 0.0000, Total Loss: 0.4022\n",
      "Early stopping triggered.\n",
      "\n",
      "--- Evaluating after Task 1 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 88.54%\n",
      "    Random baseline for Task 1: 30.47%\n",
      "    Current accuracy for Task 1: 88.54%\n",
      "    Improvement over random: 58.07%\n",
      "\n",
      "Current A_j,k matrix after Task 1:\n",
      "After Task 1: [88.54166666666667]\n",
      "\n",
      "===== Training Task 2/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n",
      "Epoch 1/10 - SSL Loss: 0.4298, CaSSLe Loss: 0.0000, Total Loss: 0.4298\n",
      "Epoch 2/10 - SSL Loss: 0.4266, CaSSLe Loss: 0.0000, Total Loss: 0.4266\n",
      "Epoch 3/10 - SSL Loss: 0.4223, CaSSLe Loss: 0.0000, Total Loss: 0.4223\n",
      "Epoch 4/10 - SSL Loss: 0.4250, CaSSLe Loss: 0.0000, Total Loss: 0.4250\n",
      "Epoch 5/10 - SSL Loss: 0.4281, CaSSLe Loss: 0.0000, Total Loss: 0.4281\n",
      "Epoch 6/10 - SSL Loss: 0.4254, CaSSLe Loss: 0.0000, Total Loss: 0.4254\n",
      "Early stopping triggered.\n",
      "\n",
      "--- Evaluating after Task 2 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.68%\n",
      "    Random baseline for Task 1: 30.47%\n",
      "    Current accuracy for Task 1: 85.68%\n",
      "    Improvement over random: 55.21%\n",
      "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 89.84%\n",
      "    Random baseline for Task 2: 32.81%\n",
      "    Current accuracy for Task 2: 89.84%\n",
      "    Improvement over random: 57.03%\n",
      "\n",
      "Current A_j,k matrix after Task 2:\n",
      "After Task 1: [88.54166666666667]\n",
      "After Task 2: [85.67708333333333, 89.84375]\n",
      "\n",
      "===== Training Task 3/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n",
      "Epoch 1/10 - SSL Loss: 0.3992, CaSSLe Loss: 0.0000, Total Loss: 0.3992\n",
      "Epoch 2/10 - SSL Loss: 0.3966, CaSSLe Loss: 0.0000, Total Loss: 0.3966\n",
      "Epoch 3/10 - SSL Loss: 0.3982, CaSSLe Loss: 0.0000, Total Loss: 0.3982\n",
      "Epoch 4/10 - SSL Loss: 0.3952, CaSSLe Loss: 0.0000, Total Loss: 0.3952\n",
      "Epoch 5/10 - SSL Loss: 0.3961, CaSSLe Loss: 0.0000, Total Loss: 0.3961\n",
      "Epoch 6/10 - SSL Loss: 0.3929, CaSSLe Loss: 0.0000, Total Loss: 0.3929\n",
      "Epoch 7/10 - SSL Loss: 0.3921, CaSSLe Loss: 0.0000, Total Loss: 0.3921\n",
      "Epoch 8/10 - SSL Loss: 0.3916, CaSSLe Loss: 0.0000, Total Loss: 0.3916\n",
      "Epoch 9/10 - SSL Loss: 0.3899, CaSSLe Loss: 0.0000, Total Loss: 0.3899\n",
      "Epoch 10/10 - SSL Loss: 0.3904, CaSSLe Loss: 0.0000, Total Loss: 0.3904\n",
      "\n",
      "--- Evaluating after Task 3 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.72%\n",
      "    Random baseline for Task 1: 30.47%\n",
      "    Current accuracy for Task 1: 86.72%\n",
      "    Improvement over random: 56.25%\n",
      "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 83.33%\n",
      "    Random baseline for Task 2: 32.81%\n",
      "    Current accuracy for Task 2: 83.33%\n",
      "    Improvement over random: 50.52%\n",
      "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
      "Filtered dataset: 12821 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.16%\n",
      "    Random baseline for Task 3: 21.35%\n",
      "    Current accuracy for Task 3: 85.16%\n",
      "    Improvement over random: 63.80%\n",
      "\n",
      "Current A_j,k matrix after Task 3:\n",
      "After Task 1: [88.54166666666667]\n",
      "After Task 2: [85.67708333333333, 89.84375]\n",
      "After Task 3: [86.71875, 83.33333333333333, 85.15625]\n",
      "\n",
      "===== Training Task 4/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n",
      "Epoch 1/10 - SSL Loss: 0.4465, CaSSLe Loss: 0.0000, Total Loss: 0.4465\n",
      "Epoch 2/10 - SSL Loss: 0.4433, CaSSLe Loss: 0.0000, Total Loss: 0.4433\n",
      "Epoch 3/10 - SSL Loss: 0.4446, CaSSLe Loss: 0.0000, Total Loss: 0.4446\n",
      "Epoch 4/10 - SSL Loss: 0.4402, CaSSLe Loss: 0.0000, Total Loss: 0.4402\n",
      "Epoch 5/10 - SSL Loss: 0.4412, CaSSLe Loss: 0.0000, Total Loss: 0.4412\n",
      "Epoch 6/10 - SSL Loss: 0.4395, CaSSLe Loss: 0.0000, Total Loss: 0.4395\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_SIZE = 224  #  image size\n",
    "BATCH_SIZE = 16  # 4 rotations × 16 = 66 processed images\n",
    "NUM_EPOCHS_PER_TASK = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "LAMBDA_CASSLE = 0 # Weight for CaSSle loss\n",
    "NUM_CLASSES_PER_TASK = 10\n",
    "NUM_TOTAL_CLASSES = 100  # Total  classes\n",
    "LINEAR_EVAL_EPOCHS = 5\n",
    "LINEAR_EVAL_BATCH_SIZE = 128\n",
    "from torchvision import models\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load pre-trained MAE-base model and processor\n",
    "mae_backbone = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base')\n",
    "mae_backbone.to(DEVICE)\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-base',use_fast=True)\n",
    "\n",
    "# Define the two augmentation pipelines\n",
    "transform_v1 = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.2, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "])\n",
    "\n",
    "transform_v2 = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.2, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)), # Kernel size must be odd\n",
    "])\n",
    "\n",
    "train_full = datasets.ImageFolder(root='/kaggle/input/imagenet100/ImageNet100/train')\n",
    "val_full = datasets.ImageFolder(root='/kaggle/input/imagenet100/ImageNet100/val')\n",
    "# Prepare Task Data\n",
    "# Divide Imagenet-100 into 10 tasks of 10 classes each\n",
    "task_class_splits = [\n",
    "    [42, 41, 91, 9, 65, 50, 1, 70, 15, 78],\n",
    "    [73, 10, 55, 56, 72, 45, 48, 92, 76, 37],\n",
    "    [30, 21, 32, 96, 80, 49, 83, 26, 87, 33],\n",
    "    [8, 47, 59, 63, 74, 44, 98, 52, 85, 12],\n",
    "    [36, 23, 39, 40, 18, 66, 61, 60, 7, 34],\n",
    "    [99, 46, 2, 51, 16, 38, 58, 68, 22, 62],\n",
    "    [24, 5, 6, 67, 82, 19, 79, 43, 90, 20],\n",
    "    [0, 95, 57, 93, 53, 89, 25, 71, 84, 77],\n",
    "    [64, 29, 27, 88, 97, 4, 54, 75, 11, 69],\n",
    "    [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
    "]\n",
    "task_datasets = []\n",
    "for i, class_list in enumerate(task_class_splits):\n",
    "    print(f\"Task {i+1} includes classes: {class_list}\")\n",
    "    task_dataset = CustomImageNet100TaskDataset(train_full, class_list,transform_v1 ,transform_v2,processor)\n",
    "    task_datasets.append(task_dataset)\n",
    "\n",
    "# Initialize CaSSLe Components\n",
    "# base_ssl_model_instance is the current model (f_t) that gets updated across tasks\n",
    "# It starts with the loaded pretrained MAE.\n",
    "base_ssl_model_instance = MAECaSSLeModel(mae_backbone=mae_backbone)\n",
    "\n",
    "# This will store the state_dict of the f_t-1 (previous encoder)\n",
    "prev_encoder_state_dict = None\n",
    "\n",
    "print(\"\\n--- Starting CaSSLe Continual Training ---\")\n",
    "print(f\"Running on device: {DEVICE}\")\n",
    "\n",
    "# Metrics storage A_j_k will be a list of lists (j tasks observed, k task evaluated on)\n",
    "# A_j_k[j_idx][k_idx]\n",
    "all_task_accuracies = [] # Stores accuracies A_{j,k}\n",
    "\n",
    "print(\"Calculating random baselines for all tasks...\")\n",
    "random_accuracies_Ri = calculate_all_random_baselines(\n",
    "    task_class_splits, train_full, val_full, processor, DEVICE\n",
    ")\n",
    "print(\"Random baselines calculated!\")\n",
    "\n",
    "\n",
    "# Continual Training Loop\n",
    "for task_id, current_task_dataset in enumerate(task_datasets):\n",
    "    print(f\"\\n===== Training Task {task_id + 1}/{len(task_datasets)} =====\")\n",
    "\n",
    "    current_task_loader = DataLoader(current_task_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                      num_workers=os.cpu_count() // 2 if os.cpu_count() else 0, pin_memory=True,drop_last=True)\n",
    "\n",
    "\n",
    "    # Pass the SAME base_ssl_model_instance, as this is the model we continue updating.\n",
    "    trainer = CaSSLeTrainer(\n",
    "        base_ssl_model=base_ssl_model_instance,\n",
    "        ca_predictor_hidden_dim=1024, # Example hidden dim for 'g'\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        lambda_cassle=LAMBDA_CASSLE,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # If it's not the first task, set up the frozen teacher (f_t-1)\n",
    "    if prev_encoder_state_dict:\n",
    "        trainer.set_previous_frozen_encoder(prev_encoder_state_dict)\n",
    "\n",
    "    # Train the current task and get the state of the trained f_t.\n",
    "    # This will be used as the f_t-1 for the next task.\n",
    "    prev_encoder_state_dict = trainer.train_task(current_task_loader, NUM_EPOCHS_PER_TASK)\n",
    "\n",
    "    print(f\"\\n--- Evaluating after Task {task_id + 1} ---\")\n",
    "\n",
    "    # Determine all classes seen up to this task\n",
    "    current_seen_classes = sorted(set().union(*task_class_splits[:task_id + 1]))\n",
    "    accuracies_after_this_task = []\n",
    "\n",
    "   \n",
    "\n",
    "    for eval_task_idx in range(task_id + 1):\n",
    "        # Classes for the specific evaluation task (k)\n",
    "        eval_task_classes = task_class_splits[eval_task_idx]\n",
    "        print(f\"  Evaluating on classes from Task {eval_task_idx+1}: {eval_task_classes}\")\n",
    "\n",
    "        # Perform linear evaluation on the classes specific to eval_task_idx\n",
    "        acc_jk = evaluate_model(\n",
    "            base_ssl_model_instance.mae_backbone, # The current encoder (f_t)\n",
    "            eval_task_classes, # Evaluate on only classes of task k\n",
    "            train_full,\n",
    "            val_full,\n",
    "            processor,\n",
    "            LINEAR_EVAL_BATCH_SIZE,\n",
    "            LINEAR_EVAL_EPOCHS,\n",
    "            DEVICE\n",
    "        )\n",
    "        accuracies_after_this_task.append(acc_jk)\n",
    "\n",
    "        # Calculate R_i for Forward Transfer only once per task\n",
    "        random_acc = random_accuracies_Ri[eval_task_idx]\n",
    "        print(f\"    Random baseline for Task {eval_task_idx+1}: {random_acc:.2f}%\")\n",
    "        print(f\"    Current accuracy for Task {eval_task_idx+1}: {acc_jk:.2f}%\")\n",
    "        print(f\"    Improvement over random: {acc_jk - random_acc:.2f}%\")\n",
    "\n",
    "    all_task_accuracies.append(accuracies_after_this_task)\n",
    "\n",
    "    # Print current state (optional)\n",
    "    print(f\"\\nCurrent A_j,k matrix after Task {task_id + 1}:\")\n",
    "    for j_idx, row in enumerate(all_task_accuracies):\n",
    "        print(f\"After Task {j_idx+1}: {row}\")\n",
    "\n",
    "\n",
    "print(\"\\n CaSSLe Continual Training Process Completed\")\n",
    "\n",
    "# Final Metric Calculation\n",
    "T = len(task_datasets)\n",
    "\n",
    "# Average Accuracy\n",
    "final_accuracies_row = all_task_accuracies[T-1]\n",
    "avg_accuracy = sum(final_accuracies_row) / T\n",
    "print(f\"\\nFinal Average Accuracy (A): {avg_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Forgetting calculation (unchanged)\n",
    "forgetting = 0\n",
    "if T > 1:\n",
    "    for i in range(T - 1):\n",
    "        max_acc = max(all_task_accuracies[t][i] for t in range(T) if i < len(all_task_accuracies[t]))\n",
    "        final_acc = all_task_accuracies[T-1][i]\n",
    "        forgetting += (max_acc - final_acc)\n",
    "    forgetting /= (T - 1)\n",
    "print(f\"Final Forgetting (F): {forgetting:.2f}%\")\n",
    "\n",
    "# Backward Transfer calculation (unchanged)\n",
    "backward_transfer = 0\n",
    "count = 0\n",
    "\n",
    "if T > 1:\n",
    "    for new_task in range(1, T):\n",
    "        for old_task in range(new_task):\n",
    "            if old_task < len(all_task_accuracies[new_task - 1]) and old_task < len(all_task_accuracies[new_task]):\n",
    "                acc_before = all_task_accuracies[new_task - 1][old_task]\n",
    "                acc_after = all_task_accuracies[new_task][old_task]\n",
    "                backward_transfer += (acc_after - acc_before)\n",
    "                count += 1\n",
    "            else:\n",
    "                print(f\"Skipping BT for old_task {old_task+1}, new_task {new_task+1}: missing data\")\n",
    "\n",
    "    backward_transfer /= count if count > 0 else 1\n",
    "else:\n",
    "    backward_transfer = 0\n",
    "\n",
    "print(f\"Final Backward Transfer (BT): {backward_transfer:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3542842-2e2d-4748-aa0c-ed83d4a1f652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:36:02.578345Z",
     "iopub.status.busy": "2025-07-22T01:36:02.577560Z",
     "iopub.status.idle": "2025-07-22T01:36:03.176806Z",
     "shell.execute_reply": "2025-07-22T01:36:03.176159Z",
     "shell.execute_reply.started": "2025-07-22T01:36:02.578319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(base_ssl_model_instance.mae_backbone.state_dict(), \"ft_task6_encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8606712a-7d62-4cd7-8207-1aaac72ef7ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T01:36:48.931818Z",
     "iopub.status.busy": "2025-07-22T01:36:48.931517Z",
     "iopub.status.idle": "2025-07-22T04:35:14.926274Z",
     "shell.execute_reply": "2025-07-22T04:35:14.925343Z",
     "shell.execute_reply.started": "2025-07-22T01:36:48.931798Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Resuming Training: Task 7/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/821681212.py:174: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "/tmp/ipykernel_36/821681212.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - SSL Loss: 0.4079, CaSSLe Loss: 0.1562, Total Loss: 0.5640\n",
      "Epoch 2/50 - SSL Loss: 0.4074, CaSSLe Loss: 0.0764, Total Loss: 0.4838\n",
      "Epoch 3/50 - SSL Loss: 0.4042, CaSSLe Loss: 0.0613, Total Loss: 0.4655\n",
      "Epoch 4/50 - SSL Loss: 0.4034, CaSSLe Loss: 0.0530, Total Loss: 0.4564\n",
      "Epoch 5/50 - SSL Loss: 0.4036, CaSSLe Loss: 0.0474, Total Loss: 0.4510\n",
      "Epoch 6/50 - SSL Loss: 0.4032, CaSSLe Loss: 0.0445, Total Loss: 0.4477\n",
      "Epoch 7/50 - SSL Loss: 0.4033, CaSSLe Loss: 0.0418, Total Loss: 0.4451\n",
      "Epoch 8/50 - SSL Loss: 0.4004, CaSSLe Loss: 0.0400, Total Loss: 0.4403\n",
      "Epoch 9/50 - SSL Loss: 0.3974, CaSSLe Loss: 0.0387, Total Loss: 0.4361\n",
      "Early stopping triggered.\n",
      "\n",
      "--- Evaluating after Task 7 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.80%\n",
      "    Random baseline for Task 1: 26.40%\n",
      "    Current accuracy for Task 1: 86.80%\n",
      "    Improvement over random: 60.40%\n",
      "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.60%\n",
      "    Random baseline for Task 2: 31.80%\n",
      "    Current accuracy for Task 2: 86.60%\n",
      "    Improvement over random: 54.80%\n",
      "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
      "Filtered dataset: 12821 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.80%\n",
      "    Random baseline for Task 3: 29.80%\n",
      "    Current accuracy for Task 3: 85.80%\n",
      "    Improvement over random: 56.00%\n",
      "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
      "Filtered dataset: 12964 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 91.20%\n",
      "    Random baseline for Task 4: 42.80%\n",
      "    Current accuracy for Task 4: 91.20%\n",
      "    Improvement over random: 48.40%\n",
      "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
      "Filtered dataset: 12454 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 88.40%\n",
      "    Random baseline for Task 5: 33.00%\n",
      "    Current accuracy for Task 5: 88.40%\n",
      "    Improvement over random: 55.40%\n",
      "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
      "Filtered dataset: 12560 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 90.20%\n",
      "    Random baseline for Task 6: 29.80%\n",
      "    Current accuracy for Task 6: 90.20%\n",
      "    Improvement over random: 60.40%\n",
      "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
      "Filtered dataset: 12849 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.80%\n",
      "    Random baseline for Task 7: 29.20%\n",
      "    Current accuracy for Task 7: 86.80%\n",
      "    Improvement over random: 57.60%\n",
      "\n",
      "Current A_j,k matrix after Task 7:\n",
      "After Task 1: [90.2]\n",
      "After Task 2: [88.4, 91.4]\n",
      "After Task 3: [89.2, 89.8, 87.8]\n",
      "After Task 4: [89.2, 88.8, 86.6, 94.6]\n",
      "After Task 5: [88.0, 89.4, 87.4, 93.8, 92.0]\n",
      "After Task 6: [86.0, 88.8, 85.2, 91.2, 89.8, 91.8]\n",
      "After Task 7: [86.8, 86.6, 85.8, 91.2, 88.4, 90.2, 86.8]\n",
      "\n",
      "===== Resuming Training: Task 8/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n",
      "Epoch 1/50 - SSL Loss: 0.4404, CaSSLe Loss: 0.1514, Total Loss: 0.5918\n",
      "Epoch 2/50 - SSL Loss: 0.4380, CaSSLe Loss: 0.0725, Total Loss: 0.5105\n",
      "Epoch 3/50 - SSL Loss: 0.4320, CaSSLe Loss: 0.0578, Total Loss: 0.4898\n",
      "Epoch 4/50 - SSL Loss: 0.4326, CaSSLe Loss: 0.0500, Total Loss: 0.4826\n",
      "Epoch 5/50 - SSL Loss: 0.4333, CaSSLe Loss: 0.0452, Total Loss: 0.4785\n",
      "Epoch 6/50 - SSL Loss: 0.4294, CaSSLe Loss: 0.0424, Total Loss: 0.4717\n",
      "Early stopping triggered.\n",
      "\n",
      "--- Evaluating after Task 8 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.80%\n",
      "    Random baseline for Task 1: 26.40%\n",
      "    Current accuracy for Task 1: 85.80%\n",
      "    Improvement over random: 59.40%\n",
      "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.00%\n",
      "    Random baseline for Task 2: 31.80%\n",
      "    Current accuracy for Task 2: 85.00%\n",
      "    Improvement over random: 53.20%\n",
      "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
      "Filtered dataset: 12821 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 84.00%\n",
      "    Random baseline for Task 3: 29.80%\n",
      "    Current accuracy for Task 3: 84.00%\n",
      "    Improvement over random: 54.20%\n",
      "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
      "Filtered dataset: 12964 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 91.00%\n",
      "    Random baseline for Task 4: 42.80%\n",
      "    Current accuracy for Task 4: 91.00%\n",
      "    Improvement over random: 48.20%\n",
      "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
      "Filtered dataset: 12454 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.80%\n",
      "    Random baseline for Task 5: 33.00%\n",
      "    Current accuracy for Task 5: 85.80%\n",
      "    Improvement over random: 52.80%\n",
      "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
      "Filtered dataset: 12560 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.40%\n",
      "    Random baseline for Task 6: 29.80%\n",
      "    Current accuracy for Task 6: 86.40%\n",
      "    Improvement over random: 56.60%\n",
      "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
      "Filtered dataset: 12849 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 83.60%\n",
      "    Random baseline for Task 7: 29.20%\n",
      "    Current accuracy for Task 7: 83.60%\n",
      "    Improvement over random: 54.40%\n",
      "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
      "Filtered dataset: 12854 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 88.20%\n",
      "    Random baseline for Task 8: 32.00%\n",
      "    Current accuracy for Task 8: 88.20%\n",
      "    Improvement over random: 56.20%\n",
      "\n",
      "Current A_j,k matrix after Task 8:\n",
      "After Task 1: [90.2]\n",
      "After Task 2: [88.4, 91.4]\n",
      "After Task 3: [89.2, 89.8, 87.8]\n",
      "After Task 4: [89.2, 88.8, 86.6, 94.6]\n",
      "After Task 5: [88.0, 89.4, 87.4, 93.8, 92.0]\n",
      "After Task 6: [86.0, 88.8, 85.2, 91.2, 89.8, 91.8]\n",
      "After Task 7: [86.8, 86.6, 85.8, 91.2, 88.4, 90.2, 86.8]\n",
      "After Task 8: [85.8, 85.0, 84.0, 91.0, 85.8, 86.4, 83.6, 88.2]\n",
      "\n",
      "===== Resuming Training: Task 9/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n",
      "Epoch 1/50 - SSL Loss: 0.4323, CaSSLe Loss: 0.1616, Total Loss: 0.5938\n",
      "Epoch 2/50 - SSL Loss: 0.4245, CaSSLe Loss: 0.0753, Total Loss: 0.4998\n",
      "Epoch 3/50 - SSL Loss: 0.4270, CaSSLe Loss: 0.0591, Total Loss: 0.4861\n",
      "Epoch 4/50 - SSL Loss: 0.4256, CaSSLe Loss: 0.0505, Total Loss: 0.4761\n",
      "Epoch 5/50 - SSL Loss: 0.4247, CaSSLe Loss: 0.0456, Total Loss: 0.4703\n",
      "Epoch 6/50 - SSL Loss: 0.4232, CaSSLe Loss: 0.0424, Total Loss: 0.4656\n",
      "Epoch 7/50 - SSL Loss: 0.4205, CaSSLe Loss: 0.0403, Total Loss: 0.4608\n",
      "Epoch 8/50 - SSL Loss: 0.4196, CaSSLe Loss: 0.0387, Total Loss: 0.4583\n",
      "Early stopping triggered.\n",
      "\n",
      "--- Evaluating after Task 9 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.00%\n",
      "    Random baseline for Task 1: 26.40%\n",
      "    Current accuracy for Task 1: 86.00%\n",
      "    Improvement over random: 59.60%\n",
      "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 87.00%\n",
      "    Random baseline for Task 2: 31.80%\n",
      "    Current accuracy for Task 2: 87.00%\n",
      "    Improvement over random: 55.20%\n",
      "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
      "Filtered dataset: 12821 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 84.60%\n",
      "    Random baseline for Task 3: 29.80%\n",
      "    Current accuracy for Task 3: 84.60%\n",
      "    Improvement over random: 54.80%\n",
      "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
      "Filtered dataset: 12964 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 89.80%\n",
      "    Random baseline for Task 4: 42.80%\n",
      "    Current accuracy for Task 4: 89.80%\n",
      "    Improvement over random: 47.00%\n",
      "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
      "Filtered dataset: 12454 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 88.60%\n",
      "    Random baseline for Task 5: 33.00%\n",
      "    Current accuracy for Task 5: 88.60%\n",
      "    Improvement over random: 55.60%\n",
      "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
      "Filtered dataset: 12560 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 89.40%\n",
      "    Random baseline for Task 6: 29.80%\n",
      "    Current accuracy for Task 6: 89.40%\n",
      "    Improvement over random: 59.60%\n",
      "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
      "Filtered dataset: 12849 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 84.20%\n",
      "    Random baseline for Task 7: 29.20%\n",
      "    Current accuracy for Task 7: 84.20%\n",
      "    Improvement over random: 55.00%\n",
      "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
      "Filtered dataset: 12854 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.40%\n",
      "    Random baseline for Task 8: 32.00%\n",
      "    Current accuracy for Task 8: 85.40%\n",
      "    Improvement over random: 53.40%\n",
      "  Evaluating on classes from Task 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
      "Filtered dataset: 12678 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 88.80%\n",
      "    Random baseline for Task 9: 34.40%\n",
      "    Current accuracy for Task 9: 88.80%\n",
      "    Improvement over random: 54.40%\n",
      "\n",
      "Current A_j,k matrix after Task 9:\n",
      "After Task 1: [90.2]\n",
      "After Task 2: [88.4, 91.4]\n",
      "After Task 3: [89.2, 89.8, 87.8]\n",
      "After Task 4: [89.2, 88.8, 86.6, 94.6]\n",
      "After Task 5: [88.0, 89.4, 87.4, 93.8, 92.0]\n",
      "After Task 6: [86.0, 88.8, 85.2, 91.2, 89.8, 91.8]\n",
      "After Task 7: [86.8, 86.6, 85.8, 91.2, 88.4, 90.2, 86.8]\n",
      "After Task 8: [85.8, 85.0, 84.0, 91.0, 85.8, 86.4, 83.6, 88.2]\n",
      "After Task 9: [86.0, 87.0, 84.6, 89.8, 88.6, 89.4, 84.2, 85.4, 88.8]\n",
      "\n",
      "===== Resuming Training: Task 10/10 =====\n",
      "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
      "Distilling from frozen teacher encoder (f_t-1): True\n",
      "Epoch 1/50 - SSL Loss: 0.4526, CaSSLe Loss: 0.1566, Total Loss: 0.6092\n",
      "Epoch 2/50 - SSL Loss: 0.4487, CaSSLe Loss: 0.0693, Total Loss: 0.5180\n",
      "Epoch 3/50 - SSL Loss: 0.4441, CaSSLe Loss: 0.0562, Total Loss: 0.5003\n",
      "Epoch 4/50 - SSL Loss: 0.4410, CaSSLe Loss: 0.0477, Total Loss: 0.4888\n",
      "Epoch 5/50 - SSL Loss: 0.4415, CaSSLe Loss: 0.0431, Total Loss: 0.4846\n",
      "Epoch 6/50 - SSL Loss: 0.4392, CaSSLe Loss: 0.0398, Total Loss: 0.4790\n",
      "Early stopping triggered.\n",
      "\n",
      "--- Evaluating after Task 10 ---\n",
      "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
      "Filtered dataset: 12797 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.80%\n",
      "    Random baseline for Task 1: 26.40%\n",
      "    Current accuracy for Task 1: 85.80%\n",
      "    Improvement over random: 59.40%\n",
      "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
      "Filtered dataset: 12856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 86.20%\n",
      "    Random baseline for Task 2: 31.80%\n",
      "    Current accuracy for Task 2: 86.20%\n",
      "    Improvement over random: 54.40%\n",
      "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
      "Filtered dataset: 12821 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 85.40%\n",
      "    Random baseline for Task 3: 29.80%\n",
      "    Current accuracy for Task 3: 85.40%\n",
      "    Improvement over random: 55.60%\n",
      "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
      "Filtered dataset: 12964 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 89.80%\n",
      "    Random baseline for Task 4: 42.80%\n",
      "    Current accuracy for Task 4: 89.80%\n",
      "    Improvement over random: 47.00%\n",
      "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
      "Filtered dataset: 12454 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 87.20%\n",
      "    Random baseline for Task 5: 33.00%\n",
      "    Current accuracy for Task 5: 87.20%\n",
      "    Improvement over random: 54.20%\n",
      "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
      "Filtered dataset: 12560 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 88.00%\n",
      "    Random baseline for Task 6: 29.80%\n",
      "    Current accuracy for Task 6: 88.00%\n",
      "    Improvement over random: 58.20%\n",
      "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
      "Filtered dataset: 12849 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 83.60%\n",
      "    Random baseline for Task 7: 29.20%\n",
      "    Current accuracy for Task 7: 83.60%\n",
      "    Improvement over random: 54.40%\n",
      "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
      "Filtered dataset: 12854 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 84.20%\n",
      "    Random baseline for Task 8: 32.00%\n",
      "    Current accuracy for Task 8: 84.20%\n",
      "    Improvement over random: 52.20%\n",
      "  Evaluating on classes from Task 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
      "Filtered dataset: 12678 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 87.00%\n",
      "    Random baseline for Task 9: 34.40%\n",
      "    Current accuracy for Task 9: 87.00%\n",
      "    Improvement over random: 52.60%\n",
      "  Evaluating on classes from Task 10: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
      "Filtered dataset: 11856 samples from 10 classes\n",
      "Filtered dataset: 500 samples from 10 classes\n",
      "Extracting training features...\n",
      "Extracting validation features...\n",
      "Linear evaluation accuracy on 10 classes: 84.60%\n",
      "    Random baseline for Task 10: 29.60%\n",
      "    Current accuracy for Task 10: 84.60%\n",
      "    Improvement over random: 55.00%\n",
      "\n",
      "Current A_j,k matrix after Task 10:\n",
      "After Task 1: [90.2]\n",
      "After Task 2: [88.4, 91.4]\n",
      "After Task 3: [89.2, 89.8, 87.8]\n",
      "After Task 4: [89.2, 88.8, 86.6, 94.6]\n",
      "After Task 5: [88.0, 89.4, 87.4, 93.8, 92.0]\n",
      "After Task 6: [86.0, 88.8, 85.2, 91.2, 89.8, 91.8]\n",
      "After Task 7: [86.8, 86.6, 85.8, 91.2, 88.4, 90.2, 86.8]\n",
      "After Task 8: [85.8, 85.0, 84.0, 91.0, 85.8, 86.4, 83.6, 88.2]\n",
      "After Task 9: [86.0, 87.0, 84.6, 89.8, 88.6, 89.4, 84.2, 85.4, 88.8]\n",
      "After Task 10: [85.8, 86.2, 85.4, 89.8, 87.2, 88.0, 83.6, 84.2, 87.0, 84.6]\n",
      "\n",
      "Final Average Accuracy (A): 86.18%\n",
      "Final Forgetting (F): 3.82%\n",
      "Final Backward Transfer (BT): -0.76%\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "# Load previous encoder (f_t from Task 6)\n",
    "base_ssl_model_instance.mae_backbone.load_state_dict(torch.load(\"ft_task6_encoder.pth\"))\n",
    "prev_encoder_state_dict = torch.load(\"ft_task6_encoder.pth\")\n",
    "\n",
    "# Set start task ID\n",
    "start_task_id = 6  # Task 7 (0-indexed)\n",
    "\n",
    "# Resume training loop\n",
    "for task_id, current_task_dataset in enumerate(task_datasets[start_task_id:], start=start_task_id):\n",
    "    print(f\"\\n===== Resuming Training: Task {task_id + 1}/{len(task_datasets)} =====\")\n",
    "\n",
    "    current_task_loader = DataLoader(\n",
    "        current_task_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() // 2 if os.cpu_count() else 0,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    trainer = CaSSLeTrainer(\n",
    "        base_ssl_model=base_ssl_model_instance,\n",
    "        ca_predictor_hidden_dim=1024,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        lambda_cassle=LAMBDA_CASSLE,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    trainer.set_previous_frozen_encoder(prev_encoder_state_dict)\n",
    "\n",
    "    prev_encoder_state_dict = trainer.train_task(current_task_loader, NUM_EPOCHS_PER_TASK)\n",
    "\n",
    "    # Save model for next resume\n",
    "    torch.save(base_ssl_model_instance.state_dict(), f\"ft_task{task_id + 1}.pth\")\n",
    "\n",
    "    print(f\"\\n--- Evaluating after Task {task_id + 1} ---\")\n",
    "    current_seen_classes = sorted(set().union(*task_class_splits[:task_id + 1]))\n",
    "    accuracies_after_this_task = []\n",
    "\n",
    "    for eval_task_idx in range(task_id + 1):\n",
    "        eval_task_classes = task_class_splits[eval_task_idx]\n",
    "        print(f\"  Evaluating on classes from Task {eval_task_idx+1}: {eval_task_classes}\")\n",
    "\n",
    "        acc_jk = evaluate_model(\n",
    "            base_ssl_model_instance.mae_backbone,\n",
    "            eval_task_classes,\n",
    "            train_full,\n",
    "            val_full,\n",
    "            processor,\n",
    "            LINEAR_EVAL_BATCH_SIZE,\n",
    "            LINEAR_EVAL_EPOCHS,\n",
    "            DEVICE\n",
    "        )\n",
    "        accuracies_after_this_task.append(acc_jk)\n",
    "\n",
    "        random_acc = random_accuracies_Ri[eval_task_idx]\n",
    "        print(f\"    Random baseline for Task {eval_task_idx+1}: {random_acc:.2f}%\")\n",
    "        print(f\"    Current accuracy for Task {eval_task_idx+1}: {acc_jk:.2f}%\")\n",
    "        print(f\"    Improvement over random: {acc_jk - random_acc:.2f}%\")\n",
    "\n",
    "    all_task_accuracies.append(accuracies_after_this_task)\n",
    "\n",
    "    # Optional: print current matrix\n",
    "    print(f\"\\nCurrent A_j,k matrix after Task {task_id + 1}:\")\n",
    "    for j_idx, row in enumerate(all_task_accuracies):\n",
    "        print(f\"After Task {j_idx+1}: {row}\")\n",
    "\n",
    "# --- Final Metric Calculation ---\n",
    "T = len(task_datasets)\n",
    "final_accuracies_row = all_task_accuracies[T-1]\n",
    "avg_accuracy = sum(final_accuracies_row) / T\n",
    "print(f\"\\nFinal Average Accuracy (A): {avg_accuracy:.2f}%\")\n",
    "\n",
    "# Forgetting\n",
    "forgetting = 0\n",
    "if T > 1:\n",
    "    for i in range(T - 1):\n",
    "        max_acc = max(all_task_accuracies[t][i] for t in range(T) if i < len(all_task_accuracies[t]))\n",
    "        final_acc = all_task_accuracies[T-1][i]\n",
    "        forgetting += (max_acc - final_acc)\n",
    "    forgetting /= (T - 1)\n",
    "print(f\"Final Forgetting (F): {forgetting:.2f}%\")\n",
    "\n",
    "# Backward Transfer\n",
    "backward_transfer = 0\n",
    "count = 0\n",
    "if T > 1:\n",
    "    for new_task in range(1, T):\n",
    "        for old_task in range(new_task):\n",
    "            if old_task < len(all_task_accuracies[new_task - 1]) and old_task < len(all_task_accuracies[new_task]):\n",
    "                acc_before = all_task_accuracies[new_task - 1][old_task]\n",
    "                acc_after = all_task_accuracies[new_task][old_task]\n",
    "                backward_transfer += (acc_after - acc_before)\n",
    "                count += 1\n",
    "backward_transfer /= count if count > 0 else 1\n",
    "print(f\"Final Backward Transfer (BT): {backward_transfer:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMYb9QzWkmSEnAdTjWXovoG",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1812295,
     "sourceId": 2955941,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4116.517978,
   "end_time": "2025-07-02T11:48:47.810707",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-02T10:40:11.292729",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0cbf9ead6cb1447c8e536602c758419a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f19e579ef494840a3eab1351223618d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c05084d73814a3aada143c0ce17269d",
       "IPY_MODEL_79143ef0bfab40d397f399efac7f7a3c",
       "IPY_MODEL_45cd04d066604d478a7ed5214ed884c5"
      ],
      "layout": "IPY_MODEL_cc22e146f14f49c683c61fd840f0c5d2"
     }
    },
    "174833fb5c61481baafdcee3143d42f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "251c1268861e4c41bdbca3323c00603b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c05084d73814a3aada143c0ce17269d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da74f7b558704e1c95ffd28a029f5457",
      "placeholder": "​",
      "style": "IPY_MODEL_e8a4d972837c420aa793bee20a6185c7",
      "value": "model.safetensors: 100%"
     }
    },
    "3516bbc7424c4cb6b69f5b73111a319b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84e755572250483ca693fc4af18c14bf",
      "placeholder": "​",
      "style": "IPY_MODEL_ec5e3756b0fb4fdf803c6e4305e5d4e8",
      "value": " 217/217 [00:00&lt;00:00, 18.7kB/s]"
     }
    },
    "374ee98c27ce4cf5afd0b8ac1a103771": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65397a410c4b4e1e96239c8e754723a3",
       "IPY_MODEL_f5bc2af6bfb142648d52ce3a44e989ab",
       "IPY_MODEL_8122bdd0004545f6b0eaec4be58e92e8"
      ],
      "layout": "IPY_MODEL_939ef0d76d314175a0fad8fc9372e43d"
     }
    },
    "45aa5b61f1ed42fd8aa5e1aa4dc2deab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45cd04d066604d478a7ed5214ed884c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e875f645c9d648feab26cd3d7e408ff3",
      "placeholder": "​",
      "style": "IPY_MODEL_174833fb5c61481baafdcee3143d42f0",
      "value": " 448M/448M [00:11&lt;00:00, 64.9MB/s]"
     }
    },
    "613eeed1f5b44b03b71c799446c3d4af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65397a410c4b4e1e96239c8e754723a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91e282d5c03e469fb95bc657d9e8ec62",
      "placeholder": "​",
      "style": "IPY_MODEL_afbfab04e1a7493b82b8279796218d78",
      "value": "config.json: 100%"
     }
    },
    "66a94539bc1c4015a8e44fac7f226a71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79143ef0bfab40d397f399efac7f7a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1057174e53c4f69ac6562b71d9c06e5",
      "max": 447670680,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0f0ac4d67c94d67afe2b0839e407f50",
      "value": 447670680
     }
    },
    "8122bdd0004545f6b0eaec4be58e92e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cbf9ead6cb1447c8e536602c758419a",
      "placeholder": "​",
      "style": "IPY_MODEL_ba62365bed54479fbfba1baf23381733",
      "value": " 676/676 [00:00&lt;00:00, 31.6kB/s]"
     }
    },
    "846dae2791684dd6ac5bef458fd9ed99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae5208bc04ac4a7ca9e11ab11db3308c",
       "IPY_MODEL_d229166e14584fcab457470d5d25199e",
       "IPY_MODEL_3516bbc7424c4cb6b69f5b73111a319b"
      ],
      "layout": "IPY_MODEL_66a94539bc1c4015a8e44fac7f226a71"
     }
    },
    "84e755572250483ca693fc4af18c14bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91e282d5c03e469fb95bc657d9e8ec62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "939ef0d76d314175a0fad8fc9372e43d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0f0ac4d67c94d67afe2b0839e407f50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae5208bc04ac4a7ca9e11ab11db3308c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45aa5b61f1ed42fd8aa5e1aa4dc2deab",
      "placeholder": "​",
      "style": "IPY_MODEL_251c1268861e4c41bdbca3323c00603b",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "afbfab04e1a7493b82b8279796218d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba62365bed54479fbfba1baf23381733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1057174e53c4f69ac6562b71d9c06e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caa63590a1d441198ddbd635c747c57e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc22e146f14f49c683c61fd840f0c5d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d229166e14584fcab457470d5d25199e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eda38306c12f4a56a1558c17ed2b3aa5",
      "max": 217,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc5727b6df6a4bc7be1a73ffb61db0c4",
      "value": 217
     }
    },
    "da74f7b558704e1c95ffd28a029f5457": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e875f645c9d648feab26cd3d7e408ff3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8a4d972837c420aa793bee20a6185c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec5e3756b0fb4fdf803c6e4305e5d4e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eda38306c12f4a56a1558c17ed2b3aa5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5bc2af6bfb142648d52ce3a44e989ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_613eeed1f5b44b03b71c799446c3d4af",
      "max": 676,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_caa63590a1d441198ddbd635c747c57e",
      "value": 676
     }
    },
    "fc5727b6df6a4bc7be1a73ffb61db0c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
