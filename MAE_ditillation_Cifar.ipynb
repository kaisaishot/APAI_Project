{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZQ0XzSFS2BC"
      },
      "source": [
        "CaSSLe : prepare the base ssl model then divide the data set into tasks and then train the model on each task while saving encoder (frozen) after each task\n",
        "during training Train the current model (student) using two losses\n",
        "\n",
        "training loop for task t(After learning task t-1):\n",
        "  \n",
        "\n",
        "*   save a copy of the models backbone and prediction/projection head (ft-1 ,gt-1)\n",
        "*   alongside the cassle predictor ht-1 from previous task\n",
        "\n",
        "\n",
        "*   your main backbone is the same from previous task\n",
        "\n",
        "*  the cassle predictor ht is reinitialized\n",
        "\n",
        "*     Apply two different augmentations(e.g.,v1,v2) to each image, just like standard SSL.\n",
        "\n",
        "*     Take the representation from one of the augmented views of the current imagePass it through the current CaSSLe predictor network ht (p1)\n",
        "\n",
        "\n",
        "*     Pass the same representation through the frozen previous task's network and its predictor (q1)\n",
        "\n",
        "\n",
        "*     compute CaSSLe Distillation Loss typically a cross-entropy or MSE loss\n",
        "      between q1 adn p1\n",
        "\n",
        "\n",
        "*     compute the ssl loss for current task using z1 and z2\n",
        "      combined loss Ltotal\n",
        "\n",
        "*    The entire current mode (ft,ht,gt) is updated using backpropagation on\n",
        "     Ltotal\n",
        "\n",
        "\n",
        "*The main representation learning components (f and g) are carried over.\n",
        "The CaSSLe predictor from the previous task is saved and frozen as a teacher.\n",
        "A new CaSSLe predictor is re-initialized for the current task and trained.*\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7K-KM05Doay"
      },
      "source": [
        "\n",
        "#**MAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50Eg3HXjLmJM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import ViTMAEForPreTraining\n",
        "from transformers import ViTMAEConfig\n",
        "from transformers import AutoImageProcessor\n",
        "from typing import List, Dict, Any\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import ConcatDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20hBi_npi1sA"
      },
      "source": [
        "mae finetune on class 1\n",
        "freeze mae1 and train mae1 on class 2 using ssl loss from mae1 with two augmentations from class 2 and ssl loss from frozen mae1 and g(z) of one aumentatio\n",
        "then save the trained mae1 as mae2\n",
        "etc..\n",
        "so we use the output of the decoder for the ssl loss and use the output of the encoder for the distillation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3tKUeDfQpMgX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Custom Dataset for Two Augmentations and Task Filtering\n",
        "class CustomCifar100TaskDataset(Dataset):\n",
        "    def __init__(self, cifar100_dataset, class_list: List[int], transform_v1, transform_v2, processor):\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "        self.processor = processor # HuggingFace processor for pixel_values\n",
        "\n",
        "        # Filter CIFAR-100 data based on the provided class_list\n",
        "        for i in range(len(cifar100_dataset)):\n",
        "            img, label = cifar100_dataset[i]\n",
        "            if label in class_list:\n",
        "                # Ensure img is a PIL Image if it's a numpy array, as torchvision transforms expect it\n",
        "                if isinstance(img, np.ndarray):\n",
        "                    img = Image.fromarray(img)\n",
        "                self.data.append(img)\n",
        "                self.targets.append(label)\n",
        "\n",
        "        self.transform_v1 = transform_v1\n",
        "        self.transform_v2 = transform_v2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        label = self.targets[idx] # Label not used by CaSSLe's SSL, but kept for consistency\n",
        "\n",
        "        # Apply two distinct augmentations to the same image\n",
        "        img_v1 = self.transform_v1(img)\n",
        "        img_v2 = self.transform_v2(img)\n",
        "\n",
        "        # HuggingFace processor converts PIL Image to tensor, normalizes, and resizes if needed\n",
        "        inputs_v1 = self.processor(images=img_v1, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        inputs_v2 = self.processor(images=img_v2, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "\n",
        "        return inputs_v1, inputs_v2, label\n",
        "\n",
        "#CaSSLe predictor g(z)\n",
        "class CaSSLePredictor(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        super(CaSSLePredictor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "class MAECaSSLeModel(nn.Module):\n",
        "    def __init__(self, mae_backbone: ViTMAEForPreTraining):\n",
        "        super().__init__()\n",
        "        self.mae_backbone = mae_backbone\n",
        "        # The output dimension of the MAE encoder (hidden_size in config)\n",
        "        self.features_for_h_dim = mae_backbone.config.hidden_size\n",
        "        #Freezing the decoder\n",
        "        for param in self.mae_backbone.decoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def forward(self, img_v1: torch.Tensor, img_v2: torch.Tensor) -> Dict[str, Any]:\n",
        "\n",
        "        # Ensure images have a batch dimension for the MAE model\n",
        "        # Using output_hidden_states=True to get encoder outputs\n",
        "        outputs_v1 = self.mae_backbone(pixel_values=img_v1, return_dict=True, output_hidden_states=True)\n",
        "        outputs_v2 = self.mae_backbone(pixel_values=img_v2, return_dict=True, output_hidden_states=True)\n",
        "\n",
        "        # For MAE, the last_hidden_state is (batch_size, sequence_length, hidden_size).\n",
        "        # We average over the sequence length (patches + CLS token) for a single feature vector per image.\n",
        "        features_for_h_v1 = outputs_v1.hidden_states[-1].mean(dim=1)\n",
        "        features_for_h_v2 = outputs_v2.hidden_states[-1].mean(dim=1)\n",
        "\n",
        "        # MAE models often compute the reconstruction loss internally.\n",
        "        ssl_loss_components_v1 = {'internal_mae_loss': outputs_v1.loss}\n",
        "        ssl_loss_components_v2 = {'internal_mae_loss': outputs_v2.loss}\n",
        "\n",
        "        return {\n",
        "            'features_for_h_v1': features_for_h_v1,\n",
        "            'features_for_h_v2': features_for_h_v2,\n",
        "            'ssl_loss_components_v1': ssl_loss_components_v1,\n",
        "            'ssl_loss_components_v2': ssl_loss_components_v2\n",
        "        }\n",
        "\n",
        "    def calculate_ssl_loss(self, ssl_loss_components: Dict[str, Any]) -> torch.Tensor:\n",
        "\n",
        "        #Extracts and returns the MAE's internal reconstruction loss.\n",
        "        return ssl_loss_components['internal_mae_loss']\n",
        "\n",
        "    def get_learnable_params(self) -> List[dict]:\n",
        "        #Returns the parameters of the entire MAE backbone\n",
        "        return [{\"params\": self.mae_backbone.parameters()}]\n",
        "\n",
        "\n",
        "#training process for each task, handling the current MAE model,\n",
        "#the frozen previous MAE model, the g predictor, and the two loss terms (L_SSL and L_D).\n",
        "class CaSSLeTrainer:\n",
        "    def __init__(self, base_ssl_model: MAECaSSLeModel,\n",
        "                 ca_predictor_hidden_dim: int,\n",
        "                 learning_rate: float = 1e-4, lambda_cassle: float = 0.1, device: str = 'cpu'):\n",
        "\n",
        "        self.base_ssl_model = base_ssl_model.to(device) # This is f_t\n",
        "        self.lambda_cassle = lambda_cassle\n",
        "        self.device = device\n",
        "\n",
        "        # Input and output dimensions for CaSSLe Predictor (g) are the MAE encoder's feature dimension\n",
        "        predictor_g_input_output_dim = self.base_ssl_model.features_for_h_dim\n",
        "\n",
        "        # Initialize the current CaSSLe predictor (g from the paper)\n",
        "        # It's newly initialized for each task\n",
        "        self.g_current = CaSSLePredictor(\n",
        "            predictor_g_input_output_dim,\n",
        "            ca_predictor_hidden_dim,\n",
        "            predictor_g_input_output_dim\n",
        "        ).to(device)\n",
        "\n",
        "        g_lr=0.001\n",
        "\n",
        "        # Optimizer for ALL trainable parameters: current MAE model (f_t) AND predictor g\n",
        "        params = self.base_ssl_model.get_learnable_params()  # This is a list of dicts\n",
        "        params.append({\"params\": self.g_current.parameters(), \"lr\": g_lr})  # Add g_current params\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
        "\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "        # For L_D = L_SSL(g(z), z̄), the paper implies reusing the SSL loss.\n",
        "        # However, for distilling features (z from f_t and z̄ from f_t-1), MSE is a common and effective choice.\n",
        "        # Given MAE's SSL is reconstruction, directly applying it to feature vectors (g(z) and z̄) is not straightforward.\n",
        "        # We'll use MSE for feature distillation as it's a standard practice for this type of knowledge transfer.\n",
        "\n",
        "\n",
        "        # This will hold the frozen previous encoder (f_{t-1}^{frozen})\n",
        "        self.f_frozen_teacher = None\n",
        "\n",
        "    def set_previous_frozen_encoder(self, encoder_state_dict: Dict[str, Any]):\n",
        "\n",
        "        # Re-instantiate the MAE model to load the full state dictionary\n",
        "        self.f_frozen_teacher = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base').to(self.device)\n",
        "        self.f_frozen_teacher.load_state_dict(encoder_state_dict)\n",
        "\n",
        "        # set requires_grad to False for all parameters of the teacher model\n",
        "        for param in self.f_frozen_teacher.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        print(f\"Frozen encoder (f_t-1) loaded and parameters frozen: {all(not p.requires_grad for p in self.f_frozen_teacher.parameters())}\")\n",
        "\n",
        "\n",
        "    def train_task(self, data_loader: torch.utils.data.DataLoader, epochs: int):\n",
        "\n",
        "        self.base_ssl_model.train()\n",
        "        self.g_current.train()\n",
        "        scaler = torch.amp.GradScaler('cuda')  # Initialize GradScaler for AMP\n",
        "        # Set f_frozen_teacher to eval mode to disable updates for teacher\n",
        "        if self.f_frozen_teacher:\n",
        "            self.f_frozen_teacher.eval()\n",
        "\n",
        "        print(f\"Distilling from frozen teacher encoder (f_t-1): {self.f_frozen_teacher is not None}\")\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience = 3\n",
        "        patience_counter = 0\n",
        "        min_delta = 1e-3\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_ssl_loss = 0\n",
        "            total_cassle_loss = 0\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch_idx, (img_v1, img_v2, _) in enumerate(data_loader): # _ for labels\n",
        "                self.optimizer.zero_grad()\n",
        "                img_v1 = img_v1.to(self.device)\n",
        "                img_v2 = img_v2.to(self.device)\n",
        "\n",
        "                # Mixed precision\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                  # Forward Pass through the current trainable MAE model (f_t)\n",
        "                  # This returns features (z_A, z_B) and components for L_SSL\n",
        "                  ssl_output = self.base_ssl_model(img_v1, img_v2)\n",
        "\n",
        "                  # Extract features for CaSSLe predictor and APPLY L2 NORMALIZATION HERE\n",
        "                  features_for_h_v1_norm = F.normalize(ssl_output['features_for_h_v1'], dim=-1)\n",
        "                  features_for_h_v2_norm = F.normalize(ssl_output['features_for_h_v2'], dim=-1)\n",
        "\n",
        "                  # Calculate Base Self-Supervised Loss (L_SSL - MAE Reconstruction Loss)\n",
        "                  # Sum MAE reconstruction loss for both views\n",
        "                  loss_ssl = self.base_ssl_model.calculate_ssl_loss(ssl_output['ssl_loss_components_v1']) + \\\n",
        "                            self.base_ssl_model.calculate_ssl_loss(ssl_output['ssl_loss_components_v2'])\n",
        "\n",
        "                  # Calculate CaSSLe Distillation Loss (L_D)\n",
        "                  loss_cassle = torch.tensor(0.0).to(self.device) # Initialize to 0 for the first task\n",
        "\n",
        "                  if self.f_frozen_teacher:\n",
        "                      # Get features from the frozen previous encoder\n",
        "\n",
        "                      with torch.no_grad():\n",
        "                        # Get encoder outputs only (bypass the decoder)\n",
        "                        outputs_v1 = self.f_frozen_teacher.vit(img_v1, output_hidden_states=True)\n",
        "                        outputs_v1_features= outputs_v1.hidden_states[-1].mean(dim=1)\n",
        "                        features_from_frozen_v1 = F.normalize(outputs_v1_features, dim=-1)\n",
        "\n",
        "                        outputs_v2 = self.f_frozen_teacher.vit(img_v2, output_hidden_states=True)\n",
        "                        outputs_v2_features= outputs_v2.hidden_states[-1].mean(dim=1)\n",
        "                        features_from_frozen_v2 = F.normalize(outputs_v2_features, dim=-1)\n",
        "\n",
        "                      # Student predictions from current trainable g\n",
        "                      # g takes features from current f_t\n",
        "                      student_pred_v1 = self.g_current(features_for_h_v1_norm)\n",
        "                      student_pred_v2 = self.g_current(features_for_h_v2_norm)\n",
        "\n",
        "                      # Teacher targets (from frozen f_t-1)\n",
        "                      teacher_target_v1 = features_from_frozen_v1\n",
        "                      teacher_target_v2 = features_from_frozen_v2\n",
        "\n",
        "                      # Compute distillation loss using MSE\n",
        "                      loss_cassle_v1 = 1 - F.cosine_similarity(student_pred_v1, teacher_target_v1, dim=-1).mean()\n",
        "                      loss_cassle_v2 =  1 - F.cosine_similarity(student_pred_v2, teacher_target_v2, dim=-1).mean()\n",
        "                      loss_cassle = (loss_cassle_v1 + loss_cassle_v2 )* self.lambda_cassle\n",
        "\n",
        "\n",
        "\n",
        "                  # Total Loss and backpropagation\n",
        "                  loss = loss_ssl +  loss_cassle\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(self.optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                total_ssl_loss += loss_ssl.item()\n",
        "                total_cassle_loss += loss_cassle.item()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            #Early stopping\n",
        "            avg_loss = total_loss / len(data_loader)\n",
        "            if avg_loss < best_loss - min_delta:\n",
        "                best_loss = avg_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    break\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - SSL Loss: {total_ssl_loss / len(data_loader):.4f}, \"\n",
        "                  f\"CaSSLe Loss: {total_cassle_loss / len(data_loader):.4f}, \"\n",
        "                  f\"Total Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            self.scheduler.step()\n",
        "\n",
        "        # After training the task, return the state_dict of the current MAE model (f_t)\n",
        "        return self.base_ssl_model.mae_backbone.state_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB7DIj_OsSBt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluation Function\n",
        "def evaluate_model(feature_extractor: torch.nn.Module,\n",
        "                   all_seen_classes: List[int],\n",
        "                   cifar100_train_full: datasets.CIFAR100, # Full CIFAR100 train dataset\n",
        "                   cifar100_test_full: datasets.CIFAR100,  # Full CIFAR100 test dataset\n",
        "                   processor, # HuggingFace processor\n",
        "                   batch_size: int = 128,\n",
        "                   linear_eval_epochs: int = 5,\n",
        "                   device: torch.device = torch.device(\"cuda\")):\n",
        "\n",
        "    # Put feature extractor is in eval mode and frozen\n",
        "    feature_extractor.eval()\n",
        "    for param in feature_extractor.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Prepare Data for Linear Classifier Training\n",
        "    # Filter training data to include only classes seen so far\n",
        "    train_linear_dataset = CustomCifar100TaskDataset(\n",
        "        cifar100_train_full,\n",
        "        all_seen_classes,\n",
        "        None,\n",
        "        None,\n",
        "        processor\n",
        "    )\n",
        "\n",
        "\n",
        "    # For linear evaluation, we extract features from the original images, usually with a standard transform.\n",
        "    linear_eval_transform = transforms.Compose([\n",
        "        transforms.Resize(INPUT_SIZE),\n",
        "        transforms.CenterCrop(INPUT_SIZE),\n",
        "        # HuggingFace processor handles ToTensor and Normalize, so we don't need it here.\n",
        "    ])\n",
        "    # wrap the dataset for linear evaluation\n",
        "    class LinearEvalDataset(Dataset):\n",
        "        def __init__(self, original_dataset, class_list, transform, processor):\n",
        "            self.data = []\n",
        "            self.targets = []\n",
        "            self.transform = transform\n",
        "            self.processor = processor\n",
        "\n",
        "            for i in range(len(original_dataset)):\n",
        "                img, label = original_dataset[i]\n",
        "                if label in class_list:\n",
        "                    if isinstance(img, np.ndarray):\n",
        "                        img = Image.fromarray(img)\n",
        "                    self.data.append(img)\n",
        "                    self.targets.append(label)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img = self.data[idx]\n",
        "            label = self.targets[idx]\n",
        "\n",
        "            img = self.transform(img)\n",
        "            inputs = self.processor(images=img, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "            return inputs, label\n",
        "\n",
        "    train_linear_dataset = LinearEvalDataset(cifar100_train_full, all_seen_classes, linear_eval_transform, processor)\n",
        "    train_linear_loader = DataLoader(train_linear_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                     num_workers=os.cpu_count() // 2 if os.cpu_count() else 0, pin_memory=False)\n",
        "\n",
        "    # Linear Classifier\n",
        "    num_output_classes = len(all_seen_classes)\n",
        "    linear_classifier = nn.Linear(feature_extractor.config.hidden_size, num_output_classes).to(device)\n",
        "\n",
        "    # Change the original 100 labels to a continuous range starting from 0, only for the current classes\n",
        "    label_to_contiguous_map = {label: i for i, label in enumerate(sorted(all_seen_classes))}\n",
        "\n",
        "    #Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(linear_classifier.parameters(), lr=0.001)\n",
        "    final_loss = 0.0\n",
        "    num_batches = 0\n",
        "    # Train Linear Classifier\n",
        "    linear_classifier.train()\n",
        "    for epoch in range(linear_eval_epochs):\n",
        "        for img_batch, label_batch in train_linear_loader:\n",
        "            img_batch = img_batch.to(device)\n",
        "            # Map original labels to contiuous range\n",
        "            label_batch = torch.tensor([label_to_contiguous_map[l.item()] for l in label_batch]).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.no_grad(): # Ensure feature extractor remains frozen\n",
        "              with torch.amp.autocast('cuda'): #Mixed precision\n",
        "                features = feature_extractor(pixel_values=img_batch, return_dict=True, output_hidden_states=True).hidden_states[-1].mean(dim=1)\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "              outputs = linear_classifier(features)\n",
        "              loss = criterion(outputs, label_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            final_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        avg_loss = final_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{linear_eval_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # Evaluate Linear Classifier on Test Data\n",
        "    linear_classifier.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Prepare test data for classes seen so far\n",
        "    test_linear_dataset = LinearEvalDataset(cifar100_test_full, all_seen_classes, linear_eval_transform, processor)\n",
        "    test_linear_loader = DataLoader(test_linear_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                   num_workers=os.cpu_count() // 2 if os.cpu_count() else 0, pin_memory=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_batch, label_batch in test_linear_loader:\n",
        "            img_batch = img_batch.to(device)\n",
        "            label_batch = torch.tensor([label_to_contiguous_map[l.item()] for l in label_batch]).to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "              features = feature_extractor(pixel_values=img_batch, return_dict=True, output_hidden_states=True).hidden_states[-1].mean(dim=1)\n",
        "              outputs = linear_classifier(features)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += label_batch.size(0)\n",
        "            total_correct += (predicted == label_batch).sum().item()\n",
        "\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    print(f\"Linear evaluation accuracy on {len(all_seen_classes)} classes: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "    for param in feature_extractor.parameters():\n",
        "        param.requires_grad = True # Re-enable gradients for next task's training\n",
        "    feature_extractor.train()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Baseline for Forward Transfer (R_i)\n",
        "def get_random_accuracy(num_classes_in_task: int,\n",
        "                        cifar100_train_full: datasets.CIFAR100,\n",
        "                        cifar100_test_full: datasets.CIFAR100,\n",
        "                        processor,\n",
        "                        target_class_list: List[int],\n",
        "                        batch_size: int = 128,\n",
        "                        linear_eval_epochs: int = 5,\n",
        "                        device: torch.device = torch.device(\"cpu\")):\n",
        "\n",
        "    # Here, we'll train a linear classifier on top of a randomly initialized MAE encoder\n",
        "    print(f\"Calculating R_i for task with classes: {target_class_list}\")\n",
        "\n",
        "    # Randomly initialize its weights\n",
        "    config = ViTMAEConfig()\n",
        "    random_mae_backbone = ViTMAEForPreTraining(config).to(device) # Random model\n",
        "    random_mae_backbone.eval()\n",
        "    for param in random_mae_backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Re-use the evaluate_model function with the random backbone\n",
        "    random_acc = evaluate_model(random_mae_backbone,\n",
        "                                 target_class_list, # Only target classes for this task\n",
        "                                 cifar100_train_full,\n",
        "                                 cifar100_test_full,\n",
        "                                 processor,\n",
        "                                 batch_size,\n",
        "                                 linear_eval_epochs,\n",
        "                                 device)\n",
        "    print(f\"Random network accuracy on task: {random_acc:.2f}%\")\n",
        "    return random_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9557213968dd4998a88a7b2b8ba8ac32",
            "758489f6eb02445ea40bbe9ce86a5d0c",
            "278cde1efa434c1dbfd9dd5a2b22e9d9",
            "6f59ca986d544bcc9b848721812e79d1",
            "72a91181c871444cafd8ac0a6d3ce0a0",
            "37b8357360314f0fbadb33538d5c1a7f",
            "ee9b45c240e24bae8abb2a84e0034d54",
            "078ab178b7864b35ab1b5190760d8db0",
            "867c779fe3764e5d878ddcd5535fb7e4",
            "b4446a05c7be4b5d9de8ef498edc8685",
            "abb67562ea474ae8b07361bc1380c12a",
            "bb9149e4edd941f4878fdddf6814c49a",
            "3bd86d5edf8c49f39ed29a2198a8e500",
            "adc20ad8a0f14386b107b5b8af4226ca",
            "fee3661f0d124ce1ad0e5686dee3c3fd",
            "c903aac6321f4c1e8450a0c6538c7e33",
            "f57308ef62f44904992da0f1d33ff63c",
            "25936c18a20a409d818ff7c0f169e233",
            "5c6cf2565e8c43f4ad95fbddef535808",
            "edbde7dae4c445f59de40aa950e0f4f0",
            "ab39e29b9ed3428cbeac9a9986900e88",
            "01cd4d497ba24864a3b52e841481ea2f",
            "da04a97d95f741a8ab14e9c7890884f7",
            "2fc2dd170e974d3bb3db2fab9dc7ca18",
            "cb8987bb6d7042feaa6d4ebb55fedabb",
            "23bdebd26192413d9c2b1dd11e76ad29",
            "c9a5ddae4afc4647a72ce0f26b2b20c0",
            "7bfe98e3e6ba4c22adc28262ff4177b2",
            "8a7a2519e885493483e2646d0d20e00e",
            "4dd5feaa520e4451a3eb411ee518e755",
            "1e6299ff7c384fb98fab30a583237f5a",
            "eb9b2909a2e44926910105ee5085695c",
            "36aff59f3dc94385b0d7ffb5d260b51f"
          ]
        },
        "id": "YfKclZBO7qpu",
        "outputId": "af4f7a82-6994-40d3-86fe-acc7c2a35ec6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9557213968dd4998a88a7b2b8ba8ac32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb9149e4edd941f4878fdddf6814c49a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/448M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da04a97d95f741a8ab14e9c7890884f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "100%|██████████| 169M/169M [00:05<00:00, 31.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1 includes classes: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Task 2 includes classes: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Task 3 includes classes: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Task 4 includes classes: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Task 5 includes classes: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Task 6 includes classes: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Task 7 includes classes: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Task 8 includes classes: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Task 9 includes classes: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Task 10 includes classes: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
            "\n",
            "--- Starting CaSSLe Continual Training ---\n",
            "Running on device: cuda\n",
            "\n",
            "===== Training Task 1/10 =====\n",
            "Distilling from frozen teacher encoder (f_t-1): False\n",
            "Epoch 1/15 - SSL Loss: 0.0908, CaSSLe Loss: 0.0000, Total Loss: 0.0908\n",
            "Epoch 2/15 - SSL Loss: 0.0846, CaSSLe Loss: 0.0000, Total Loss: 0.0846\n",
            "Epoch 3/15 - SSL Loss: 0.0834, CaSSLe Loss: 0.0000, Total Loss: 0.0834\n",
            "Epoch 4/15 - SSL Loss: 0.0824, CaSSLe Loss: 0.0000, Total Loss: 0.0824\n",
            "Epoch 5/15 - SSL Loss: 0.0836, CaSSLe Loss: 0.0000, Total Loss: 0.0836\n",
            "Epoch 6/15 - SSL Loss: 0.0806, CaSSLe Loss: 0.0000, Total Loss: 0.0806\n",
            "Epoch 7/15 - SSL Loss: 0.0766, CaSSLe Loss: 0.0000, Total Loss: 0.0766\n",
            "Epoch 8/15 - SSL Loss: 0.0774, CaSSLe Loss: 0.0000, Total Loss: 0.0774\n",
            "Epoch 9/15 - SSL Loss: 0.0747, CaSSLe Loss: 0.0000, Total Loss: 0.0747\n",
            "Epoch 10/15 - SSL Loss: 0.0757, CaSSLe Loss: 0.0000, Total Loss: 0.0757\n",
            "Epoch 11/15 - SSL Loss: 0.0738, CaSSLe Loss: 0.0000, Total Loss: 0.0738\n",
            "Early stopping triggered.\n",
            "\n",
            "--- Evaluating after Task 1 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.8377\n",
            "Epoch 2/5, Average Loss: 1.2677\n",
            "Epoch 3/5, Average Loss: 1.0274\n",
            "Epoch 4/5, Average Loss: 0.8917\n",
            "Epoch 5/5, Average Loss: 0.7980\n",
            "Linear evaluation accuracy on 10 classes: 87.00%\n",
            "Calculating R_i for task with classes: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 2.1343\n",
            "Epoch 2/5, Average Loss: 2.0738\n",
            "Epoch 3/5, Average Loss: 2.0416\n",
            "Epoch 4/5, Average Loss: 2.0163\n",
            "Epoch 5/5, Average Loss: 2.0009\n",
            "Linear evaluation accuracy on 10 classes: 34.30%\n",
            "Random network accuracy on task: 34.30%\n",
            "\n",
            "Current A_j,k matrix after Task 1:\n",
            "After Task 1: [87.0]\n",
            "\n",
            "===== Training Task 2/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0972, CaSSLe Loss: 0.2720, Total Loss: 0.3692\n",
            "Epoch 2/15 - SSL Loss: 0.0886, CaSSLe Loss: 0.0698, Total Loss: 0.1583\n",
            "Epoch 3/15 - SSL Loss: 0.0854, CaSSLe Loss: 0.0541, Total Loss: 0.1395\n",
            "Epoch 4/15 - SSL Loss: 0.0877, CaSSLe Loss: 0.0459, Total Loss: 0.1336\n",
            "Epoch 5/15 - SSL Loss: 0.0866, CaSSLe Loss: 0.0400, Total Loss: 0.1266\n",
            "Epoch 6/15 - SSL Loss: 0.0862, CaSSLe Loss: 0.0361, Total Loss: 0.1223\n",
            "Epoch 7/15 - SSL Loss: 0.0819, CaSSLe Loss: 0.0330, Total Loss: 0.1149\n",
            "Epoch 8/15 - SSL Loss: 0.0797, CaSSLe Loss: 0.0326, Total Loss: 0.1123\n",
            "Epoch 9/15 - SSL Loss: 0.0793, CaSSLe Loss: 0.0323, Total Loss: 0.1116\n",
            "Epoch 10/15 - SSL Loss: 0.0777, CaSSLe Loss: 0.0321, Total Loss: 0.1098\n",
            "Epoch 11/15 - SSL Loss: 0.0772, CaSSLe Loss: 0.0318, Total Loss: 0.1090\n",
            "Epoch 12/15 - SSL Loss: 0.0756, CaSSLe Loss: 0.0316, Total Loss: 0.1072\n",
            "Epoch 13/15 - SSL Loss: 0.0780, CaSSLe Loss: 0.0313, Total Loss: 0.1093\n",
            "Epoch 14/15 - SSL Loss: 0.0761, CaSSLe Loss: 0.0311, Total Loss: 0.1072\n",
            "Early stopping triggered.\n",
            "\n",
            "--- Evaluating after Task 2 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.4443\n",
            "Epoch 2/5, Average Loss: 1.0654\n",
            "Epoch 3/5, Average Loss: 0.8976\n",
            "Epoch 4/5, Average Loss: 0.8010\n",
            "Epoch 5/5, Average Loss: 0.7337\n",
            "Linear evaluation accuracy on 10 classes: 83.30%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 1.2911\n",
            "Epoch 2/5, Average Loss: 0.9174\n",
            "Epoch 3/5, Average Loss: 0.7660\n",
            "Epoch 4/5, Average Loss: 0.6785\n",
            "Epoch 5/5, Average Loss: 0.6236\n",
            "Linear evaluation accuracy on 10 classes: 86.90%\n",
            "Calculating R_i for task with classes: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 1.9945\n",
            "Epoch 2/5, Average Loss: 1.9404\n",
            "Epoch 3/5, Average Loss: 1.8988\n",
            "Epoch 4/5, Average Loss: 1.8769\n",
            "Epoch 5/5, Average Loss: 1.8525\n",
            "Linear evaluation accuracy on 10 classes: 37.90%\n",
            "Random network accuracy on task: 37.90%\n",
            "\n",
            "Current A_j,k matrix after Task 2:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "\n",
            "===== Training Task 3/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0762, CaSSLe Loss: 0.3374, Total Loss: 0.4135\n",
            "Epoch 2/15 - SSL Loss: 0.0731, CaSSLe Loss: 0.1071, Total Loss: 0.1802\n",
            "Epoch 3/15 - SSL Loss: 0.0724, CaSSLe Loss: 0.0696, Total Loss: 0.1420\n",
            "Epoch 4/15 - SSL Loss: 0.0715, CaSSLe Loss: 0.0573, Total Loss: 0.1288\n",
            "Epoch 5/15 - SSL Loss: 0.0717, CaSSLe Loss: 0.0525, Total Loss: 0.1242\n",
            "Epoch 6/15 - SSL Loss: 0.0704, CaSSLe Loss: 0.0482, Total Loss: 0.1186\n",
            "Epoch 7/15 - SSL Loss: 0.0680, CaSSLe Loss: 0.0441, Total Loss: 0.1122\n",
            "Epoch 8/15 - SSL Loss: 0.0660, CaSSLe Loss: 0.0429, Total Loss: 0.1089\n",
            "Epoch 9/15 - SSL Loss: 0.0668, CaSSLe Loss: 0.0429, Total Loss: 0.1097\n",
            "Epoch 10/15 - SSL Loss: 0.0653, CaSSLe Loss: 0.0425, Total Loss: 0.1077\n",
            "Epoch 11/15 - SSL Loss: 0.0638, CaSSLe Loss: 0.0420, Total Loss: 0.1058\n",
            "Epoch 12/15 - SSL Loss: 0.0653, CaSSLe Loss: 0.0417, Total Loss: 0.1070\n",
            "Epoch 13/15 - SSL Loss: 0.0643, CaSSLe Loss: 0.0414, Total Loss: 0.1057\n",
            "Early stopping triggered.\n",
            "\n",
            "--- Evaluating after Task 3 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.2442\n",
            "Epoch 2/5, Average Loss: 0.9339\n",
            "Epoch 3/5, Average Loss: 0.8051\n",
            "Epoch 4/5, Average Loss: 0.7281\n",
            "Epoch 5/5, Average Loss: 0.6708\n",
            "Linear evaluation accuracy on 10 classes: 84.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.9873\n",
            "Epoch 2/5, Average Loss: 0.7323\n",
            "Epoch 3/5, Average Loss: 0.6269\n",
            "Epoch 4/5, Average Loss: 0.5682\n",
            "Epoch 5/5, Average Loss: 0.5298\n",
            "Linear evaluation accuracy on 10 classes: 86.10%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 1.0301\n",
            "Epoch 2/5, Average Loss: 0.7334\n",
            "Epoch 3/5, Average Loss: 0.6164\n",
            "Epoch 4/5, Average Loss: 0.5463\n",
            "Epoch 5/5, Average Loss: 0.5003\n",
            "Linear evaluation accuracy on 10 classes: 89.70%\n",
            "Calculating R_i for task with classes: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 1.8929\n",
            "Epoch 2/5, Average Loss: 1.7779\n",
            "Epoch 3/5, Average Loss: 1.7290\n",
            "Epoch 4/5, Average Loss: 1.6916\n",
            "Epoch 5/5, Average Loss: 1.6620\n",
            "Linear evaluation accuracy on 10 classes: 41.50%\n",
            "Random network accuracy on task: 41.50%\n",
            "\n",
            "Current A_j,k matrix after Task 3:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "\n",
            "===== Training Task 4/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0827, CaSSLe Loss: 0.4232, Total Loss: 0.5059\n",
            "Epoch 2/15 - SSL Loss: 0.0817, CaSSLe Loss: 0.1295, Total Loss: 0.2112\n",
            "Epoch 3/15 - SSL Loss: 0.0805, CaSSLe Loss: 0.0908, Total Loss: 0.1712\n",
            "Epoch 4/15 - SSL Loss: 0.0784, CaSSLe Loss: 0.0763, Total Loss: 0.1547\n",
            "Epoch 5/15 - SSL Loss: 0.0775, CaSSLe Loss: 0.0693, Total Loss: 0.1468\n",
            "Epoch 6/15 - SSL Loss: 0.0770, CaSSLe Loss: 0.0652, Total Loss: 0.1423\n",
            "Epoch 7/15 - SSL Loss: 0.0743, CaSSLe Loss: 0.0596, Total Loss: 0.1340\n",
            "Epoch 8/15 - SSL Loss: 0.0729, CaSSLe Loss: 0.0581, Total Loss: 0.1310\n",
            "Epoch 9/15 - SSL Loss: 0.0732, CaSSLe Loss: 0.0577, Total Loss: 0.1308\n",
            "Epoch 10/15 - SSL Loss: 0.0714, CaSSLe Loss: 0.0570, Total Loss: 0.1285\n",
            "Epoch 11/15 - SSL Loss: 0.0721, CaSSLe Loss: 0.0568, Total Loss: 0.1290\n",
            "Epoch 12/15 - SSL Loss: 0.0719, CaSSLe Loss: 0.0566, Total Loss: 0.1285\n",
            "Epoch 13/15 - SSL Loss: 0.0716, CaSSLe Loss: 0.0557, Total Loss: 0.1274\n",
            "Epoch 14/15 - SSL Loss: 0.0718, CaSSLe Loss: 0.0560, Total Loss: 0.1278\n",
            "Epoch 15/15 - SSL Loss: 0.0701, CaSSLe Loss: 0.0557, Total Loss: 0.1257\n",
            "\n",
            "--- Evaluating after Task 4 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.1471\n",
            "Epoch 2/5, Average Loss: 0.8675\n",
            "Epoch 3/5, Average Loss: 0.7579\n",
            "Epoch 4/5, Average Loss: 0.6887\n",
            "Epoch 5/5, Average Loss: 0.6428\n",
            "Linear evaluation accuracy on 10 classes: 84.40%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 1.0255\n",
            "Epoch 2/5, Average Loss: 0.7529\n",
            "Epoch 3/5, Average Loss: 0.6451\n",
            "Epoch 4/5, Average Loss: 0.5861\n",
            "Epoch 5/5, Average Loss: 0.5468\n",
            "Linear evaluation accuracy on 10 classes: 83.90%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.9942\n",
            "Epoch 2/5, Average Loss: 0.7111\n",
            "Epoch 3/5, Average Loss: 0.5964\n",
            "Epoch 4/5, Average Loss: 0.5320\n",
            "Epoch 5/5, Average Loss: 0.4915\n",
            "Linear evaluation accuracy on 10 classes: 88.30%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 1.2359\n",
            "Epoch 2/5, Average Loss: 0.9184\n",
            "Epoch 3/5, Average Loss: 0.8007\n",
            "Epoch 4/5, Average Loss: 0.7310\n",
            "Epoch 5/5, Average Loss: 0.6783\n",
            "Linear evaluation accuracy on 10 classes: 81.00%\n",
            "Calculating R_i for task with classes: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 2.1200\n",
            "Epoch 2/5, Average Loss: 2.0298\n",
            "Epoch 3/5, Average Loss: 1.9865\n",
            "Epoch 4/5, Average Loss: 1.9524\n",
            "Epoch 5/5, Average Loss: 1.9248\n",
            "Linear evaluation accuracy on 10 classes: 36.90%\n",
            "Random network accuracy on task: 36.90%\n",
            "\n",
            "Current A_j,k matrix after Task 4:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "\n",
            "===== Training Task 5/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0649, CaSSLe Loss: 0.5288, Total Loss: 0.5937\n",
            "Epoch 2/15 - SSL Loss: 0.0597, CaSSLe Loss: 0.1456, Total Loss: 0.2052\n",
            "Epoch 3/15 - SSL Loss: 0.0597, CaSSLe Loss: 0.1060, Total Loss: 0.1656\n",
            "Epoch 4/15 - SSL Loss: 0.0595, CaSSLe Loss: 0.0931, Total Loss: 0.1526\n",
            "Epoch 5/15 - SSL Loss: 0.0594, CaSSLe Loss: 0.0874, Total Loss: 0.1468\n",
            "Epoch 6/15 - SSL Loss: 0.0592, CaSSLe Loss: 0.0820, Total Loss: 0.1411\n",
            "Epoch 7/15 - SSL Loss: 0.0565, CaSSLe Loss: 0.0732, Total Loss: 0.1298\n",
            "Epoch 8/15 - SSL Loss: 0.0561, CaSSLe Loss: 0.0713, Total Loss: 0.1274\n",
            "Epoch 9/15 - SSL Loss: 0.0547, CaSSLe Loss: 0.0703, Total Loss: 0.1250\n",
            "Epoch 10/15 - SSL Loss: 0.0535, CaSSLe Loss: 0.0696, Total Loss: 0.1231\n",
            "Epoch 11/15 - SSL Loss: 0.0549, CaSSLe Loss: 0.0690, Total Loss: 0.1239\n",
            "Epoch 12/15 - SSL Loss: 0.0542, CaSSLe Loss: 0.0687, Total Loss: 0.1228\n",
            "Epoch 13/15 - SSL Loss: 0.0543, CaSSLe Loss: 0.0676, Total Loss: 0.1219\n",
            "Epoch 14/15 - SSL Loss: 0.0536, CaSSLe Loss: 0.0678, Total Loss: 0.1214\n",
            "Epoch 15/15 - SSL Loss: 0.0541, CaSSLe Loss: 0.0675, Total Loss: 0.1217\n",
            "\n",
            "--- Evaluating after Task 5 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.0751\n",
            "Epoch 2/5, Average Loss: 0.8185\n",
            "Epoch 3/5, Average Loss: 0.7124\n",
            "Epoch 4/5, Average Loss: 0.6471\n",
            "Epoch 5/5, Average Loss: 0.6023\n",
            "Linear evaluation accuracy on 10 classes: 83.90%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.8841\n",
            "Epoch 2/5, Average Loss: 0.6739\n",
            "Epoch 3/5, Average Loss: 0.5886\n",
            "Epoch 4/5, Average Loss: 0.5397\n",
            "Epoch 5/5, Average Loss: 0.5084\n",
            "Linear evaluation accuracy on 10 classes: 83.30%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.9330\n",
            "Epoch 2/5, Average Loss: 0.6796\n",
            "Epoch 3/5, Average Loss: 0.5754\n",
            "Epoch 4/5, Average Loss: 0.5123\n",
            "Epoch 5/5, Average Loss: 0.4725\n",
            "Linear evaluation accuracy on 10 classes: 88.00%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 1.2768\n",
            "Epoch 2/5, Average Loss: 0.9465\n",
            "Epoch 3/5, Average Loss: 0.8148\n",
            "Epoch 4/5, Average Loss: 0.7433\n",
            "Epoch 5/5, Average Loss: 0.6892\n",
            "Linear evaluation accuracy on 10 classes: 80.70%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 0.9199\n",
            "Epoch 2/5, Average Loss: 0.6732\n",
            "Epoch 3/5, Average Loss: 0.5787\n",
            "Epoch 4/5, Average Loss: 0.5198\n",
            "Epoch 5/5, Average Loss: 0.4809\n",
            "Linear evaluation accuracy on 10 classes: 90.80%\n",
            "Calculating R_i for task with classes: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 2.0735\n",
            "Epoch 2/5, Average Loss: 1.9950\n",
            "Epoch 3/5, Average Loss: 1.9507\n",
            "Epoch 4/5, Average Loss: 1.9156\n",
            "Epoch 5/5, Average Loss: 1.8904\n",
            "Linear evaluation accuracy on 10 classes: 37.60%\n",
            "Random network accuracy on task: 37.60%\n",
            "\n",
            "Current A_j,k matrix after Task 5:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "After Task 5: [83.9, 83.3, 88.0, 80.7, 90.8]\n",
            "\n",
            "===== Training Task 6/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0752, CaSSLe Loss: 0.5764, Total Loss: 0.6516\n",
            "Epoch 2/15 - SSL Loss: 0.0728, CaSSLe Loss: 0.1760, Total Loss: 0.2488\n",
            "Epoch 3/15 - SSL Loss: 0.0725, CaSSLe Loss: 0.1257, Total Loss: 0.1983\n",
            "Epoch 4/15 - SSL Loss: 0.0717, CaSSLe Loss: 0.1091, Total Loss: 0.1808\n",
            "Epoch 5/15 - SSL Loss: 0.0712, CaSSLe Loss: 0.0997, Total Loss: 0.1709\n",
            "Epoch 6/15 - SSL Loss: 0.0704, CaSSLe Loss: 0.0940, Total Loss: 0.1644\n",
            "Epoch 7/15 - SSL Loss: 0.0687, CaSSLe Loss: 0.0844, Total Loss: 0.1531\n",
            "Epoch 8/15 - SSL Loss: 0.0674, CaSSLe Loss: 0.0818, Total Loss: 0.1493\n",
            "Epoch 9/15 - SSL Loss: 0.0656, CaSSLe Loss: 0.0809, Total Loss: 0.1465\n",
            "Epoch 10/15 - SSL Loss: 0.0668, CaSSLe Loss: 0.0801, Total Loss: 0.1469\n",
            "Epoch 11/15 - SSL Loss: 0.0662, CaSSLe Loss: 0.0794, Total Loss: 0.1456\n",
            "Epoch 12/15 - SSL Loss: 0.0644, CaSSLe Loss: 0.0791, Total Loss: 0.1435\n",
            "Epoch 13/15 - SSL Loss: 0.0663, CaSSLe Loss: 0.0777, Total Loss: 0.1440\n",
            "Epoch 14/15 - SSL Loss: 0.0650, CaSSLe Loss: 0.0775, Total Loss: 0.1425\n",
            "Epoch 15/15 - SSL Loss: 0.0640, CaSSLe Loss: 0.0776, Total Loss: 0.1416\n",
            "\n",
            "--- Evaluating after Task 6 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.0683\n",
            "Epoch 2/5, Average Loss: 0.8187\n",
            "Epoch 3/5, Average Loss: 0.7156\n",
            "Epoch 4/5, Average Loss: 0.6553\n",
            "Epoch 5/5, Average Loss: 0.6141\n",
            "Linear evaluation accuracy on 10 classes: 84.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.8920\n",
            "Epoch 2/5, Average Loss: 0.6826\n",
            "Epoch 3/5, Average Loss: 0.5938\n",
            "Epoch 4/5, Average Loss: 0.5429\n",
            "Epoch 5/5, Average Loss: 0.5101\n",
            "Linear evaluation accuracy on 10 classes: 85.70%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.9100\n",
            "Epoch 2/5, Average Loss: 0.6510\n",
            "Epoch 3/5, Average Loss: 0.5521\n",
            "Epoch 4/5, Average Loss: 0.4988\n",
            "Epoch 5/5, Average Loss: 0.4603\n",
            "Linear evaluation accuracy on 10 classes: 88.20%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 1.0865\n",
            "Epoch 2/5, Average Loss: 0.8401\n",
            "Epoch 3/5, Average Loss: 0.7352\n",
            "Epoch 4/5, Average Loss: 0.6758\n",
            "Epoch 5/5, Average Loss: 0.6335\n",
            "Linear evaluation accuracy on 10 classes: 79.00%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 0.8703\n",
            "Epoch 2/5, Average Loss: 0.6444\n",
            "Epoch 3/5, Average Loss: 0.5561\n",
            "Epoch 4/5, Average Loss: 0.4984\n",
            "Epoch 5/5, Average Loss: 0.4624\n",
            "Linear evaluation accuracy on 10 classes: 87.30%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Epoch 1/5, Average Loss: 0.8347\n",
            "Epoch 2/5, Average Loss: 0.6103\n",
            "Epoch 3/5, Average Loss: 0.5160\n",
            "Epoch 4/5, Average Loss: 0.4603\n",
            "Epoch 5/5, Average Loss: 0.4233\n",
            "Linear evaluation accuracy on 10 classes: 90.90%\n",
            "Calculating R_i for task with classes: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Epoch 1/5, Average Loss: 2.0468\n",
            "Epoch 2/5, Average Loss: 1.9612\n",
            "Epoch 3/5, Average Loss: 1.9163\n",
            "Epoch 4/5, Average Loss: 1.8772\n",
            "Epoch 5/5, Average Loss: 1.8482\n",
            "Linear evaluation accuracy on 10 classes: 40.80%\n",
            "Random network accuracy on task: 40.80%\n",
            "\n",
            "Current A_j,k matrix after Task 6:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "After Task 5: [83.9, 83.3, 88.0, 80.7, 90.8]\n",
            "After Task 6: [84.6, 85.7, 88.2, 79.0, 87.3, 90.9]\n",
            "\n",
            "===== Training Task 7/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0741, CaSSLe Loss: 0.6556, Total Loss: 0.7297\n",
            "Epoch 2/15 - SSL Loss: 0.0709, CaSSLe Loss: 0.1921, Total Loss: 0.2630\n",
            "Epoch 3/15 - SSL Loss: 0.0708, CaSSLe Loss: 0.1418, Total Loss: 0.2126\n",
            "Epoch 4/15 - SSL Loss: 0.0705, CaSSLe Loss: 0.1249, Total Loss: 0.1953\n",
            "Epoch 5/15 - SSL Loss: 0.0695, CaSSLe Loss: 0.1138, Total Loss: 0.1834\n",
            "Epoch 6/15 - SSL Loss: 0.0685, CaSSLe Loss: 0.1078, Total Loss: 0.1763\n",
            "Epoch 7/15 - SSL Loss: 0.0670, CaSSLe Loss: 0.0967, Total Loss: 0.1637\n",
            "Epoch 8/15 - SSL Loss: 0.0653, CaSSLe Loss: 0.0933, Total Loss: 0.1586\n",
            "Epoch 9/15 - SSL Loss: 0.0653, CaSSLe Loss: 0.0923, Total Loss: 0.1577\n",
            "Epoch 10/15 - SSL Loss: 0.0657, CaSSLe Loss: 0.0919, Total Loss: 0.1576\n",
            "Epoch 11/15 - SSL Loss: 0.0646, CaSSLe Loss: 0.0910, Total Loss: 0.1556\n",
            "Epoch 12/15 - SSL Loss: 0.0630, CaSSLe Loss: 0.0905, Total Loss: 0.1535\n",
            "Epoch 13/15 - SSL Loss: 0.0632, CaSSLe Loss: 0.0891, Total Loss: 0.1523\n",
            "Epoch 14/15 - SSL Loss: 0.0636, CaSSLe Loss: 0.0889, Total Loss: 0.1525\n",
            "Epoch 15/15 - SSL Loss: 0.0635, CaSSLe Loss: 0.0891, Total Loss: 0.1526\n",
            "\n",
            "--- Evaluating after Task 7 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.0509\n",
            "Epoch 2/5, Average Loss: 0.7991\n",
            "Epoch 3/5, Average Loss: 0.6947\n",
            "Epoch 4/5, Average Loss: 0.6377\n",
            "Epoch 5/5, Average Loss: 0.5964\n",
            "Linear evaluation accuracy on 10 classes: 84.20%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.8759\n",
            "Epoch 2/5, Average Loss: 0.6632\n",
            "Epoch 3/5, Average Loss: 0.5783\n",
            "Epoch 4/5, Average Loss: 0.5348\n",
            "Epoch 5/5, Average Loss: 0.5001\n",
            "Linear evaluation accuracy on 10 classes: 83.20%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.8971\n",
            "Epoch 2/5, Average Loss: 0.6455\n",
            "Epoch 3/5, Average Loss: 0.5516\n",
            "Epoch 4/5, Average Loss: 0.4962\n",
            "Epoch 5/5, Average Loss: 0.4580\n",
            "Linear evaluation accuracy on 10 classes: 87.60%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 1.0364\n",
            "Epoch 2/5, Average Loss: 0.8148\n",
            "Epoch 3/5, Average Loss: 0.7251\n",
            "Epoch 4/5, Average Loss: 0.6648\n",
            "Epoch 5/5, Average Loss: 0.6271\n",
            "Linear evaluation accuracy on 10 classes: 80.40%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 0.8643\n",
            "Epoch 2/5, Average Loss: 0.6373\n",
            "Epoch 3/5, Average Loss: 0.5449\n",
            "Epoch 4/5, Average Loss: 0.4940\n",
            "Epoch 5/5, Average Loss: 0.4589\n",
            "Linear evaluation accuracy on 10 classes: 89.90%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Epoch 1/5, Average Loss: 0.9008\n",
            "Epoch 2/5, Average Loss: 0.6534\n",
            "Epoch 3/5, Average Loss: 0.5487\n",
            "Epoch 4/5, Average Loss: 0.4873\n",
            "Epoch 5/5, Average Loss: 0.4478\n",
            "Linear evaluation accuracy on 10 classes: 89.50%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Epoch 1/5, Average Loss: 0.9770\n",
            "Epoch 2/5, Average Loss: 0.6997\n",
            "Epoch 3/5, Average Loss: 0.5943\n",
            "Epoch 4/5, Average Loss: 0.5308\n",
            "Epoch 5/5, Average Loss: 0.4873\n",
            "Linear evaluation accuracy on 10 classes: 87.70%\n",
            "Calculating R_i for task with classes: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Epoch 1/5, Average Loss: 1.9111\n",
            "Epoch 2/5, Average Loss: 1.8238\n",
            "Epoch 3/5, Average Loss: 1.7754\n",
            "Epoch 4/5, Average Loss: 1.7438\n",
            "Epoch 5/5, Average Loss: 1.7202\n",
            "Linear evaluation accuracy on 10 classes: 44.40%\n",
            "Random network accuracy on task: 44.40%\n",
            "\n",
            "Current A_j,k matrix after Task 7:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "After Task 5: [83.9, 83.3, 88.0, 80.7, 90.8]\n",
            "After Task 6: [84.6, 85.7, 88.2, 79.0, 87.3, 90.9]\n",
            "After Task 7: [84.2, 83.2, 87.6, 80.4, 89.9, 89.5, 87.7]\n",
            "\n",
            "===== Training Task 8/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0621, CaSSLe Loss: 0.6995, Total Loss: 0.7617\n",
            "Epoch 2/15 - SSL Loss: 0.0603, CaSSLe Loss: 0.2018, Total Loss: 0.2621\n",
            "Epoch 3/15 - SSL Loss: 0.0605, CaSSLe Loss: 0.1455, Total Loss: 0.2060\n",
            "Epoch 4/15 - SSL Loss: 0.0597, CaSSLe Loss: 0.1287, Total Loss: 0.1884\n",
            "Epoch 5/15 - SSL Loss: 0.0594, CaSSLe Loss: 0.1171, Total Loss: 0.1765\n",
            "Epoch 6/15 - SSL Loss: 0.0598, CaSSLe Loss: 0.1132, Total Loss: 0.1730\n",
            "Epoch 7/15 - SSL Loss: 0.0574, CaSSLe Loss: 0.0986, Total Loss: 0.1560\n",
            "Epoch 8/15 - SSL Loss: 0.0560, CaSSLe Loss: 0.0948, Total Loss: 0.1509\n",
            "Epoch 9/15 - SSL Loss: 0.0552, CaSSLe Loss: 0.0941, Total Loss: 0.1493\n",
            "Epoch 10/15 - SSL Loss: 0.0542, CaSSLe Loss: 0.0934, Total Loss: 0.1476\n",
            "Epoch 11/15 - SSL Loss: 0.0559, CaSSLe Loss: 0.0928, Total Loss: 0.1487\n",
            "Epoch 12/15 - SSL Loss: 0.0536, CaSSLe Loss: 0.0918, Total Loss: 0.1454\n",
            "Epoch 13/15 - SSL Loss: 0.0553, CaSSLe Loss: 0.0909, Total Loss: 0.1462\n",
            "Epoch 14/15 - SSL Loss: 0.0541, CaSSLe Loss: 0.0903, Total Loss: 0.1444\n",
            "Epoch 15/15 - SSL Loss: 0.0543, CaSSLe Loss: 0.0906, Total Loss: 0.1449\n",
            "\n",
            "--- Evaluating after Task 8 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.0391\n",
            "Epoch 2/5, Average Loss: 0.8049\n",
            "Epoch 3/5, Average Loss: 0.6992\n",
            "Epoch 4/5, Average Loss: 0.6415\n",
            "Epoch 5/5, Average Loss: 0.6029\n",
            "Linear evaluation accuracy on 10 classes: 83.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.9294\n",
            "Epoch 2/5, Average Loss: 0.7002\n",
            "Epoch 3/5, Average Loss: 0.6071\n",
            "Epoch 4/5, Average Loss: 0.5529\n",
            "Epoch 5/5, Average Loss: 0.5170\n",
            "Linear evaluation accuracy on 10 classes: 84.50%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.7620\n",
            "Epoch 2/5, Average Loss: 0.5831\n",
            "Epoch 3/5, Average Loss: 0.5092\n",
            "Epoch 4/5, Average Loss: 0.4607\n",
            "Epoch 5/5, Average Loss: 0.4290\n",
            "Linear evaluation accuracy on 10 classes: 87.70%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 1.0264\n",
            "Epoch 2/5, Average Loss: 0.8081\n",
            "Epoch 3/5, Average Loss: 0.7210\n",
            "Epoch 4/5, Average Loss: 0.6647\n",
            "Epoch 5/5, Average Loss: 0.6284\n",
            "Linear evaluation accuracy on 10 classes: 78.90%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 0.8745\n",
            "Epoch 2/5, Average Loss: 0.6387\n",
            "Epoch 3/5, Average Loss: 0.5450\n",
            "Epoch 4/5, Average Loss: 0.4927\n",
            "Epoch 5/5, Average Loss: 0.4567\n",
            "Linear evaluation accuracy on 10 classes: 88.70%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Epoch 1/5, Average Loss: 0.8872\n",
            "Epoch 2/5, Average Loss: 0.6501\n",
            "Epoch 3/5, Average Loss: 0.5505\n",
            "Epoch 4/5, Average Loss: 0.4947\n",
            "Epoch 5/5, Average Loss: 0.4553\n",
            "Linear evaluation accuracy on 10 classes: 88.00%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Epoch 1/5, Average Loss: 0.9762\n",
            "Epoch 2/5, Average Loss: 0.7217\n",
            "Epoch 3/5, Average Loss: 0.6103\n",
            "Epoch 4/5, Average Loss: 0.5453\n",
            "Epoch 5/5, Average Loss: 0.5044\n",
            "Linear evaluation accuracy on 10 classes: 87.60%\n",
            "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Epoch 1/5, Average Loss: 0.9780\n",
            "Epoch 2/5, Average Loss: 0.7305\n",
            "Epoch 3/5, Average Loss: 0.6259\n",
            "Epoch 4/5, Average Loss: 0.5702\n",
            "Epoch 5/5, Average Loss: 0.5320\n",
            "Linear evaluation accuracy on 10 classes: 85.10%\n",
            "Calculating R_i for task with classes: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Epoch 1/5, Average Loss: 1.8382\n",
            "Epoch 2/5, Average Loss: 1.7713\n",
            "Epoch 3/5, Average Loss: 1.7300\n",
            "Epoch 4/5, Average Loss: 1.6971\n",
            "Epoch 5/5, Average Loss: 1.6726\n",
            "Linear evaluation accuracy on 10 classes: 45.80%\n",
            "Random network accuracy on task: 45.80%\n",
            "\n",
            "Current A_j,k matrix after Task 8:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "After Task 5: [83.9, 83.3, 88.0, 80.7, 90.8]\n",
            "After Task 6: [84.6, 85.7, 88.2, 79.0, 87.3, 90.9]\n",
            "After Task 7: [84.2, 83.2, 87.6, 80.4, 89.9, 89.5, 87.7]\n",
            "After Task 8: [83.6, 84.5, 87.7, 78.9, 88.7, 88.0, 87.6, 85.1]\n",
            "\n",
            "===== Training Task 9/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0848, CaSSLe Loss: 0.7366, Total Loss: 0.8214\n",
            "Epoch 2/15 - SSL Loss: 0.0827, CaSSLe Loss: 0.2213, Total Loss: 0.3041\n",
            "Epoch 3/15 - SSL Loss: 0.0825, CaSSLe Loss: 0.1578, Total Loss: 0.2403\n",
            "Epoch 4/15 - SSL Loss: 0.0820, CaSSLe Loss: 0.1365, Total Loss: 0.2184\n",
            "Epoch 5/15 - SSL Loss: 0.0814, CaSSLe Loss: 0.1256, Total Loss: 0.2070\n",
            "Epoch 6/15 - SSL Loss: 0.0811, CaSSLe Loss: 0.1218, Total Loss: 0.2029\n",
            "Epoch 7/15 - SSL Loss: 0.0774, CaSSLe Loss: 0.1066, Total Loss: 0.1840\n",
            "Epoch 8/15 - SSL Loss: 0.0779, CaSSLe Loss: 0.1028, Total Loss: 0.1808\n",
            "Epoch 9/15 - SSL Loss: 0.0763, CaSSLe Loss: 0.1015, Total Loss: 0.1778\n",
            "Epoch 10/15 - SSL Loss: 0.0751, CaSSLe Loss: 0.1007, Total Loss: 0.1758\n",
            "Epoch 11/15 - SSL Loss: 0.0752, CaSSLe Loss: 0.1000, Total Loss: 0.1752\n",
            "Epoch 12/15 - SSL Loss: 0.0755, CaSSLe Loss: 0.0995, Total Loss: 0.1750\n",
            "Epoch 13/15 - SSL Loss: 0.0740, CaSSLe Loss: 0.0978, Total Loss: 0.1718\n",
            "Epoch 14/15 - SSL Loss: 0.0745, CaSSLe Loss: 0.0978, Total Loss: 0.1723\n",
            "Epoch 15/15 - SSL Loss: 0.0749, CaSSLe Loss: 0.0975, Total Loss: 0.1724\n",
            "\n",
            "--- Evaluating after Task 9 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.0010\n",
            "Epoch 2/5, Average Loss: 0.7872\n",
            "Epoch 3/5, Average Loss: 0.6960\n",
            "Epoch 4/5, Average Loss: 0.6357\n",
            "Epoch 5/5, Average Loss: 0.5941\n",
            "Linear evaluation accuracy on 10 classes: 83.60%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.8627\n",
            "Epoch 2/5, Average Loss: 0.6702\n",
            "Epoch 3/5, Average Loss: 0.5948\n",
            "Epoch 4/5, Average Loss: 0.5473\n",
            "Epoch 5/5, Average Loss: 0.5166\n",
            "Linear evaluation accuracy on 10 classes: 84.30%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.8143\n",
            "Epoch 2/5, Average Loss: 0.6093\n",
            "Epoch 3/5, Average Loss: 0.5300\n",
            "Epoch 4/5, Average Loss: 0.4807\n",
            "Epoch 5/5, Average Loss: 0.4449\n",
            "Linear evaluation accuracy on 10 classes: 87.20%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 0.9943\n",
            "Epoch 2/5, Average Loss: 0.7970\n",
            "Epoch 3/5, Average Loss: 0.7098\n",
            "Epoch 4/5, Average Loss: 0.6615\n",
            "Epoch 5/5, Average Loss: 0.6276\n",
            "Linear evaluation accuracy on 10 classes: 80.70%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 0.8849\n",
            "Epoch 2/5, Average Loss: 0.6449\n",
            "Epoch 3/5, Average Loss: 0.5489\n",
            "Epoch 4/5, Average Loss: 0.4947\n",
            "Epoch 5/5, Average Loss: 0.4588\n",
            "Linear evaluation accuracy on 10 classes: 87.40%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Epoch 1/5, Average Loss: 0.9137\n",
            "Epoch 2/5, Average Loss: 0.6576\n",
            "Epoch 3/5, Average Loss: 0.5530\n",
            "Epoch 4/5, Average Loss: 0.4976\n",
            "Epoch 5/5, Average Loss: 0.4628\n",
            "Linear evaluation accuracy on 10 classes: 86.20%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Epoch 1/5, Average Loss: 0.8835\n",
            "Epoch 2/5, Average Loss: 0.6612\n",
            "Epoch 3/5, Average Loss: 0.5735\n",
            "Epoch 4/5, Average Loss: 0.5212\n",
            "Epoch 5/5, Average Loss: 0.4829\n",
            "Linear evaluation accuracy on 10 classes: 86.40%\n",
            "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Epoch 1/5, Average Loss: 0.9862\n",
            "Epoch 2/5, Average Loss: 0.7332\n",
            "Epoch 3/5, Average Loss: 0.6365\n",
            "Epoch 4/5, Average Loss: 0.5779\n",
            "Epoch 5/5, Average Loss: 0.5371\n",
            "Linear evaluation accuracy on 10 classes: 85.20%\n",
            "  Evaluating on classes from Task 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Epoch 1/5, Average Loss: 1.1525\n",
            "Epoch 2/5, Average Loss: 0.9110\n",
            "Epoch 3/5, Average Loss: 0.7965\n",
            "Epoch 4/5, Average Loss: 0.7287\n",
            "Epoch 5/5, Average Loss: 0.6847\n",
            "Linear evaluation accuracy on 10 classes: 81.30%\n",
            "Calculating R_i for task with classes: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Epoch 1/5, Average Loss: 2.1659\n",
            "Epoch 2/5, Average Loss: 2.0916\n",
            "Epoch 3/5, Average Loss: 2.0628\n",
            "Epoch 4/5, Average Loss: 2.0386\n",
            "Epoch 5/5, Average Loss: 2.0213\n",
            "Linear evaluation accuracy on 10 classes: 30.90%\n",
            "Random network accuracy on task: 30.90%\n",
            "\n",
            "Current A_j,k matrix after Task 9:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "After Task 5: [83.9, 83.3, 88.0, 80.7, 90.8]\n",
            "After Task 6: [84.6, 85.7, 88.2, 79.0, 87.3, 90.9]\n",
            "After Task 7: [84.2, 83.2, 87.6, 80.4, 89.9, 89.5, 87.7]\n",
            "After Task 8: [83.6, 84.5, 87.7, 78.9, 88.7, 88.0, 87.6, 85.1]\n",
            "After Task 9: [83.6, 84.3, 87.2, 80.7, 87.4, 86.2, 86.4, 85.2, 81.3]\n",
            "\n",
            "===== Training Task 10/10 =====\n",
            "Frozen encoder (f_t-1) loaded and parameters frozen: True\n",
            "Distilling from frozen teacher encoder (f_t-1): True\n",
            "Epoch 1/15 - SSL Loss: 0.0783, CaSSLe Loss: 0.7992, Total Loss: 0.8775\n",
            "Epoch 2/15 - SSL Loss: 0.0763, CaSSLe Loss: 0.2381, Total Loss: 0.3144\n",
            "Epoch 3/15 - SSL Loss: 0.0762, CaSSLe Loss: 0.1673, Total Loss: 0.2434\n",
            "Epoch 4/15 - SSL Loss: 0.0755, CaSSLe Loss: 0.1456, Total Loss: 0.2210\n",
            "Epoch 5/15 - SSL Loss: 0.0752, CaSSLe Loss: 0.1353, Total Loss: 0.2105\n",
            "Epoch 6/15 - SSL Loss: 0.0731, CaSSLe Loss: 0.1292, Total Loss: 0.2023\n",
            "Epoch 7/15 - SSL Loss: 0.0714, CaSSLe Loss: 0.1144, Total Loss: 0.1858\n",
            "Epoch 8/15 - SSL Loss: 0.0709, CaSSLe Loss: 0.1098, Total Loss: 0.1807\n",
            "Epoch 9/15 - SSL Loss: 0.0692, CaSSLe Loss: 0.1087, Total Loss: 0.1779\n",
            "Epoch 10/15 - SSL Loss: 0.0694, CaSSLe Loss: 0.1072, Total Loss: 0.1766\n",
            "Epoch 11/15 - SSL Loss: 0.0710, CaSSLe Loss: 0.1071, Total Loss: 0.1781\n",
            "Epoch 12/15 - SSL Loss: 0.0695, CaSSLe Loss: 0.1059, Total Loss: 0.1754\n",
            "Epoch 13/15 - SSL Loss: 0.0694, CaSSLe Loss: 0.1051, Total Loss: 0.1745\n",
            "Epoch 14/15 - SSL Loss: 0.0693, CaSSLe Loss: 0.1048, Total Loss: 0.1741\n",
            "Epoch 15/15 - SSL Loss: 0.0684, CaSSLe Loss: 0.1043, Total Loss: 0.1728\n",
            "\n",
            "--- Evaluating after Task 10 ---\n",
            "  Evaluating on classes from Task 1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78]\n",
            "Epoch 1/5, Average Loss: 1.0072\n",
            "Epoch 2/5, Average Loss: 0.7830\n",
            "Epoch 3/5, Average Loss: 0.6919\n",
            "Epoch 4/5, Average Loss: 0.6384\n",
            "Epoch 5/5, Average Loss: 0.6021\n",
            "Linear evaluation accuracy on 10 classes: 83.40%\n",
            "  Evaluating on classes from Task 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37]\n",
            "Epoch 1/5, Average Loss: 0.9125\n",
            "Epoch 2/5, Average Loss: 0.6887\n",
            "Epoch 3/5, Average Loss: 0.6019\n",
            "Epoch 4/5, Average Loss: 0.5524\n",
            "Epoch 5/5, Average Loss: 0.5165\n",
            "Linear evaluation accuracy on 10 classes: 82.80%\n",
            "  Evaluating on classes from Task 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33]\n",
            "Epoch 1/5, Average Loss: 0.8671\n",
            "Epoch 2/5, Average Loss: 0.6418\n",
            "Epoch 3/5, Average Loss: 0.5509\n",
            "Epoch 4/5, Average Loss: 0.4947\n",
            "Epoch 5/5, Average Loss: 0.4598\n",
            "Linear evaluation accuracy on 10 classes: 87.70%\n",
            "  Evaluating on classes from Task 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12]\n",
            "Epoch 1/5, Average Loss: 1.0978\n",
            "Epoch 2/5, Average Loss: 0.8356\n",
            "Epoch 3/5, Average Loss: 0.7382\n",
            "Epoch 4/5, Average Loss: 0.6759\n",
            "Epoch 5/5, Average Loss: 0.6386\n",
            "Linear evaluation accuracy on 10 classes: 79.70%\n",
            "  Evaluating on classes from Task 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34]\n",
            "Epoch 1/5, Average Loss: 0.8761\n",
            "Epoch 2/5, Average Loss: 0.6490\n",
            "Epoch 3/5, Average Loss: 0.5575\n",
            "Epoch 4/5, Average Loss: 0.5010\n",
            "Epoch 5/5, Average Loss: 0.4657\n",
            "Linear evaluation accuracy on 10 classes: 89.10%\n",
            "  Evaluating on classes from Task 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62]\n",
            "Epoch 1/5, Average Loss: 0.9214\n",
            "Epoch 2/5, Average Loss: 0.6716\n",
            "Epoch 3/5, Average Loss: 0.5636\n",
            "Epoch 4/5, Average Loss: 0.5035\n",
            "Epoch 5/5, Average Loss: 0.4620\n",
            "Linear evaluation accuracy on 10 classes: 87.60%\n",
            "  Evaluating on classes from Task 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20]\n",
            "Epoch 1/5, Average Loss: 0.9067\n",
            "Epoch 2/5, Average Loss: 0.6681\n",
            "Epoch 3/5, Average Loss: 0.5697\n",
            "Epoch 4/5, Average Loss: 0.5153\n",
            "Epoch 5/5, Average Loss: 0.4835\n",
            "Linear evaluation accuracy on 10 classes: 86.20%\n",
            "  Evaluating on classes from Task 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77]\n",
            "Epoch 1/5, Average Loss: 0.9373\n",
            "Epoch 2/5, Average Loss: 0.7089\n",
            "Epoch 3/5, Average Loss: 0.6197\n",
            "Epoch 4/5, Average Loss: 0.5622\n",
            "Epoch 5/5, Average Loss: 0.5252\n",
            "Linear evaluation accuracy on 10 classes: 86.10%\n",
            "  Evaluating on classes from Task 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69]\n",
            "Epoch 1/5, Average Loss: 1.2001\n",
            "Epoch 2/5, Average Loss: 0.9250\n",
            "Epoch 3/5, Average Loss: 0.8199\n",
            "Epoch 4/5, Average Loss: 0.7538\n",
            "Epoch 5/5, Average Loss: 0.7077\n",
            "Linear evaluation accuracy on 10 classes: 80.00%\n",
            "  Evaluating on classes from Task 10: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
            "Epoch 1/5, Average Loss: 0.7816\n",
            "Epoch 2/5, Average Loss: 0.5878\n",
            "Epoch 3/5, Average Loss: 0.5158\n",
            "Epoch 4/5, Average Loss: 0.4721\n",
            "Epoch 5/5, Average Loss: 0.4468\n",
            "Linear evaluation accuracy on 10 classes: 85.70%\n",
            "Calculating R_i for task with classes: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]\n",
            "Epoch 1/5, Average Loss: 2.0627\n",
            "Epoch 2/5, Average Loss: 2.0013\n",
            "Epoch 3/5, Average Loss: 1.9632\n",
            "Epoch 4/5, Average Loss: 1.9306\n",
            "Epoch 5/5, Average Loss: 1.9068\n",
            "Linear evaluation accuracy on 10 classes: 35.80%\n",
            "Random network accuracy on task: 35.80%\n",
            "\n",
            "Current A_j,k matrix after Task 10:\n",
            "After Task 1: [87.0]\n",
            "After Task 2: [83.3, 86.9]\n",
            "After Task 3: [84.6, 86.1, 89.7]\n",
            "After Task 4: [84.4, 83.9, 88.3, 81.0]\n",
            "After Task 5: [83.9, 83.3, 88.0, 80.7, 90.8]\n",
            "After Task 6: [84.6, 85.7, 88.2, 79.0, 87.3, 90.9]\n",
            "After Task 7: [84.2, 83.2, 87.6, 80.4, 89.9, 89.5, 87.7]\n",
            "After Task 8: [83.6, 84.5, 87.7, 78.9, 88.7, 88.0, 87.6, 85.1]\n",
            "After Task 9: [83.6, 84.3, 87.2, 80.7, 87.4, 86.2, 86.4, 85.2, 81.3]\n",
            "After Task 10: [83.4, 82.8, 87.7, 79.7, 89.1, 87.6, 86.2, 86.1, 80.0, 85.7]\n",
            "\n",
            " CaSSLe Continual Training Process Completed\n",
            "\n",
            "Final Average Accuracy (A): 84.83%\n",
            "Final Forgetting (F): 2.09%\n",
            "Final Forward Transfer (FT): 0.00%\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_SIZE = 224 # MAE's default input size for ViT-base\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS_PER_TASK = 15\n",
        "LEARNING_RATE = 0.0001\n",
        "LAMBDA_CASSLE = 2 # Weight for the CaSSLe distillation loss\n",
        "NUM_CLASSES_PER_TASK = 10\n",
        "NUM_TOTAL_CLASSES = 100\n",
        "LINEAR_EVAL_EPOCHS = 5 # Number of epochs to train the linear classifier\n",
        "LINEAR_EVAL_BATCH_SIZE = 128\n",
        "# Initial Setup\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Load pre-trained MAE-base model and processor\n",
        "mae_backbone = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base')\n",
        "mae_backbone.to(DEVICE)\n",
        "processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-base')\n",
        "\n",
        "# Define the two augmentation pipelines\n",
        "transform_v1 = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.2, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "])\n",
        "\n",
        "transform_v2 = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.2, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.)), # Kernel size must be odd\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 training dataset\n",
        "cifar100_train_full = datasets.CIFAR100(root='./data', train=True, download=True)\n",
        "cifar100_test_full = datasets.CIFAR100(root='./data', train=False, download=True)\n",
        "# Prepare Task Data\n",
        "# Divide CIFAR-100 into 10 tasks of 10 classes each\n",
        "all_classes = list(range(NUM_TOTAL_CLASSES))\n",
        "random.shuffle(all_classes) # Shuffle classes for a fairer split\n",
        "\n",
        "task_class_splits = []\n",
        "for i in range(0, NUM_TOTAL_CLASSES, NUM_CLASSES_PER_TASK):\n",
        "    task_class_splits.append(all_classes[i:i + NUM_CLASSES_PER_TASK])\n",
        "\n",
        "task_datasets = []\n",
        "for i, class_list in enumerate(task_class_splits):\n",
        "    print(f\"Task {i+1} includes classes: {class_list}\")\n",
        "    task_dataset = CustomCifar100TaskDataset(\n",
        "        cifar100_train_full,\n",
        "        class_list,\n",
        "        transform_v1,\n",
        "        transform_v2,\n",
        "        processor\n",
        "    )\n",
        "    task_datasets.append(task_dataset)\n",
        "\n",
        "# Initialize CaSSLe Components\n",
        "# base_ssl_model_instance is the current model (f_t) that gets updated across tasks\n",
        "# It starts with the loaded pretrained MAE.\n",
        "base_ssl_model_instance = MAECaSSLeModel(mae_backbone=mae_backbone)\n",
        "\n",
        "# This will store the state_dict of the f_t-1 (previous encoder)\n",
        "prev_encoder_state_dict = None\n",
        "\n",
        "print(\"\\n--- Starting CaSSLe Continual Training ---\")\n",
        "print(f\"Running on device: {DEVICE}\")\n",
        "\n",
        "# Metrics storage A_j_k will be a list of lists (j tasks observed, k task evaluated on)\n",
        "# A_j_k[j_idx][k_idx]\n",
        "all_task_accuracies = [] # Stores accuracies A_{j,k}\n",
        "\n",
        "\n",
        "random_accuracies_Ri = {} # {task_idx: R_i_value}\n",
        "\n",
        "# Continual Training Loop\n",
        "for task_id, current_task_dataset in enumerate(task_datasets):\n",
        "    print(f\"\\n===== Training Task {task_id + 1}/{len(task_datasets)} =====\")\n",
        "\n",
        "    current_task_loader = DataLoader(current_task_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                      num_workers=os.cpu_count() // 2 if os.cpu_count() else 0, pin_memory=False)\n",
        "\n",
        "\n",
        "    # Pass the SAME base_ssl_model_instance, as this is the model we continue updating.\n",
        "    trainer = CaSSLeTrainer(\n",
        "        base_ssl_model=base_ssl_model_instance,\n",
        "        ca_predictor_hidden_dim=256, # Example hidden dim for 'g'\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        lambda_cassle=LAMBDA_CASSLE,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    # If it's not the first task, set up the frozen teacher (f_t-1)\n",
        "    if prev_encoder_state_dict:\n",
        "        trainer.set_previous_frozen_encoder(prev_encoder_state_dict)\n",
        "\n",
        "    # Train the current task and get the state of the trained f_t.\n",
        "    # This will be used as the f_t-1 for the next task.\n",
        "    prev_encoder_state_dict = trainer.train_task(current_task_loader, NUM_EPOCHS_PER_TASK)\n",
        "\n",
        "    print(f\"\\n--- Evaluating after Task {task_id + 1} ---\")\n",
        "\n",
        "    # Determine all classes seen up to this task\n",
        "    current_seen_classes = []\n",
        "    for i in range(task_id + 1):\n",
        "        current_seen_classes.extend(task_class_splits[i])\n",
        "    current_seen_classes = sorted(list(set(current_seen_classes))) # Ensure unique and sorted\n",
        "\n",
        "    accuracies_after_this_task = [] # Stores accuracies\n",
        "\n",
        "    for eval_task_idx in range(task_id + 1):\n",
        "        # Classes for the specific evaluation task (k)\n",
        "        eval_task_classes = task_class_splits[eval_task_idx]\n",
        "        print(f\"  Evaluating on classes from Task {eval_task_idx+1}: {eval_task_classes}\")\n",
        "\n",
        "        # Perform linear evaluation on the classes specific to eval_task_idx\n",
        "        acc_jk = evaluate_model(\n",
        "            base_ssl_model_instance.mae_backbone, # The current encoder (f_t)\n",
        "            eval_task_classes, # Evaluate on only classes of task k\n",
        "            cifar100_train_full,\n",
        "            cifar100_test_full,\n",
        "            processor,\n",
        "            LINEAR_EVAL_BATCH_SIZE,\n",
        "            LINEAR_EVAL_EPOCHS,\n",
        "            DEVICE\n",
        "        )\n",
        "        accuracies_after_this_task.append(acc_jk)\n",
        "\n",
        "            # Calculate R_i for Forward Transfer only once per task\n",
        "        if eval_task_idx not in random_accuracies_Ri:\n",
        "              random_accuracies_Ri[eval_task_idx] = get_random_accuracy(\n",
        "                NUM_CLASSES_PER_TASK,\n",
        "                cifar100_train_full,\n",
        "                cifar100_test_full,\n",
        "                processor,\n",
        "                eval_task_classes, # R_i is for task i\n",
        "                LINEAR_EVAL_BATCH_SIZE,\n",
        "                LINEAR_EVAL_EPOCHS,\n",
        "                DEVICE\n",
        "            )\n",
        "\n",
        "    all_task_accuracies.append(accuracies_after_this_task)\n",
        "\n",
        "    # Print current state (optional)\n",
        "    print(f\"\\nCurrent A_j,k matrix after Task {task_id + 1}:\")\n",
        "    for j_idx, row in enumerate(all_task_accuracies):\n",
        "        print(f\"After Task {j_idx+1}: {row}\")\n",
        "\n",
        "\n",
        "print(\"\\n CaSSLe Continual Training Process Completed\")\n",
        "\n",
        "# Final Metric Calculation\n",
        "T = len(task_datasets)\n",
        "\n",
        "# Average Accuracy (A)\n",
        "final_accuracies_row = all_task_accuracies[T-1] if T > 0 else []\n",
        "avg_accuracy = sum(final_accuracies_row) / T if T > 0 else 0\n",
        "print(f\"\\nFinal Average Accuracy (A): {avg_accuracy:.2f}%\")\n",
        "\n",
        "# Forgetting (F)\n",
        "forgetting = 0\n",
        "if T > 1:\n",
        "    for i in range(T - 1): # For each previous task i (0-indexed)\n",
        "        max_acc_on_task_i = 0\n",
        "        for t_idx in range(T): # Max over all times t when task i was evaluated\n",
        "            if t_idx < len(all_task_accuracies) and i < len(all_task_accuracies[t_idx]):\n",
        "                max_acc_on_task_i = max(max_acc_on_task_i, all_task_accuracies[t_idx][i])\n",
        "\n",
        "        final_acc_on_task_i = all_task_accuracies[T-1][i] # A_T,i\n",
        "        forgetting += (max_acc_on_task_i - final_acc_on_task_i)\n",
        "    forgetting /= (T - 1)\n",
        "print(f\"Final Forgetting (F): {forgetting:.2f}%\")\n",
        "\n",
        "#Forward Transfer\n",
        "forward_transfer = 0\n",
        "if T > 1:\n",
        "    count = 0\n",
        "    for i in range(2, T+1):\n",
        "        row_idx = i - 2\n",
        "        col_idx = i - 1\n",
        "        if row_idx < len(all_task_accuracies) and col_idx < len(all_task_accuracies[row_idx]):\n",
        "            acc_i_minus_1_on_i = all_task_accuracies[row_idx][col_idx]\n",
        "            r_i_value = random_accuracies_Ri[col_idx]\n",
        "            forward_transfer += (acc_i_minus_1_on_i - r_i_value)\n",
        "            count += 1\n",
        "        else:\n",
        "            # Skip if data missing for this pair\n",
        "            continue\n",
        "    if count > 0:\n",
        "        forward_transfer /= count\n",
        "    else:\n",
        "        forward_transfer = 0\n",
        "print(f\"Final Forward Transfer (FT): {forward_transfer:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2z2szzFz1bM",
        "outputId": "ca6bdee2-48c5-41b2-f6b9-62c0c63101e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Backward Transfer (BT): -0.40%\n"
          ]
        }
      ],
      "source": [
        "# Backward Transfer (BT)\n",
        "backward_transfer = 0\n",
        "count = 0\n",
        "\n",
        "if T > 1:\n",
        "    for new_task in range(1, T):  # starting from second task\n",
        "        for old_task in range(new_task):  # all previously learned tasks\n",
        "            # Accuracy on old_task before learning new_task\n",
        "            acc_before = all_task_accuracies[new_task - 1][old_task] if (new_task - 1) < len(all_task_accuracies) and old_task < len(all_task_accuracies[new_task - 1]) else None\n",
        "\n",
        "            # Accuracy on old_task after learning new_task\n",
        "            acc_after = all_task_accuracies[new_task][old_task] if new_task < len(all_task_accuracies) and old_task < len(all_task_accuracies[new_task]) else None\n",
        "\n",
        "            if acc_before is not None and acc_after is not None:\n",
        "                backward_transfer += (acc_after - acc_before)\n",
        "                count += 1\n",
        "            else:\n",
        "                print(f\"Skipping BT for old_task {old_task+1}, new_task {new_task+1}: missing data\")\n",
        "\n",
        "backward_transfer = backward_transfer / count if count > 0 else 0\n",
        "print(f\"Final Backward Transfer (BT): {backward_transfer:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPANDUlKz6tf",
        "outputId": "e1a4b2ee-88bf-417a-f799-1d5f1eff9278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 2: first_acc = 86.90, random = 37.90\n",
            "Task 3: first_acc = 89.70, random = 41.50\n",
            "Task 4: first_acc = 81.00, random = 36.90\n",
            "Task 5: first_acc = 90.80, random = 37.60\n",
            "Task 6: first_acc = 90.90, random = 40.80\n",
            "Task 7: first_acc = 87.70, random = 44.40\n",
            "Task 8: first_acc = 85.10, random = 45.80\n",
            "Task 9: first_acc = 81.30, random = 30.90\n",
            "Task 10: first_acc = 85.70, random = 35.80\n",
            "Approx. Forward Transfer (FT): 47.50%\n"
          ]
        }
      ],
      "source": [
        "# Approximate Forward Transfer (Lazy FT)\n",
        "forward_transfer = 0\n",
        "count = 0\n",
        "\n",
        "if T > 1:\n",
        "    for k in range(1, T):  # Task k (1 to T-1)\n",
        "        # Use the accuracy when task k is first trained\n",
        "        if k < len(all_task_accuracies) and k < len(all_task_accuracies[k]):\n",
        "            first_acc = all_task_accuracies[k][k]  # Model learns task k\n",
        "            random_baseline = random_accuracies_Ri[k]\n",
        "\n",
        "            print(f\"Task {k+1}: first_acc = {first_acc:.2f}, random = {random_baseline:.2f}\")\n",
        "\n",
        "            forward_transfer += (first_acc - random_baseline)\n",
        "            count += 1\n",
        "        else:\n",
        "            print(f\"Skipping Task {k+1}: missing all_task_accuracies[{k}][{k}]\")\n",
        "\n",
        "forward_transfer = forward_transfer / count if count > 0 else 0\n",
        "print(f\"Approx. Forward Transfer (FT): {forward_transfer:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01cd4d497ba24864a3b52e841481ea2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "078ab178b7864b35ab1b5190760d8db0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e6299ff7c384fb98fab30a583237f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23bdebd26192413d9c2b1dd11e76ad29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb9b2909a2e44926910105ee5085695c",
            "placeholder": "​",
            "style": "IPY_MODEL_36aff59f3dc94385b0d7ffb5d260b51f",
            "value": " 217/217 [00:00&lt;00:00, 25.1kB/s]"
          }
        },
        "25936c18a20a409d818ff7c0f169e233": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "278cde1efa434c1dbfd9dd5a2b22e9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_078ab178b7864b35ab1b5190760d8db0",
            "max": 676,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_867c779fe3764e5d878ddcd5535fb7e4",
            "value": 676
          }
        },
        "2fc2dd170e974d3bb3db2fab9dc7ca18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bfe98e3e6ba4c22adc28262ff4177b2",
            "placeholder": "​",
            "style": "IPY_MODEL_8a7a2519e885493483e2646d0d20e00e",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "36aff59f3dc94385b0d7ffb5d260b51f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37b8357360314f0fbadb33538d5c1a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bd86d5edf8c49f39ed29a2198a8e500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f57308ef62f44904992da0f1d33ff63c",
            "placeholder": "​",
            "style": "IPY_MODEL_25936c18a20a409d818ff7c0f169e233",
            "value": "model.safetensors: 100%"
          }
        },
        "4dd5feaa520e4451a3eb411ee518e755": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6cf2565e8c43f4ad95fbddef535808": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f59ca986d544bcc9b848721812e79d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4446a05c7be4b5d9de8ef498edc8685",
            "placeholder": "​",
            "style": "IPY_MODEL_abb67562ea474ae8b07361bc1380c12a",
            "value": " 676/676 [00:00&lt;00:00, 32.2kB/s]"
          }
        },
        "72a91181c871444cafd8ac0a6d3ce0a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "758489f6eb02445ea40bbe9ce86a5d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37b8357360314f0fbadb33538d5c1a7f",
            "placeholder": "​",
            "style": "IPY_MODEL_ee9b45c240e24bae8abb2a84e0034d54",
            "value": "config.json: 100%"
          }
        },
        "7bfe98e3e6ba4c22adc28262ff4177b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867c779fe3764e5d878ddcd5535fb7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a7a2519e885493483e2646d0d20e00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9557213968dd4998a88a7b2b8ba8ac32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_758489f6eb02445ea40bbe9ce86a5d0c",
              "IPY_MODEL_278cde1efa434c1dbfd9dd5a2b22e9d9",
              "IPY_MODEL_6f59ca986d544bcc9b848721812e79d1"
            ],
            "layout": "IPY_MODEL_72a91181c871444cafd8ac0a6d3ce0a0"
          }
        },
        "ab39e29b9ed3428cbeac9a9986900e88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb67562ea474ae8b07361bc1380c12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adc20ad8a0f14386b107b5b8af4226ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c6cf2565e8c43f4ad95fbddef535808",
            "max": 447670680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edbde7dae4c445f59de40aa950e0f4f0",
            "value": 447670680
          }
        },
        "b4446a05c7be4b5d9de8ef498edc8685": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9149e4edd941f4878fdddf6814c49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bd86d5edf8c49f39ed29a2198a8e500",
              "IPY_MODEL_adc20ad8a0f14386b107b5b8af4226ca",
              "IPY_MODEL_fee3661f0d124ce1ad0e5686dee3c3fd"
            ],
            "layout": "IPY_MODEL_c903aac6321f4c1e8450a0c6538c7e33"
          }
        },
        "c903aac6321f4c1e8450a0c6538c7e33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a5ddae4afc4647a72ce0f26b2b20c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8987bb6d7042feaa6d4ebb55fedabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dd5feaa520e4451a3eb411ee518e755",
            "max": 217,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e6299ff7c384fb98fab30a583237f5a",
            "value": 217
          }
        },
        "da04a97d95f741a8ab14e9c7890884f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fc2dd170e974d3bb3db2fab9dc7ca18",
              "IPY_MODEL_cb8987bb6d7042feaa6d4ebb55fedabb",
              "IPY_MODEL_23bdebd26192413d9c2b1dd11e76ad29"
            ],
            "layout": "IPY_MODEL_c9a5ddae4afc4647a72ce0f26b2b20c0"
          }
        },
        "eb9b2909a2e44926910105ee5085695c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edbde7dae4c445f59de40aa950e0f4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee9b45c240e24bae8abb2a84e0034d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f57308ef62f44904992da0f1d33ff63c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee3661f0d124ce1ad0e5686dee3c3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab39e29b9ed3428cbeac9a9986900e88",
            "placeholder": "​",
            "style": "IPY_MODEL_01cd4d497ba24864a3b52e841481ea2f",
            "value": " 448M/448M [00:07&lt;00:00, 48.2MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
